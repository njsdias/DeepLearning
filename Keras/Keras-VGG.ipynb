{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Very Deep Convolutional Networks for Large-Scale Image Recognition, by K. Simonyan and A. Zisserman*\n",
    "One model in the paper denoted as D or VGG-16 has 16 deep layers. An implementation in Java Caffe (http://caffe.berkeleyvision.org/) has been\n",
    "used for training the model on the ImageNet ILSVRC-2012 (http://imagenet.\n",
    "org/challenges/LSVRC/2012/) dataset, which includes images of 1,000 classes and is split into three sets:\n",
    "training (1.3 million images), validation (50,000 images), and testing (100,000 images). Each image\n",
    "is (224 x 224) on three channels. The model achieves 7.5% top 5 error on ILSVRC-2012-val and\n",
    "7.4% top 5 error on ILSVRC-2012-test.\n",
    "\n",
    "\n",
    "The goal of this competition is to estimate the content of photographs for the purpose of retrieval and\n",
    "automatic annotation using a subset of the large hand-labeled ImageNet dataset (10 million labeled\n",
    "images depicting 10,000 + object categories) as training. Test images will be presented with no\n",
    "initial annotation—no segmentation or labels—and **algorithms will have to produce labelings\n",
    "specifying what objects are present in the images**.\n",
    "\n",
    "\n",
    "The weights learned by the model implemented in Caffe have been directly converted in Keras (for\n",
    "more information refer to: https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3) and can be used for\n",
    "preloading into the Keras model, which is implemented here as described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "import cv2, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a VGG16 network\n",
    "# 16 Layers\n",
    "def VGG_16(weights_path=None):\n",
    "    model = Sequential()\n",
    "    #-----------------------------------------------------\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))               # 1- layer\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))    \n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))               # 2- layer\n",
    "    \n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    #----------------------------------------------------- \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))              # 3- layer\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))              # 4- layer\n",
    "    \n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    #-----------------------------------------------------\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))              # 5- layer\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))              # 6- layer\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))              # 7- layer\n",
    "    \n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    #-----------------------------------------------------\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))              # 8- layer\n",
    "   \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))              # 9- layer\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))              # 10- layer\n",
    "    \n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    #-----------------------------------------------------\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))              # 11- layer\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))              # 12- layer\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))              # 13- layer\n",
    "    \n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    #-----------------------------------------------------\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #top layer of the VGG net\n",
    "    model.add(Dense(4096, activation='relu'))                     # 14- layer\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))                     # 15- layer\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='softmax'))                  # 16- layer\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Image and Prediciting\n",
    "For obtain the witghs of the model you can search in google for:\n",
    "        - download vgg16_weights.h5  (https://drive.google.com/uc?id=0Bz7KyqmuGsilT0J5dmRCM0ROVHc&export=download)\n",
    "\n",
    "**Note:** This file have 528 MB\n",
    "\n",
    "**Tip:** For checking the meaning of the resulted number after run the predicting, please consult the website:\n",
    "\n",
    "        https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    im = cv2.resize(cv2.imread('cat-standing.jpg'), (224, 224)).astype(np.float32)\n",
    "    im = im.transpose((2,0,1))\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    K.set_image_dim_ordering(\"th\")\n",
    "    \n",
    "    # Test pretrained model\n",
    "    model = VGG_16('vgg16_weights.h5')\n",
    "    optimizer = SGD()\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "    out = model.predict(im)\n",
    "    print np.argmax(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='brown'>Utilizing Keras built-in VGG-16 net module </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prebuild model with pre-trained weights on imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prebuild model with pre-trained weights on imagenet\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "# resize into VGG16 trained images' format\n",
    "im = cv2.resize(cv2.imread('steam-locomotive.jpg'), (224, 224))\n",
    "im = np.expand_dims(im, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "out = model.predict(im)\n",
    "plt.plot(out.ravel())\n",
    "plt.show()\n",
    "print np.argmax(out)\n",
    "#this should print 820 for steaming train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='brown'> Extracting features from an intermediate layer in a DCNN </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intermediate layer has the capability to\n",
    "extract important features from an image, and these features are more likely to help in different kinds\n",
    "of classification. **This has multiple advantages.**\n",
    "- First, we can rely on publicly available large-scale\n",
    "training and transfer this learning to novel domains. \n",
    "- Second, we can save time for expensive large\n",
    "training. \n",
    "- Third, we can provide reasonable solutions even when we don't have a large number of\n",
    "training examples for our domain. We also get a good starting network shape for the task at hand,\n",
    "instead of guessing it.\n",
    "\n",
    "Next is the code to implements the idea by extracting features from a specific layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-built and pre-trained deep learning VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=True)\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print (i, layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from block4_pool block\n",
    "model = Model(input=base_model.input, output=base_model.get_layer('block4_pool').output)\n",
    "img_path = 'cat-standing.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "# get the features from this block\n",
    "features = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='brown'> Every deep inception-v3 net used for transfer learning </font>\n",
    "\n",
    "Computer vision researchers now commonly use pre-trained CNNs to\n",
    "generate representations for novel tasks, where the dataset may not be large enough to train an entire\n",
    "CNN from scratch. Another common tactic is to take the pre-trained ImageNet network and then to\n",
    "fine-tune the entire network to the novel task.\n",
    "\n",
    "**Inception-v3** net is a very deep ConvNet developed by Google. Keras implements the full network\n",
    "described in the following diagram and it comes pre-trained on ImageNet. The default input size for\n",
    "this model is 299 x 299 on three channels\n",
    "\n",
    "**For more info:** https://keras.io/applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
