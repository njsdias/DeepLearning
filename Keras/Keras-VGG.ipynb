{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Very Deep Convolutional Networks for Large-Scale Image Recognition, by K. Simonyan and A. Zisserman*\n",
    "One model in the paper denoted as D or VGG-16 has 16 deep layers. An implementation in Java Caffe (http://caffe.berkeleyvision.org/) has been\n",
    "used for training the model on the ImageNet ILSVRC-2012 (http://imagenet.\n",
    "org/challenges/LSVRC/2012/) dataset, which includes images of 1,000 classes and is split into three sets:\n",
    "training (1.3 million images), validation (50,000 images), and testing (100,000 images). Each image\n",
    "is (224 x 224) on three channels. The model achieves 7.5% top 5 error on ILSVRC-2012-val and\n",
    "7.4% top 5 error on ILSVRC-2012-test.\n",
    "\n",
    "\n",
    "The goal of this competition is to estimate the content of photographs for the purpose of retrieval and\n",
    "automatic annotation using a subset of the large hand-labeled ImageNet dataset (10 million labeled\n",
    "images depicting 10,000 + object categories) as training. Test images will be presented with no\n",
    "initial annotation—no segmentation or labels—and **algorithms will have to produce labelings\n",
    "specifying what objects are present in the images**.\n",
    "\n",
    "\n",
    "The weights learned by the model implemented in Caffe have been directly converted in Keras (for\n",
    "more information refer to: https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3) and can be used for\n",
    "preloading into the Keras model, which is implemented here as described in the paper.\n",
    "\n",
    "**Additional code** https://gist.github.com/nitish11/73ba862753929e08b3b319ff1e8c9c09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "import cv2, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a VGG16 network\n",
    "# 16 Layers\n",
    "def VGG_16(weights_path=None):\n",
    "    model = Sequential()\n",
    "    #-----------------------------------------------------\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))               # 1- layer\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))    \n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))               # 2- layer\n",
    "    \n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    #----------------------------------------------------- \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))              # 3- layer\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))              # 4- layer\n",
    "    \n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    #-----------------------------------------------------\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))              # 5- layer\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))              # 6- layer\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))              # 7- layer\n",
    "    \n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    #-----------------------------------------------------\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))              # 8- layer\n",
    "   \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))              # 9- layer\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))              # 10- layer\n",
    "    \n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    #-----------------------------------------------------\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))              # 11- layer\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))              # 12- layer\n",
    "    \n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))              # 13- layer\n",
    "    \n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    #-----------------------------------------------------\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #top layer of the VGG net\n",
    "    model.add(Dense(4096, activation='relu'))                     # 14- layer\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))                     # 15- layer\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='softmax'))                  # 16- layer\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Image and Prediciting\n",
    "For obtain the witghs of the model you can search in google for:\n",
    "        - download vgg16_weights.h5  (https://drive.google.com/uc?id=0Bz7KyqmuGsilT0J5dmRCM0ROVHc&export=download)\n",
    "\n",
    "**Note:** This file have 528 MB\n",
    "\n",
    "**Tip:** For checking the meaning of the resulted number after run the prediction, please consult the website:\n",
    "\n",
    "        https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    im = cv2.resize(cv2.imread('cat-standing.jpg'), (224, 224)).astype(np.float32)\n",
    "    im = im.transpose((2,0,1))\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    K.set_image_dim_ordering(\"th\")\n",
    "    \n",
    "    # Test pretrained model\n",
    "    model = VGG_16('vgg16_weights.h5')\n",
    "    optimizer = SGD()\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "    out = model.predict(im)\n",
    "    print np.argmax(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='brown'>Utilizing Keras built-in VGG-16 net module </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prebuild model with pre-trained weights on imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prebuild model with pre-trained weights on imagenet\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "# resize into VGG16 trained images' format\n",
    "im = cv2.resize(cv2.imread('steam-locomotive.jpg'), (224, 224))\n",
    "im = np.expand_dims(im, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "out = model.predict(im)\n",
    "plt.plot(out.ravel())\n",
    "plt.show()\n",
    "print np.argmax(out)\n",
    "#this should print 820 for steaming train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='brown'> Extracting features from an intermediate layer in a DCNN </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intermediate layer has the capability to\n",
    "extract important features from an image, and these features are more likely to help in different kinds\n",
    "of classification. **This has multiple advantages.**\n",
    "- First, we can rely on publicly available large-scale\n",
    "training and transfer this learning to novel domains. \n",
    "- Second, we can save time for expensive large\n",
    "training. \n",
    "- Third, we can provide reasonable solutions even when we don't have a large number of\n",
    "training examples for our domain. We also get a good starting network shape for the task at hand,\n",
    "instead of guessing it.\n",
    "\n",
    "Next is the code to implements the idea by extracting features from a specific layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-built and pre-trained deep learning VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=True)\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print (i, layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from block4_pool block\n",
    "model = Model(input=base_model.input, output=base_model.get_layer('block4_pool').output)\n",
    "img_path = 'cat-standing.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "# get the features from this block\n",
    "features = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='brown'> Every deep inception-v3 net used for transfer learning </font>\n",
    "\n",
    "Computer vision researchers now commonly use pre-trained CNNs to\n",
    "generate representations for novel tasks, where the dataset may not be large enough to train an entire\n",
    "CNN from scratch. Another common tactic is to take the pre-trained ImageNet network and then to\n",
    "fine-tune the entire network to the novel task.\n",
    "\n",
    "**Inception-v3** net is a very deep ConvNet **developed by Google**. The default input size for\n",
    "this model is 299 x 299 on three channels\n",
    "\n",
    "**For more info:** https://keras.io/applications/\n",
    "\n",
    "We suppose to have\n",
    "a training dataset D in a domain, different from ImageNet. D has 1,024 features in input and 200\n",
    "categories in output. Let us see a code fragment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the base pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a trained inception-v3; we do not include the top model because we want to fine-tune on D.\n",
    "The top level is a dense layer with 1,024 inputs and where the last output level is a softmax dense\n",
    "layer with 200 classes of output. **x = GlobalAveragePooling2D()(x)** is used to convert the input to the correct\n",
    "shape for the dense layer to handle. In fact, **base_model.output** tensor has the shape (samples, channels,\n",
    "rows, cols) for **dim_ordering=\"th\"** or (samples, rows, cols, channels) for **dim_ordering=\"tf\"** but dense needs\n",
    "them as (samples, channels) and GlobalAveragePooling2D averages across (rows, cols). So if you look at\n",
    "the last four layers (where include_top=True), you see these shapes:\n",
    "\n",
    "** layer.name, layer.input_shape, layer.output_shape**\n",
    "\n",
    "            ('mixed10', [(None, 8, 8, 320), (None, 8, 8, 768), (None, 8, 8, 768), (None, 8, 8, 192)], (None, 8, 8, 2048))\n",
    "            ('avg_pool', (None, 8, 8, 2048), (None, 1, 1, 2048))\n",
    "            ('flatten', (None, 1, 1, 2048), (None, 2048))\n",
    "            ('predictions', (None, 2048), (None, 1000))\n",
    "\n",
    "When you do include_top=False, you are removing the last three layers and exposing the mixed10 layer, so\n",
    "the GlobalAveragePooling2D layer converts the (None, 8, 8, 2048) to (None, 2048), where each element in\n",
    "the (None, 2048) tensor is the average value for each corresponding (8, 8) subtensor in the (None, 8,\n",
    "8, 2048) tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)# let's add a fully-connected layer as first layer\n",
    "x = Dense(1024, activation='relu')(x)# and a logistic layer with 200 classes as last layer\n",
    "predictions = Dense(200, activation='softmax')(x)# model to train\n",
    "model = Model(input=base_model.input, output=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the convolutional levels are pre-trained, so we freeze them during the training of the full model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that is, freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers: layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is then compiled and trained for a few epochs so that the top layers are trained:\n",
    "    \n",
    "            # compile the model (should be done *after* setting layers to non-trainable)\n",
    "            model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "            # train the model on the new data for a few epochs model.fit_generator(...)\n",
    "\n",
    "Then we freeze the top layers in inception and fine-tune some inception layer. In this example, we\n",
    "decide to freeze the first 172 layers (an hyperparameter to tune):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we chose to train the top 2 inception blocks, that is, we will freeze\n",
    "# the first 172 layers and unfreeze the rest:\n",
    "for layer in model.layers[:172]: layer.trainable = False\n",
    "for layer in model.layers[172:]: layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is then recompiled for fine-tune optimization. We need to recompile the model for these\n",
    "modifications to take effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers\n",
    "import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks)\n",
    "# alongside the top Dense layers\n",
    "model.fit_generator(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a new deep network that reuses the standard Inception-v3 network, but it is trained on a\n",
    "new domain D via transfer learning. Of course, there are many parameters to fine-tune for achieving\n",
    "good accuracy. However, we are now reusing a very large pre-trained network as a starting point via\n",
    "transfer learning. In doing so, we can save the need to train on our machines by reusing what is\n",
    "already available in Keras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
