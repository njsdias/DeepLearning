{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look up embeddings from pre-trained\n",
    "Our final strategy is to look up embeddings from pre-trained networks. The simplest way to do this\n",
    "with the current examples **is to just set the _trainable_ parameter of the embedding layer to False**. This\n",
    "ensures that backpropagation will not update the weights on the embedding layer:\n",
    "\n",
    "        model.add(Embedding(vocab_sz, EMBED_SIZE, input_length=maxlen,\n",
    "                            weights=[embedding_weights],\n",
    "                            trainable=False))\n",
    "        \n",
    "        model.add(SpatialDropout1D(Dropout(0.2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in general, this is not how you would use pre-trained embeddings in your code. Typically,\n",
    "it involves:\n",
    "- preprocessing your dataset to create word vectors by looking up words in one of the pretrained\n",
    "models\n",
    "- using this data to train some other model. \n",
    "\n",
    "The second model would not contain an Embedding layer, and may not even be a deep learning network.\n",
    "\n",
    "The following example describes a dense network that takes as its input a vector of size 100,\n",
    "representing a sentence, and outputs a 1 or 0 for positive or negative sentiment. Our dataset is still the\n",
    "one from the UMICH S1650 sentiment classification competition with around 7,000 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We begin with the imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from gensim.models import KeyedVectors                                       #for word2vec\n",
    "from keras.layers.core import Dense, SpatialDropout1D, Dropout        \n",
    "#from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "#from keras.layers.pooling import GlobalMaxPooling1D                          #for word2vec\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "#import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np     \n",
    "import codecs\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from time import gmtime, strftime\n",
    "import datetime, os\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the random seed for repeatability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create folder to TensroBoard save the graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"lookupglove{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**set some constant values**\n",
    "\n",
    "In order to create the 100-dimensional vectors for each sentence, we add up the **GloVe 100-dimensional\n",
    "vectors** for the words in the sentence, so we choose the glove.6B.100d.txt file \n",
    "(https://www.kaggle.com/terenceliu4444/glove6b100dtxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"data/umich-sentiment-train.txt\"\n",
    "GLOVE_MODEL = \"data/glove.6B.100d.txt\"\n",
    "VOCAB_SIZE = 5000\n",
    "EMBED_SIZE = 100\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block reads the sentences and creates a word frequency table. From this, the most common\n",
    "5000 tokens are selected and lookup tables (from word to word index and back) are created. In\n",
    "addition, we create a pseudo-token _UNK_ for tokens that do not exist in the vocabulary. Using these\n",
    "lookup tables, we convert each sentence to a sequence of word IDs, padding these sequences so that\n",
    "all sequences are of the same length (the maximum number of words in a sentence in the training set).\n",
    "We also convert the labels to categorical format:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading data**: reads the sentences and creates a word frequency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "fin = codecs.open(INPUT_FILE, \"r\", encoding='utf-8')\n",
    "maxlen = 0\n",
    "for line in fin:\n",
    "    _, sent = line.strip().split(\"\\t\")\n",
    "    words = [x.lower() for x in nltk.word_tokenize(sent)]   # lower case of words\n",
    "    if len(words) > maxlen:\n",
    "        maxlen = len(words)                                 # We pad each of our sentences to predetermined \n",
    "                                                            # length maxlen (in this case the number of words in the\n",
    "                                                            # longest sentence in the training set)\n",
    "    for word in words:\n",
    "        counter[word] += 1\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**creating vocabulary**: the most common\n",
    "5000 tokens are selected and lookup tables (from word to word index and back) are created. In\n",
    "addition, we create a pseudo-token _UNK_ for tokens that do not exist in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = collections.defaultdict(int)\n",
    "for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\n",
    "    word2index[word[0]] = wid + 1\n",
    "vocab_sz = len(word2index) + 1\n",
    "index2word = {v: k for k, v in word2index.items()}\n",
    "index2word[0] = \"_UNK_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**creating word sequences**:  Using these\n",
    "lookup tables, we convert each sentence to a sequence of word IDs, padding these sequences so that\n",
    "all sequences are of the same length (the maximum number of words in a sentence in the training set).\n",
    "We also convert the labels to categorical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws, ys = [], []\n",
    "fin = codecs.open(INPUT_FILE, \"r\", encoding='utf-8')\n",
    "for line in fin:\n",
    "    label, sent = line.strip().split(\"\\t\")\n",
    "    ys.append(int(label))\n",
    "    words = [x.lower() for x in nltk.word_tokenize(sent)]\n",
    "    wids = [word2index[word] for word in words]\n",
    "    ws.append(wids)\n",
    "fin.close()\n",
    "W = pad_sequences(ws, maxlen=maxlen)\n",
    "Y = np_utils.to_categorical(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the GloVe vectors into a dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# for word2vec\\nword2index = collections.defaultdict(int)\\nfor wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\\n    word2index[word[0]] = wid + 1\\nvocab_sz = len(word2index) + 1\\nindex2word = {v: k for k, v in word2index.items()}\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2emb = collections.defaultdict(int)\n",
    "fglove = open(GLOVE_MODEL, \"rb\")\n",
    "for line in fglove:\n",
    "    cols = line.strip().split()\n",
    "    word = cols[0].decode('utf-8')\n",
    "    embedding = np.array(cols[1:], dtype=\"float32\")\n",
    "    word2emb[word] = embedding\n",
    "fglove.close()\n",
    "\n",
    "\"\"\"\n",
    "# for word2vec\n",
    "word2index = collections.defaultdict(int)\n",
    "for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\n",
    "    word2index[word[0]] = wid + 1\n",
    "vocab_sz = len(word2index) + 1\n",
    "index2word = {v: k for k, v in word2index.items()}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tranfering Embeddings**:\n",
    "The next block looks up the words for each sentence from the word ID matrix _W_ and populates a\n",
    "matrix _E_ with the corresponding embedding vector. These embedding vectors are then added to create\n",
    "a sentence vector, which is written back into the _X_ matrix. The output of this code block is the matrix _X_\n",
    "of size (_num_records_ and _EMBED_SIZE_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# for word2vec\\nxs, ys = [], []\\nfin = codecs.open(INPUT_FILE, \"r\", encoding=\\'utf-8\\')\\nfor line in fin:\\n    label, sent = line.strip().split(\"\\t\")\\n    ys.append(int(label))\\n    words = [x.lower() for x in nltk.word_tokenize(sent)]\\n    wids = [word2index[word] for word in words]\\n    xs.append(wids)\\nfin.close()\\nX = pad_sequences(xs, maxlen=maxlen)\\nY = np_utils.to_categorical(ys)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.zeros((W.shape[0], EMBED_SIZE))\n",
    "for i in range(W.shape[0]):\n",
    "    E = np.zeros((EMBED_SIZE, maxlen))\n",
    "    words = [index2word[wid] for wid in W[i].tolist()]\n",
    "    for j in range(maxlen):\n",
    "        E[:, j] = word2emb[words[j]]    \n",
    "    X[i, :] = np.sum(E, axis=1)\n",
    "\n",
    "\"\"\"\n",
    "# for word2vec\n",
    "xs, ys = [], []\n",
    "fin = codecs.open(INPUT_FILE, \"r\", encoding='utf-8')\n",
    "for line in fin:\n",
    "    label, sent = line.strip().split(\"\\t\")\n",
    "    ys.append(int(label))\n",
    "    words = [x.lower() for x in nltk.word_tokenize(sent)]\n",
    "    wids = [word2index[word] for word in words]\n",
    "    xs.append(wids)\n",
    "fin.close()\n",
    "X = pad_sequences(xs, maxlen=maxlen)\n",
    "Y = np_utils.to_categorical(ys)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**split the data into 70/30**:We have now preprocessed our data using the pre-trained model and are ready to use it to train and evaluate our final model. Let us split the data into 70/30 training/test as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network Model:** The network we will train for doing the sentiment analysis task is a simple dense network. We compile it with a categorical cross-entropy loss function and the Adam optimizer, and train it with the\n",
    "sentence vectors that we built out of the pre-trained embeddings. Finally, we evaluate the model on\n",
    "the 30% test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=EMBED_SIZE, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compline CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the Trained Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4960 samples, validate on 2126 samples\n",
      "Epoch 1/10\n",
      "4960/4960 [==============================] - 0s 32us/step - loss: 0.0601 - acc: 0.9812 - val_loss: 0.0938 - val_acc: 0.9666\n",
      "Epoch 2/10\n",
      "4960/4960 [==============================] - 0s 32us/step - loss: 0.0571 - acc: 0.9812 - val_loss: 0.0870 - val_acc: 0.9722\n",
      "Epoch 3/10\n",
      "4960/4960 [==============================] - 0s 34us/step - loss: 0.0575 - acc: 0.9819 - val_loss: 0.0854 - val_acc: 0.9713\n",
      "Epoch 4/10\n",
      "4960/4960 [==============================] - 0s 33us/step - loss: 0.0466 - acc: 0.9845 - val_loss: 0.0851 - val_acc: 0.9718\n",
      "Epoch 5/10\n",
      "4960/4960 [==============================] - 0s 35us/step - loss: 0.0496 - acc: 0.9837 - val_loss: 0.0802 - val_acc: 0.9746\n",
      "Epoch 6/10\n",
      "4960/4960 [==============================] - 0s 34us/step - loss: 0.0474 - acc: 0.9845 - val_loss: 0.0875 - val_acc: 0.9699\n",
      "Epoch 7/10\n",
      "4960/4960 [==============================] - 0s 40us/step - loss: 0.0465 - acc: 0.9843 - val_loss: 0.0813 - val_acc: 0.9732\n",
      "Epoch 8/10\n",
      "4960/4960 [==============================] - 0s 39us/step - loss: 0.0474 - acc: 0.9855 - val_loss: 0.0964 - val_acc: 0.9699\n",
      "Epoch 9/10\n",
      "4960/4960 [==============================] - 0s 65us/step - loss: 0.0456 - acc: 0.9847 - val_loss: 0.0777 - val_acc: 0.9765\n",
      "Epoch 10/10\n",
      "4960/4960 [==============================] - 0s 34us/step - loss: 0.0460 - acc: 0.9829 - val_loss: 0.0878 - val_acc: 0.9722\n",
      "2126/2126 [==============================] - 0s 22us/step\n",
      "Test score: 0.088, accuracy: 0.972\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    callbacks=[tensorboard],\n",
    "                    validation_data=(Xtest, Ytest))\n",
    "\n",
    "# evaluate model\n",
    "score = model.evaluate(Xtest, Ytest, verbose=1)\n",
    "print(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation and Accuracy Plots**\n",
    "<img src=\"LookupPreTainGlove1.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure of the Neural Network Model**\n",
    "<img src=\"LookupPreTainGlove2.JPG\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
