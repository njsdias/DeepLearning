{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning learned embeddings from word2vec\n",
    "\n",
    "We will use the same network as the one we used to learn our embeddings from\n",
    "scratch. In terms of code, the only major difference is an extra block of code to load the word2vec\n",
    "model and build up the weight matrix for the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from keras.layers.core import Dense, SpatialDropout1D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from time import gmtime, strftime\n",
    "import datetime, os\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create folder to TensroBoard save the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"word2vecEmb{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is setting up the constants. The only difference here is that we reduced the NUM_EPOCHS setting\n",
    "from 20 to 10. **Recall that** initializing the matrix with values from a pre-trained model tends to set them\n",
    "to good values that converge faster.\n",
    "\n",
    "**Download file** : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"data/umich-sentiment-train.txt\"\n",
    "WORD2VEC_MODEL = \"data/GoogleNews-vectors-negative300.bin.gz\"                    #this file have 1.5G\n",
    "VOCAB_SIZE = 5000\n",
    "EMBED_SIZE = 300\n",
    "NUM_FILTERS = 256\n",
    "NUM_WORDS = 3\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts the words from the dataset and creates a vocabulary of the most frequent terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "fin = codecs.open(INPUT_FILE, \"r\", encoding='utf-8')\n",
    "maxlen = 0\n",
    "for line in fin:\n",
    "    _, sent = line.strip().split(\"\\t\")\n",
    "    words = [x.lower() for x in nltk.word_tokenize(sent)]   # lower case of words\n",
    "    if len(words) > maxlen:\n",
    "        maxlen = len(words)                                 # We pad each of our sentences to predetermined \n",
    "                                                            # length maxlen (in this case the number of words in the\n",
    "                                                            # longest sentence in the training set)\n",
    "    for word in words:\n",
    "        counter[word] += 1\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parses the dataset again to create a list of padded word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = collections.defaultdict(int)\n",
    "for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\n",
    "    word2index[word[0]] = wid + 1\n",
    "# Adding one because UNK.\n",
    "# It means representing words that are not seen in the vocubulary\n",
    "vocab_sz = len(word2index) + 1\n",
    "index2word = {v: k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also converts the labels to categorical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "fin = codecs.open(INPUT_FILE, \"r\", encoding='utf-8')\n",
    "for line in fin:\n",
    "    label, sent = line.strip().split(\"\\t\")\n",
    "    ys.append(int(label))\n",
    "    words = [x.lower() for x in nltk.word_tokenize(sent)]\n",
    "    wids = [word2index[word] for word in words]\n",
    "    xs.append(wids)\n",
    "fin.close()\n",
    "X = pad_sequences(xs, maxlen=maxlen)\n",
    "Y = np_utils.to_categorical(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it splits the data into a training and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next block loads up the word2vec model from a pre-trained model.**\n",
    "\n",
    "This model is trained with about 10 billion words of Google News articles and has a vocabulary size of 3 million.\n",
    "\n",
    "The dimensions of the embedding_weights matrix is *vocab_sz* and *EMBED_SIZE*. The vocab_sz is one more than the\n",
    "maximum number of unique terms in the vocabulary, the additional pseudo-token _UNK_ representing\n",
    "words that are not seen in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec model\n",
    "# it takes a long time\n",
    "word2vec = KeyedVectors.load_word2vec_format(WORD2VEC_MODEL, binary=True)\n",
    "embedding_weights = np.zeros((vocab_sz, EMBED_SIZE))\n",
    "for word, index in word2index.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word2vec[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in this block from our previous example is that we initialize\n",
    "the weights of the embedding layer with the *embedding_weights* matrix we built in the previous block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ctw00071\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ctw00071\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_sz, \n",
    "                    EMBED_SIZE,\n",
    "                    input_length=maxlen,\n",
    "                    weights=[embedding_weights]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "model.add(Conv1D(filters=NUM_FILTERS,\n",
    "                 kernel_size=NUM_WORDS,\n",
    "                 activation=\"relu\"))\n",
    "\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compile our model with the categorical cross-entropy loss function and the Adam optimizer,\n",
    "and train the network with batch size 64 and for 10 epochs, then evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ctw00071\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\ctw00071\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 4960 samples, validate on 2126 samples\n",
      "Epoch 1/10\n",
      "4960/4960 [==============================] - 9s 2ms/step - loss: 0.1140 - acc: 0.9625 - val_loss: 0.0212 - val_acc: 0.9939\n",
      "Epoch 2/10\n",
      "4960/4960 [==============================] - 7s 1ms/step - loss: 0.0098 - acc: 0.9982 - val_loss: 0.0140 - val_acc: 0.9958\n",
      "Epoch 3/10\n",
      "4960/4960 [==============================] - 8s 2ms/step - loss: 0.0047 - acc: 0.9990 - val_loss: 0.0142 - val_acc: 0.9939\n",
      "Epoch 4/10\n",
      "4960/4960 [==============================] - 8s 2ms/step - loss: 0.0022 - acc: 0.9996 - val_loss: 0.0127 - val_acc: 0.9967\n",
      "Epoch 5/10\n",
      "4960/4960 [==============================] - 9s 2ms/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.0136 - val_acc: 0.9962\n",
      "Epoch 6/10\n",
      "4960/4960 [==============================] - 8s 2ms/step - loss: 0.0014 - acc: 0.9998 - val_loss: 0.0126 - val_acc: 0.9958\n",
      "Epoch 7/10\n",
      "4960/4960 [==============================] - 8s 2ms/step - loss: 0.0023 - acc: 0.9996 - val_loss: 0.0130 - val_acc: 0.9958\n",
      "Epoch 8/10\n",
      "4960/4960 [==============================] - 8s 2ms/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.0154 - val_acc: 0.9967\n",
      "Epoch 9/10\n",
      "4960/4960 [==============================] - 8s 2ms/step - loss: 0.0022 - acc: 0.9996 - val_loss: 0.0158 - val_acc: 0.9972\n",
      "Epoch 10/10\n",
      "4960/4960 [==============================] - 8s 2ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0143 - val_acc: 0.9962\n",
      "2126/2126 [==============================] - 1s 436us/step\n",
      "Test score: 0.014, accuracy: 0.996\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", \n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    callbacks=[tensorboard],\n",
    "                    validation_data=(Xtest, Ytest))\n",
    "\n",
    "# evaluate model\n",
    "score = model.evaluate(Xtest, Ytest, verbose=1)\n",
    "print(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation and Accuracy Plots**\n",
    "<img src=\"tensorembed1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure of the Neural Network Model**\n",
    "<img src=\"tensorembed2.JPG\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
