{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using third-party implementations of word2vec\n",
    "\n",
    "The third-party implementations of word2vec are readily available.\n",
    "\n",
    "The gensim library provides an implementation of word2vec. Because Keras does not provide any support for\n",
    "word2vec, and integrating the gensim implementation into Keras code is very common practice.\n",
    "\n",
    "**To install gensim:** \n",
    "        -> pip install --upgrade gensim \n",
    "        -> conda install -c conda-forge gensim\n",
    "        \n",
    "**Dataset:** http://mattmahoney.net/dc/text8.zip \n",
    "\n",
    "The text8 corpus is a file containing about 17 million words derived from Wikipedia text.\n",
    "Wikipedia text was cleaned to remove markup, punctuation, and non-ASCII text, and the first 100 million characters of this cleaned text became the text8 corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read words from text8 and split up the words into senetences of 50 words each. The gensim library provides a built-in text8 handler that does something similar. Since we want\n",
    "to illustrate how to generate a model with any (preferably large) corpus that may or may not fit into\n",
    "memory, we will show you how to generate these sentences using a Python generator.\n",
    "\n",
    "In this case,\n",
    "we do ingest the entire file into memory, but when traversing through directories of files, generators\n",
    "allows us to load parts of the data into memory at a time, process them, and yield them to the caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text8Sentences(object):\n",
    "    def __init__(self, fname, maxlen):\n",
    "        self.fname = fname\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(os.path.join(DATA_DIR, \"text8\"), \"r\") as ftext:\n",
    "            text = ftext.read().split(\" \")\n",
    "            words = []\n",
    "            for word in text:\n",
    "                if len(words) >= self.maxlen:\n",
    "                    yield words\n",
    "                    words = []\n",
    "                words.append(word)\n",
    "            yield words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python logging to report on progress\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "    level=logging.INFO)\n",
    "\n",
    "DATA_DIR = \"data/\"\n",
    "MODEL_NAME = \"word2vec_text8\"\n",
    "model_file = Path(MODEL_NAME)\n",
    "sentences = Text8Sentences(os.path.join(DATA_DIR, \"text8\"), 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next lines trains the model with the sentences from the dataset.\n",
    "We have chosen the size of the **embedding vectors to be 300**, and we only \n",
    "consider **words that appear a minimum of 30 times** in the corpus.\n",
    "\n",
    "The default **window size is 5** which means that it consider the $w_{(i-5)}$ to $w_{(i+5)}$.\n",
    "\n",
    "By default, the word2vec model created is CBOW, but you\n",
    "can change that by setting sg=1 (skip-gram) in the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-01 02:03:34,498 : INFO : collecting all words and their counts\n",
      "2019-05-01 02:03:36,184 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-01 02:03:36,354 : INFO : PROGRESS: at sentence #10000, processed 500000 words, keeping 33464 word types\n",
      "2019-05-01 02:03:36,533 : INFO : PROGRESS: at sentence #20000, processed 1000000 words, keeping 52755 word types\n",
      "2019-05-01 02:03:36,706 : INFO : PROGRESS: at sentence #30000, processed 1500000 words, keeping 65589 word types\n",
      "2019-05-01 02:03:36,881 : INFO : PROGRESS: at sentence #40000, processed 2000000 words, keeping 78383 word types\n",
      "2019-05-01 02:03:37,065 : INFO : PROGRESS: at sentence #50000, processed 2500000 words, keeping 88008 word types\n",
      "2019-05-01 02:03:37,237 : INFO : PROGRESS: at sentence #60000, processed 3000000 words, keeping 96645 word types\n",
      "2019-05-01 02:03:37,413 : INFO : PROGRESS: at sentence #70000, processed 3500000 words, keeping 104309 word types\n",
      "2019-05-01 02:03:37,588 : INFO : PROGRESS: at sentence #80000, processed 4000000 words, keeping 111461 word types\n",
      "2019-05-01 02:03:37,759 : INFO : PROGRESS: at sentence #90000, processed 4500000 words, keeping 118752 word types\n",
      "2019-05-01 02:03:37,936 : INFO : PROGRESS: at sentence #100000, processed 5000000 words, keeping 125355 word types\n",
      "2019-05-01 02:03:38,110 : INFO : PROGRESS: at sentence #110000, processed 5500000 words, keeping 133141 word types\n",
      "2019-05-01 02:03:38,287 : INFO : PROGRESS: at sentence #120000, processed 6000000 words, keeping 139566 word types\n",
      "2019-05-01 02:03:38,463 : INFO : PROGRESS: at sentence #130000, processed 6500000 words, keeping 145782 word types\n",
      "2019-05-01 02:03:38,639 : INFO : PROGRESS: at sentence #140000, processed 7000000 words, keeping 151934 word types\n",
      "2019-05-01 02:03:38,822 : INFO : PROGRESS: at sentence #150000, processed 7500000 words, keeping 158046 word types\n",
      "2019-05-01 02:03:38,998 : INFO : PROGRESS: at sentence #160000, processed 8000000 words, keeping 164115 word types\n",
      "2019-05-01 02:03:39,175 : INFO : PROGRESS: at sentence #170000, processed 8500000 words, keeping 171256 word types\n",
      "2019-05-01 02:03:39,362 : INFO : PROGRESS: at sentence #180000, processed 9000000 words, keeping 178163 word types\n",
      "2019-05-01 02:03:39,540 : INFO : PROGRESS: at sentence #190000, processed 9500000 words, keeping 184129 word types\n",
      "2019-05-01 02:03:39,714 : INFO : PROGRESS: at sentence #200000, processed 10000000 words, keeping 189075 word types\n",
      "2019-05-01 02:03:39,890 : INFO : PROGRESS: at sentence #210000, processed 10500000 words, keeping 194511 word types\n",
      "2019-05-01 02:03:40,070 : INFO : PROGRESS: at sentence #220000, processed 11000000 words, keeping 198758 word types\n",
      "2019-05-01 02:03:40,243 : INFO : PROGRESS: at sentence #230000, processed 11500000 words, keeping 203441 word types\n",
      "2019-05-01 02:03:40,429 : INFO : PROGRESS: at sentence #240000, processed 12000000 words, keeping 207895 word types\n",
      "2019-05-01 02:03:40,631 : INFO : PROGRESS: at sentence #250000, processed 12500000 words, keeping 212668 word types\n",
      "2019-05-01 02:03:40,892 : INFO : PROGRESS: at sentence #260000, processed 13000000 words, keeping 217128 word types\n",
      "2019-05-01 02:03:41,144 : INFO : PROGRESS: at sentence #270000, processed 13500000 words, keeping 221416 word types\n",
      "2019-05-01 02:03:41,395 : INFO : PROGRESS: at sentence #280000, processed 14000000 words, keeping 226855 word types\n",
      "2019-05-01 02:03:41,635 : INFO : PROGRESS: at sentence #290000, processed 14500000 words, keeping 231424 word types\n",
      "2019-05-01 02:03:41,875 : INFO : PROGRESS: at sentence #300000, processed 15000000 words, keeping 237391 word types\n",
      "2019-05-01 02:03:42,071 : INFO : PROGRESS: at sentence #310000, processed 15500000 words, keeping 241697 word types\n",
      "2019-05-01 02:03:42,248 : INFO : PROGRESS: at sentence #320000, processed 16000000 words, keeping 245649 word types\n",
      "2019-05-01 02:03:42,429 : INFO : PROGRESS: at sentence #330000, processed 16500000 words, keeping 249621 word types\n",
      "2019-05-01 02:03:42,610 : INFO : PROGRESS: at sentence #340000, processed 17000000 words, keeping 253834 word types\n",
      "2019-05-01 02:03:42,845 : INFO : collected 253855 word types from a corpus of 17005208 raw words and 340105 sentences\n",
      "2019-05-01 02:03:42,847 : INFO : Loading a fresh vocabulary\n",
      "2019-05-01 02:03:42,981 : INFO : effective_min_count=30 retains 25097 unique words (9% of original 253855, drops 228758)\n",
      "2019-05-01 02:03:42,982 : INFO : effective_min_count=30 leaves 16191060 word corpus (95% of original 17005208, drops 814148)\n",
      "2019-05-01 02:03:43,075 : INFO : deleting the raw counts dictionary of 253855 items\n",
      "2019-05-01 02:03:43,224 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2019-05-01 02:03:43,225 : INFO : downsampling leaves estimated 11928484 word corpus (73.7% of prior 16191060)\n",
      "2019-05-01 02:03:43,317 : INFO : estimated required memory for 25097 words and 300 dimensions: 72781300 bytes\n",
      "2019-05-01 02:03:43,318 : INFO : resetting layer weights\n",
      "2019-05-01 02:03:43,652 : INFO : training model with 3 workers on 25097 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-01 02:03:45,463 : INFO : EPOCH 1 - PROGRESS: at 0.06% examples, 4127 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:03:46,466 : INFO : EPOCH 1 - PROGRESS: at 6.23% examples, 262576 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:03:47,474 : INFO : EPOCH 1 - PROGRESS: at 12.35% examples, 382264 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:03:48,474 : INFO : EPOCH 1 - PROGRESS: at 18.41% examples, 453139 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:03:49,478 : INFO : EPOCH 1 - PROGRESS: at 23.99% examples, 490498 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:03:50,479 : INFO : EPOCH 1 - PROGRESS: at 27.99% examples, 489051 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-01 02:03:51,480 : INFO : EPOCH 1 - PROGRESS: at 32.11% examples, 489859 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:03:52,495 : INFO : EPOCH 1 - PROGRESS: at 36.22% examples, 489984 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-01 02:03:53,507 : INFO : EPOCH 1 - PROGRESS: at 40.28% examples, 488672 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-01 02:03:54,513 : INFO : EPOCH 1 - PROGRESS: at 44.40% examples, 488852 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:03:55,521 : INFO : EPOCH 1 - PROGRESS: at 50.46% examples, 508539 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-01 02:03:56,531 : INFO : EPOCH 1 - PROGRESS: at 56.63% examples, 526063 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:03:57,538 : INFO : EPOCH 1 - PROGRESS: at 60.75% examples, 523343 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:03:58,545 : INFO : EPOCH 1 - PROGRESS: at 64.74% examples, 520055 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:03:59,547 : INFO : EPOCH 1 - PROGRESS: at 69.80% examples, 525302 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:00,549 : INFO : EPOCH 1 - PROGRESS: at 75.74% examples, 535868 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:01,557 : INFO : EPOCH 1 - PROGRESS: at 81.45% examples, 543135 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:02,573 : INFO : EPOCH 1 - PROGRESS: at 86.91% examples, 548308 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-01 02:04:03,582 : INFO : EPOCH 1 - PROGRESS: at 91.62% examples, 548771 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:04,587 : INFO : EPOCH 1 - PROGRESS: at 96.62% examples, 550709 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:05,504 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-01 02:04:05,511 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-01 02:04:05,516 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-01 02:04:05,517 : INFO : EPOCH - 1 : training on 17005208 raw words (11929253 effective words) took 21.9s, 545680 effective words/s\n",
      "2019-05-01 02:04:07,406 : INFO : EPOCH 2 - PROGRESS: at 0.06% examples, 3967 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:08,408 : INFO : EPOCH 2 - PROGRESS: at 5.76% examples, 236959 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:09,418 : INFO : EPOCH 2 - PROGRESS: at 11.00% examples, 333035 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:10,428 : INFO : EPOCH 2 - PROGRESS: at 16.70% examples, 403331 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-01 02:04:11,431 : INFO : EPOCH 2 - PROGRESS: at 22.11% examples, 444121 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:12,434 : INFO : EPOCH 2 - PROGRESS: at 27.93% examples, 481645 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:13,449 : INFO : EPOCH 2 - PROGRESS: at 33.64% examples, 506788 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-01 02:04:14,458 : INFO : EPOCH 2 - PROGRESS: at 38.93% examples, 520693 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:15,459 : INFO : EPOCH 2 - PROGRESS: at 44.69% examples, 537726 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:16,470 : INFO : EPOCH 2 - PROGRESS: at 50.46% examples, 551156 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:17,475 : INFO : EPOCH 2 - PROGRESS: at 56.22% examples, 562664 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:18,481 : INFO : EPOCH 2 - PROGRESS: at 62.27% examples, 574712 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:19,491 : INFO : EPOCH 2 - PROGRESS: at 67.45% examples, 577412 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:20,500 : INFO : EPOCH 2 - PROGRESS: at 72.80% examples, 581526 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:21,515 : INFO : EPOCH 2 - PROGRESS: at 77.98% examples, 582195 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-01 02:04:22,520 : INFO : EPOCH 2 - PROGRESS: at 83.86% examples, 588882 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-01 02:04:23,530 : INFO : EPOCH 2 - PROGRESS: at 89.33% examples, 592046 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:24,536 : INFO : EPOCH 2 - PROGRESS: at 95.38% examples, 598517 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:25,555 : INFO : EPOCH 2 - PROGRESS: at 99.68% examples, 593449 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-01 02:04:25,593 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-01 02:04:25,600 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-01 02:04:25,604 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-01 02:04:25,605 : INFO : EPOCH - 2 : training on 17005208 raw words (11928477 effective words) took 20.1s, 593961 effective words/s\n",
      "2019-05-01 02:04:27,419 : INFO : EPOCH 3 - PROGRESS: at 0.06% examples, 4114 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:28,425 : INFO : EPOCH 3 - PROGRESS: at 6.00% examples, 252257 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:29,430 : INFO : EPOCH 3 - PROGRESS: at 11.88% examples, 367001 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-01 02:04:30,432 : INFO : EPOCH 3 - PROGRESS: at 17.64% examples, 433533 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:31,435 : INFO : EPOCH 3 - PROGRESS: at 23.76% examples, 484958 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:32,437 : INFO : EPOCH 3 - PROGRESS: at 29.70% examples, 518676 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:33,438 : INFO : EPOCH 3 - PROGRESS: at 35.58% examples, 542807 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:34,440 : INFO : EPOCH 3 - PROGRESS: at 41.52% examples, 561617 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:35,442 : INFO : EPOCH 3 - PROGRESS: at 47.63% examples, 579113 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:36,450 : INFO : EPOCH 3 - PROGRESS: at 53.92% examples, 594695 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:37,451 : INFO : EPOCH 3 - PROGRESS: at 60.04% examples, 606255 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:38,453 : INFO : EPOCH 3 - PROGRESS: at 66.21% examples, 616433 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-01 02:04:39,462 : INFO : EPOCH 3 - PROGRESS: at 72.27% examples, 623987 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-01 02:04:40,470 : INFO : EPOCH 3 - PROGRESS: at 78.33% examples, 629287 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:41,480 : INFO : EPOCH 3 - PROGRESS: at 84.39% examples, 634617 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:42,481 : INFO : EPOCH 3 - PROGRESS: at 89.91% examples, 636019 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:43,496 : INFO : EPOCH 3 - PROGRESS: at 96.03% examples, 640526 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-01 02:04:44,456 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-01 02:04:44,462 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-01 02:04:44,464 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-01 02:04:44,465 : INFO : EPOCH - 3 : training on 17005208 raw words (11928087 effective words) took 18.9s, 632576 effective words/s\n",
      "2019-05-01 02:04:46,287 : INFO : EPOCH 4 - PROGRESS: at 0.06% examples, 4097 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:47,288 : INFO : EPOCH 4 - PROGRESS: at 5.94% examples, 249637 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:48,294 : INFO : EPOCH 4 - PROGRESS: at 11.06% examples, 340598 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:49,296 : INFO : EPOCH 4 - PROGRESS: at 16.41% examples, 402273 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:50,304 : INFO : EPOCH 4 - PROGRESS: at 21.99% examples, 447115 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:51,305 : INFO : EPOCH 4 - PROGRESS: at 27.76% examples, 483508 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:52,308 : INFO : EPOCH 4 - PROGRESS: at 33.87% examples, 515912 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:53,315 : INFO : EPOCH 4 - PROGRESS: at 40.11% examples, 541605 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:54,322 : INFO : EPOCH 4 - PROGRESS: at 46.22% examples, 560705 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:55,323 : INFO : EPOCH 4 - PROGRESS: at 51.93% examples, 571837 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:56,332 : INFO : EPOCH 4 - PROGRESS: at 57.75% examples, 582062 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:04:57,336 : INFO : EPOCH 4 - PROGRESS: at 63.63% examples, 591337 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-01 02:04:58,346 : INFO : EPOCH 4 - PROGRESS: at 69.21% examples, 596442 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-01 02:04:59,351 : INFO : EPOCH 4 - PROGRESS: at 74.98% examples, 602598 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:00,367 : INFO : EPOCH 4 - PROGRESS: at 80.15% examples, 601860 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:01,379 : INFO : EPOCH 4 - PROGRESS: at 84.74% examples, 598106 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:02,392 : INFO : EPOCH 4 - PROGRESS: at 89.44% examples, 595550 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:03,392 : INFO : EPOCH 4 - PROGRESS: at 95.32% examples, 600965 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:04,464 : INFO : EPOCH 4 - PROGRESS: at 99.50% examples, 593471 words/s, in_qsize 6, out_qsize 2\n",
      "2019-05-01 02:05:04,534 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-01 02:05:04,540 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-01 02:05:04,546 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-01 02:05:04,547 : INFO : EPOCH - 4 : training on 17005208 raw words (11928080 effective words) took 20.1s, 594082 effective words/s\n",
      "2019-05-01 02:05:06,494 : INFO : EPOCH 5 - PROGRESS: at 0.06% examples, 3816 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:07,500 : INFO : EPOCH 5 - PROGRESS: at 5.23% examples, 211446 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:08,501 : INFO : EPOCH 5 - PROGRESS: at 11.17% examples, 333439 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:09,503 : INFO : EPOCH 5 - PROGRESS: at 17.29% examples, 413804 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:10,507 : INFO : EPOCH 5 - PROGRESS: at 23.29% examples, 464654 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:11,517 : INFO : EPOCH 5 - PROGRESS: at 29.23% examples, 500171 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:12,523 : INFO : EPOCH 5 - PROGRESS: at 35.22% examples, 527898 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-01 02:05:13,527 : INFO : EPOCH 5 - PROGRESS: at 41.34% examples, 550220 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:14,529 : INFO : EPOCH 5 - PROGRESS: at 47.28% examples, 566512 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:15,535 : INFO : EPOCH 5 - PROGRESS: at 53.16% examples, 578598 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:16,538 : INFO : EPOCH 5 - PROGRESS: at 59.10% examples, 589603 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-01 02:05:17,543 : INFO : EPOCH 5 - PROGRESS: at 65.22% examples, 600269 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:18,545 : INFO : EPOCH 5 - PROGRESS: at 71.27% examples, 609287 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:19,551 : INFO : EPOCH 5 - PROGRESS: at 77.51% examples, 616967 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:20,555 : INFO : EPOCH 5 - PROGRESS: at 83.39% examples, 621809 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:21,582 : INFO : EPOCH 5 - PROGRESS: at 89.33% examples, 625922 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-01 02:05:22,595 : INFO : EPOCH 5 - PROGRESS: at 94.85% examples, 627154 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-01 02:05:23,683 : INFO : EPOCH 5 - PROGRESS: at 99.50% examples, 620284 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-01 02:05:23,748 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-01 02:05:23,755 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-01 02:05:23,758 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-01 02:05:23,758 : INFO : EPOCH - 5 : training on 17005208 raw words (11928979 effective words) took 19.2s, 621016 effective words/s\n",
      "2019-05-01 02:05:23,759 : INFO : training on a 85026040 raw words (59642876 effective words) took 100.1s, 595798 effective words/s\n"
     ]
    }
   ],
   "source": [
    "if model_file.is_file():\n",
    "    model = word2vec.Word2Vec.load(MODEL_NAME)\n",
    "else:\n",
    "    model = word2vec.Word2Vec(sentences, size=300, min_count=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find words that are most similar to a certain word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.most_similar(\"woman\")\n",
      "[('child', 0.7407524585723877), ('girl', 0.720534086227417), ('man', 0.6518779993057251), ('lover', 0.6452478766441345), ('prostitute', 0.6388713121414185), ('baby', 0.6201745867729187), ('herself', 0.6199727654457092), ('mother', 0.6185761094093323), ('person', 0.6185750365257263), ('lady', 0.6109557747840881)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n('child', 0.7407524585723877), \\n('girl', 0.720534086227417), (\\n'man', 0.6518779993057251), \\n('lover', 0.6452478766441345), \\n('prostitute', 0.6388713121414185), \\n('baby', 0.6201745867729187), \\n('herself', 0.6199727654457092), \\n('mother', 0.6185761094093323), \\n('person', 0.6185750365257263), \\n('lady', 0.6109557747840881)]\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\"\"model.most_similar(\"woman\")\"\"\")\n",
    "print(model.wv.most_similar(\"woman\"))\n",
    "\n",
    "\"\"\"\n",
    "('child', 0.7407524585723877), \n",
    "('girl', 0.720534086227417), (\n",
    "'man', 0.6518779993057251), \n",
    "('lover', 0.6452478766441345), \n",
    "('prostitute', 0.6388713121414185), \n",
    "('baby', 0.6201745867729187), \n",
    "('herself', 0.6199727654457092), \n",
    "('mother', 0.6185761094093323), \n",
    "('person', 0.6185750365257263), \n",
    "('lady', 0.6109557747840881)]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can provide hints for finding word similarity. For example, the following command returns the top\n",
    "10 words that are like woman and king but unlike man:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.most_similar(positive=[\"woman\", \"king\"],\n",
      "      negative=[\"man\"], topn=10)\n",
      "[('queen', 0.5695605874061584), ('isabella', 0.553027331829071), ('empress', 0.5448185205459595), ('princess', 0.5404852628707886), ('daughter', 0.5338696837425232), ('throne', 0.5229605436325073), ('elizabeth', 0.5115128755569458), ('pharaoh', 0.5077822804450989), ('prince', 0.5037112236022949), ('son', 0.5034430623054504)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n[('queen', 0.5695605874061584), \\n('isabella', 0.553027331829071), \\n('empress', 0.5448185205459595), \\n('princess', 0.5404852628707886), \\n('daughter', 0.5338696837425232), \\n('throne', 0.5229605436325073), \\n('elizabeth', 0.5115128755569458), \\n('pharaoh', 0.5077822804450989), (\\n'prince', 0.5037112236022949), \\n('son', 0.5034430623054504)]\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\"\"model.most_similar(positive=[\"woman\", \"king\"],\n",
    "      negative=[\"man\"], topn=10)\"\"\")\n",
    "print(model.wv.most_similar(positive=['woman', 'king'],\n",
    "                         negative=['man'],topn=10))\n",
    "\"\"\"\n",
    "[('queen', 0.5695605874061584), \n",
    "('isabella', 0.553027331829071), \n",
    "('empress', 0.5448185205459595), \n",
    "('princess', 0.5404852628707886), \n",
    "('daughter', 0.5338696837425232), \n",
    "('throne', 0.5229605436325073), \n",
    "('elizabeth', 0.5115128755569458), \n",
    "('pharaoh', 0.5077822804450989), (\n",
    "'prince', 0.5037112236022949), \n",
    "('son', 0.5034430623054504)]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find similarities between individual words. To give a feel of how the positions of the\n",
    "words in the embedding space correlates with their semantic meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72053397"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(\"girl\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59525347"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(\"girl\", \"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3213757"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(\"girl\", \"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4743592"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(\"bus\", \"car\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
