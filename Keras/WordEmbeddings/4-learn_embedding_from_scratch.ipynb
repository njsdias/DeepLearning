{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn embeddings from scratch\n",
    "\n",
    "In this example, we will train a **one-dimensional Convolutional Neural Network (CNN) to classify\n",
    "sentences as either positive or negative.** Words in sentences exhibit linear structure in the same way as images exhibit spatial structure.\n",
    "\n",
    "Traditional (non-deep learning) NLP approaches to language modeling involve creating word ngrams\n",
    "(https://en.wikipedia.org/wiki/N-gram) to exploit this linear structure inherent among words. **One-dimensional\n",
    "CNNs do something similar**, learning convolution filters that operate on sentences a few\n",
    "words at a time, and max pooling the results to create a vector that represents the most important\n",
    "ideas in the sentence. There is another class of neural network, called **Recurrent Neural Network (RNN)**, which is\n",
    "specially designed **to handle sequence data**, including text, which is a sequence of words. \n",
    "\n",
    "**Install NLTK (Natural Language Toolkit)** to parse the text into sentences and words. he statistical models supplied by NLTK are more powerful at parsing than regular expressions \n",
    "            \n",
    "    conda install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **sequence of word indices is fed into an array of embedding layers** of a set size (in our case, the\n",
    "number of words in the longest sentence). The embedding layer is initialized by default to random\n",
    "values. **The output of the embedding layer is connected to a 1D convolutional layer** that convolves (in\n",
    "our example) word trigrams in 256 different ways (essentially, it applies different learned linear\n",
    "combinations of weights on the word embeddings). These features are then pooled into a single\n",
    "pooled word by a global max pooling layer. **This vector (256) is then input to a dense layer**, which\n",
    "outputs a vector (2). **A softmax activation will return a pair of probabilities**, one corresponding to\n",
    "positive sentiment and another corresponding to negative sentiment. The network is shown in the\n",
    "following figure:\n",
    "<img src=\"CNN_Text.JPG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Dense, SpatialDropout1D, Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np     \n",
    "import codecs\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from time import gmtime, strftime\n",
    "import datetime, os\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For run TensorBoard, which display the graphs to evaluate de Neural Network Model:\n",
    "\n",
    "- 1: create a folder named as *logs* inside of your main folder\n",
    "- 2: run the next command in the terminal of your main folder:  *tensorboard --logdir=logs/*\n",
    "- 3: write the http address in new aba on your browser\n",
    "- 4: press enter in terminal: the TensorBoard window pop up in your browser\n",
    "\n",
    "Note: Wait until the number of epochs is 20 as defined in : NUM_EPOCHS = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"wordtext{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "we want consistent results between runs Since the initializations of \n",
    "the weight matrices are random, differences in initialization can lead\n",
    "to differences in output, so this is a way to control that\n",
    "\"\"\"\n",
    "np.random.seed(42)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will classify sentences from the **UMICH SI650 sentiment classification competition on Kaggle**. The dataset has around 7000 sentences, and is **labeled 1 for positive and 0 for negative**. The format of the file is a sentiment label (0 or 1) followed by a tab, followed by a sentence.\n",
    "\n",
    "**Download data from:** https://www.kaggle.com/c/si650winter11/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"data/umich-sentiment-train.txt\"\n",
    "VOCAB_SIZE = 5000\n",
    "EMBED_SIZE = 100\n",
    "NUM_FILTERS = 256\n",
    "NUM_WORDS = 3\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "fin = codecs.open(INPUT_FILE, \"r\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block, we first read our input sentences and construct our vocabulary out of the most\n",
    "frequent words in the corpus. We then use this vocabulary to convert our input sentences into a list of\n",
    "word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 0\n",
    "for line in fin:\n",
    "    _, sent = line.strip().split(\"\\t\")\n",
    "    words = [x.lower() for x in nltk.word_tokenize(sent)]   # lower case of words\n",
    "    if len(words) > maxlen:\n",
    "        maxlen = len(words)                                 # We pad each of our sentences to predetermined \n",
    "                                                            # length maxlen (in this case the number of words in the\n",
    "                                                            # longest sentence in the training set)\n",
    "    for word in words:\n",
    "        counter[word] += 1\n",
    "fin.close()\n",
    "\n",
    "word2index = collections.defaultdict(int)\n",
    "for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\n",
    "    word2index[word[0]] = wid + 1\n",
    "# Adding one because UNK.\n",
    "# It means representing words that are not seen in the vocubulary\n",
    "vocab_sz = len(word2index) + 1\n",
    "index2word = {v: k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "fin = codecs.open(INPUT_FILE, \"r\", encoding='utf-8')\n",
    "for line in fin:\n",
    "    label, sent = line.strip().split(\"\\t\")\n",
    "    ys.append(int(label))\n",
    "    words = [x.lower() for x in nltk.word_tokenize(sent)]\n",
    "    wids = [word2index[word] for word in words]\n",
    "    xs.append(wids)\n",
    "fin.close()\n",
    "X = pad_sequences(xs, maxlen=maxlen)\n",
    "Y = np_utils.to_categorical(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split up our data into a 70/30 training and test set. The data is now in a form ready to be\n",
    "fed into the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the network that we described earlier in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ctw00071\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ctw00071\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_sz, EMBED_SIZE, input_length=maxlen))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "model.add(Conv1D(filters=NUM_FILTERS,\n",
    "                 kernel_size=NUM_WORDS,\n",
    "                 activation=\"relu\"))\n",
    "\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the network gives us 99.98% accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4960 samples, validate on 2126 samples\n",
      "Epoch 1/20\n",
      "4960/4960 [==============================] - 4s 753us/step - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0217 - val_acc: 0.9929\n",
      "Epoch 2/20\n",
      "4960/4960 [==============================] - 3s 702us/step - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0212 - val_acc: 0.9939\n",
      "Epoch 3/20\n",
      "4960/4960 [==============================] - 3s 629us/step - loss: 0.0021 - acc: 0.9996 - val_loss: 0.0206 - val_acc: 0.9939\n",
      "Epoch 4/20\n",
      "4960/4960 [==============================] - 3s 624us/step - loss: 4.7868e-04 - acc: 0.9998 - val_loss: 0.0196 - val_acc: 0.9944\n",
      "Epoch 5/20\n",
      "4960/4960 [==============================] - 3s 678us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0210 - val_acc: 0.9953\n",
      "Epoch 6/20\n",
      "4960/4960 [==============================] - 3s 648us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0197 - val_acc: 0.9934\n",
      "Epoch 7/20\n",
      "4960/4960 [==============================] - 3s 670us/step - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0186 - val_acc: 0.9939\n",
      "Epoch 8/20\n",
      "4960/4960 [==============================] - 3s 678us/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0200 - val_acc: 0.9929\n",
      "Epoch 9/20\n",
      "4960/4960 [==============================] - 3s 584us/step - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0182 - val_acc: 0.9939\n",
      "Epoch 10/20\n",
      "4960/4960 [==============================] - 3s 584us/step - loss: 7.8039e-04 - acc: 0.9998 - val_loss: 0.0206 - val_acc: 0.9939\n",
      "Epoch 11/20\n",
      "4960/4960 [==============================] - 3s 585us/step - loss: 8.1929e-04 - acc: 0.9998 - val_loss: 0.0186 - val_acc: 0.9929\n",
      "Epoch 12/20\n",
      "4960/4960 [==============================] - 3s 581us/step - loss: 8.1319e-04 - acc: 0.9998 - val_loss: 0.0193 - val_acc: 0.9934\n",
      "Epoch 13/20\n",
      "4960/4960 [==============================] - 3s 580us/step - loss: 6.9607e-04 - acc: 0.9998 - val_loss: 0.0184 - val_acc: 0.9934\n",
      "Epoch 14/20\n",
      "4960/4960 [==============================] - 3s 588us/step - loss: 0.0011 - acc: 0.9996 - val_loss: 0.0182 - val_acc: 0.9934\n",
      "Epoch 15/20\n",
      "4960/4960 [==============================] - 3s 612us/step - loss: 6.4625e-04 - acc: 0.9996 - val_loss: 0.0186 - val_acc: 0.9939\n",
      "Epoch 16/20\n",
      "4960/4960 [==============================] - 3s 622us/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0197 - val_acc: 0.9934\n",
      "Epoch 17/20\n",
      "4960/4960 [==============================] - 3s 575us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0179 - val_acc: 0.9934\n",
      "Epoch 18/20\n",
      "4960/4960 [==============================] - 3s 600us/step - loss: 0.0010 - acc: 0.9996 - val_loss: 0.0176 - val_acc: 0.9934\n",
      "Epoch 19/20\n",
      "4960/4960 [==============================] - 3s 580us/step - loss: 7.9822e-04 - acc: 0.9998 - val_loss: 0.0192 - val_acc: 0.9939\n",
      "Epoch 20/20\n",
      "4960/4960 [==============================] - 3s 603us/step - loss: 6.7787e-04 - acc: 0.9998 - val_loss: 0.0184 - val_acc: 0.9929\n",
      "2126/2126 [==============================] - 0s 151us/step\n",
      "Test score: 0.018, accuracy: 0.993\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", \n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    callbacks=[tensorboard],\n",
    "                    validation_data=(Xtest, Ytest))\n",
    "\n",
    "# evaluate model\n",
    "score = model.evaluate(Xtest, Ytest, verbose=1)\n",
    "print(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation plots for Loss and Accuracy** \n",
    "<img src=\"tensorboard2.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure of the Neural Network Model**\n",
    "<img src=\"tensorboard3.JPG\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
