{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The skip-gram word2vec model\n",
    "The skip-gram model is trained to predict the surrounding words given the current word. To\n",
    "understand how the skip-gram word2vec model works, consider the following example sentence:\n",
    "\n",
    "    I love green eggs and ham.\n",
    "\n",
    "Assuming a window size of three, this sentence can be broken down into the following sets of\n",
    "(context, word) pairs:\n",
    "- [I, green], love)\n",
    "- [love, eggs], green)\n",
    "- [green, and], eggs)\n",
    "- ...\n",
    "\n",
    "Since the skip-gram model predicts a context word given the center word, we can convert the\n",
    "preceding dataset to one of (input, output) pairs. That is, given an input word, we expect the skipgram\n",
    "model to predict the output word:\n",
    "\n",
    "    (love, I), (love, green), (green, love), (green, eggs), (eggs, green), (eggs, and), ...\n",
    "\n",
    "We can also generate additional negative samples by pairing each input word with some random\n",
    "word in the vocabulary. For example:\n",
    "    \n",
    "    (love, Sam), (love, zebra), (green, thing), ...\n",
    "\n",
    "Finally, we generate positive and negative examples for our classifier:\n",
    "    \n",
    "    ((love, I), 1), ((love, green), 1), ..., ((love, Sam), 0), ((love, zebra), 0), ...\n",
    "\n",
    "We can now train a classifier that takes in a word vector and a context vector and learns to predict\n",
    "one or zero depending on whether it sees a positive or negative sample. The deliverables from this\n",
    "trained network are the weights of the word embedding layer (the gray box in the following figure):\n",
    "<img src=\"skip-gram.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.layers import Merge # out of date\n",
    "#from keras.layers import dot\n",
    "#from keras.layers.core import Dense, Reshape\n",
    "#from keras.layers.embeddings import Embedding\n",
    "#from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import Input, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000   # vocabulary size is set at 5000\n",
    "embed_size = 300    # output embedding size is 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to this model is the word ID in the\n",
    "vocabulary. The embedding weights are initially set to small random values. During training, the\n",
    "model will update these weights using backpropagation. The next layer reshapes the input to the\n",
    "embedding size."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "word_model = Sequential()\n",
    "\n",
    "word_model.add(Embedding(vocab_size, embed_size,\n",
    "                         embeddings_initializer=\"glorot_uniform\",\n",
    "                         input_length=1))                            # window size is 1\n",
    "\n",
    "word_model.add(Reshape((embed_size, )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other model that we need is a sequential model for the context words. For each of our skip-gram\n",
    "pairs, we have a single context word corresponding to the target word, so this model is identical to\n",
    "the word model:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "context_model = Sequential()\n",
    "\n",
    "context_model.add(Embedding(vocab_size, embed_size,\n",
    "                            embeddings_initializer=\"glorot_uniform\",\n",
    "                            input_length=1))\n",
    "\n",
    "context_model.add(Reshape((embed_size,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs of the two models are each a vector of size (embed_size). **These outputs are merged into one\n",
    "using a dot product and fed into a dense layer**, which has a single output wrapped in a sigmoid\n",
    "activation layer. The **Sigmoid Activation** modulates the output so numbers higher than 0.5 tend rapidly to 1\n",
    "and flatten out, and numbers lower than 0.5 tend rapidly to 0 and also flatten out.\n",
    "\n",
    "The next code lines don't work due to the deprecation of the *Merge API*:\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Merge([word_model, context_model], mode=\"dot\"))\n",
    "        model.add(Dense(1, init=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "\n",
    "The next lines of code is working translating the Sequential API of Keras - to the Functional API to solve the Merge API problem. For now the objective is merge two vectors to build a tensor using dot product.     \n",
    "        <img src=\"concatenate_keras2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Sequential API of Keras\n",
    "word_model = Sequential()\n",
    "word_model.add(Embedding(vocab_size, embed_size,\n",
    "                         embeddings_initializer=\"glorot_uniform\",\n",
    "                         input_length=1))                            # window size is 1\n",
    "\n",
    "word_model.add(Reshape((embed_size, )))\n",
    "\"\"\"\n",
    "\n",
    "# Functional API of Keras \n",
    "word_input = Input(shape=(1,))\n",
    "word_x = layers.Embedding(vocab_size, \n",
    "                          embed_size, \n",
    "                          embeddings_initializer='glorot_uniform')(word_input)\n",
    "word_reshape = layers.Reshape((embed_size,))(word_x)\n",
    "word_model = Model(word_input, word_reshape)    \n",
    "\n",
    "\"\"\"\n",
    "#Sequential API of Keras \n",
    "context_model = Sequential()\n",
    "context_model.add(Embedding(vocab_size, embed_size,\n",
    "                            embeddings_initializer=\"glorot_uniform\",\n",
    "                            input_length=1))\n",
    "\n",
    "context_model.add(Reshape((embed_size,)))\n",
    "\"\"\"\n",
    "# Functional API of Keras \n",
    "context_input = Input(shape=(1,))\n",
    "context_x = layers.Embedding(vocab_size, \n",
    "                             embed_size, \n",
    "                             embeddings_initializer='glorot_uniform')(context_input)\n",
    "context_reshape = layers.Reshape((embed_size,))(context_x)\n",
    "context_model = Model(context_input, context_reshape)\n",
    "\n",
    "\n",
    "dot_output = layers.dot([word_model.output, context_model.output], axes=1, normalize=False)\n",
    "model_output = layers.Dense(1, kernel_initializer='glorot_uniform', activation='sigmoid')(dot_output)\n",
    "model = Model([word_input, context_input], model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function used is the mean_squared_error; the idea is to minimize the dot product for positive\n",
    "examples and maximize it for negative examples. If you recall, the dot product multiplies\n",
    "corresponding elements of two vectors and sums up the resultâ€”this causes similar vectors to have\n",
    "higher dot products than dissimilar vectors, since the former has more overlapping elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_layer = model.layers[0]\n",
    "word_embed_layer = word_model.layers[0]\n",
    "weights = word_model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting skip-grams for a text that has been converted to a list of word indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 56\n",
      "(ham (6), i (1)) -> 0\n",
      "(eggs (4), love (2)) -> 0\n",
      "(ham (6), eggs (4)) -> 0\n",
      "(i (1), green (3)) -> 0\n",
      "(and (5), eggs (4)) -> 0\n",
      "(and (5), eggs (4)) -> 0\n",
      "(i (1), green (3)) -> 0\n",
      "(love (2), green (3)) -> 1\n",
      "(i (1), eggs (4)) -> 1\n",
      "(ham (6), green (3)) -> 1\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "text = \"I love green eggs and ham .\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring the tokenizer and run the text against it. This will produce a list of word tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer creates a dictionary mapping each unique word to an integer ID and makes it available\n",
    "in the word_index attribute. We extract this and create a two-way lookup table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = tokenizer.word_index\n",
    "id2word = {v: k for k, v in word2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we convert our input list of words to a list of IDs and pass it to the skipgrams function. We then\n",
    "print the first 10 of the 56 (pair, label) skip-gram tuples generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 56\n",
      "(love (2), and (5)) -> 0\n",
      "(eggs (4), ham (6)) -> 1\n",
      "(love (2), love (2)) -> 0\n",
      "(and (5), i (1)) -> 1\n",
      "(and (5), green (3)) -> 1\n",
      "(and (5), i (1)) -> 0\n",
      "(and (5), green (3)) -> 0\n",
      "(i (1), and (5)) -> 0\n",
      "(i (1), eggs (4)) -> 1\n",
      "(ham (6), and (5)) -> 1\n"
     ]
    }
   ],
   "source": [
    "wids = [word2id[w] for w in text_to_word_sequence(text)]\n",
    "pairs, labels = skipgrams(wids, len(word2id))\n",
    "print(len(pairs), len(labels))\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "        id2word[pairs[i][0]], pairs[i][0],\n",
    "        id2word[pairs[i][1]], pairs[i][1],\n",
    "labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of negative sampling, used for generating the negative examples, consists of\n",
    "randomly pairing up arbitrary tokens from the text. As the size of the input text increases, this is more\n",
    "likely to pick up unrelated word pairs. In our example, since our text is very short, there is a chance\n",
    "that it can end up generating positive examples as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
