{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Recurrent Neural Network  (RNN): Generating the next character or next word of text\n",
    "Used extensively for building language models. A language model allows us to\n",
    "**predict the probability of a word in a text given the previous words**. Language models are important\n",
    "for various higher level tasks such as machine translation, spelling correction, and so on.\n",
    "\n",
    "**A side effect** of the ability to predict the next word given previous words is a generative model that\n",
    "allows us **to generate text by sampling from the output probabilities.** In language modeling, our input\n",
    "is typically a sequence of words and the output is a sequence of predicted words. **The training data**\n",
    "used is existing **unlabeled text**, where we set the label $y_t$ at time $t$ to be the input $x_{t+1}$ at time $t+1$.\n",
    "\n",
    "For our first example of using Keras for building RNNs, **we will train a character based language\n",
    "model** on the text of \"Alice in Wonderland\" **to predict the next character given 10 previous characters.**\n",
    "We have chosen to build a character-based model here because it has a smaller vocabulary and trains\n",
    "quicker. The idea is the same as using a word-based language model, except we use characters\n",
    "instead of words. We will then use the trained model to generate some text in the same style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "#from keras.utils.visualize_util import plot    #it not be found\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Input File**:http://www.gutenberg.org/files/11/11-0.txt)\n",
    "\n",
    "The file contains line breaks and non-ASCII characters, so we do some preliminary cleanup and write out the contents into a variable called _text_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from input...\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILE = \"data/alice_in_wonderland.txt\"\n",
    "# extract the input as a stream of characters\n",
    "print(\"Extracting text from input...\")\n",
    "fin = open(INPUT_FILE, 'rb')\n",
    "lines = []\n",
    "for line in fin:\n",
    "    line = line.strip().lower()\n",
    "    line = line.decode(\"ascii\", \"ignore\")\n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    lines.append(line)\n",
    "fin.close()\n",
    "text = \" \".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the index for the characters:** \n",
    "    \n",
    "Since we are building a character-level RNN, our vocabulary is the set of characters that occur in the\n",
    "text. There are 42 of them in our case.Since we will be dealing with the indexes to these characters\n",
    "rather than the characters themselves, the following code snippet creates the necessary lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n",
    "char2index = dict((c, i) for i, c in enumerate(chars))\n",
    "index2char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the input and label texts**\n",
    "\n",
    "We do this by stepping through the text by a number\n",
    "of characters given by the STEP variable (1 in our case) and then extracting a span of text whose size is\n",
    "determined by the SEQLEN variable (10 in our case). The next character after the span is our label\n",
    "character.\n",
    "For the next input text: _it turned into a pig_\n",
    "- it turned_ -> the next character is \"i\" \n",
    "- t turned i -> the next character is \"n\" \n",
    "- turned in -> the next character is \"t\" \n",
    "- turned int -> the next character is \"o\"\n",
    "- urned into -> the next character is \" \"\n",
    "- rned into -> the next character is \"a\"\n",
    "- ned into a-> the next character is \" \"\n",
    "- ed into a -> the next character is \"p\"\n",
    "- d into a p-> the next character is \"i\"\n",
    "- into a pi-> the next character is \"g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQLEN = 10\n",
    "STEP = 1\n",
    "input_chars = []\n",
    "label_chars = []\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i:i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorize the input and label texts**\n",
    "\n",
    "Each row of the input to the RNN corresponds to one of the input texts shown previously. There are _SEQLEN_ characters in this input, and since our vocabulary size is given by nb_chars_, we represent each input character as a one-hot encoded vector of size (nb_chars). **Thus each input row is a tensor of size (SEQLEN and nb_chars)**. Our output label is a single character, so similar to the way we represent each character of our input, it is represented\n",
    "as a one-hot vector of size (nb_chars). Thus, the shape of each label is nb_chars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)   # input is a tensor with size: SEQLEN x nb_chars\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)           # output with size: nb_chars  \n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the RNN Model**\n",
    "\n",
    "the RNN's output dimension needs to be determined by experimentation. In general, if we\n",
    "choose too small a size, then the model does not have sufficient capacity for generating good text, and\n",
    "you will see long runs of repeating characters or runs of repeating word groups. On the other hand, if\n",
    "the value chosen is too large, the model has too many parameters and needs a lot more data to train\n",
    "effectively. \n",
    "\n",
    "We want to return a single character as output, not a sequence of characters, so\n",
    "**return_sequences=False**. We have already seen that the input to the RNN is of shape (SEQLEN and nb_chars).\n",
    "In addition, we set **unroll=True** because it **improves performance on the TensorFlow backend**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128                      \n",
    "BATCH_SIZE = 128                                                # RNN's output dimension\n",
    "NUM_ITERATIONS = 25\n",
    "NUM_EPOCHS_PER_ITERATION = 1\n",
    "NUM_PREDS_PER_EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(SimpleRNN(HIDDEN_SIZE, \n",
    "                    return_sequences=False,                     # we want only one character, not a sequence\n",
    "                    input_shape=(SEQLEN, nb_chars),             # input is a tensor with size: SEQLEN x nb_chars\n",
    "                    unroll=True))                               # to improve the performance on the TensorFlow backend\n",
    "\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training approach is a little different from what we have seen so far. \n",
    "- So far our approach has been to train a model for a fixed number of epochs, then evaluate it against a portion of held-out test data.\n",
    "- Since we don't have any labeled data here, we train the model for an epoch(NUM_EPOCHS_PER_ITERATION=1) then test it.\n",
    "\n",
    "We continue training like this for 25 (NUM_ITERATIONS=25) iterations,\n",
    "stopping once we see intelligible output. So effectively, we are training for NUM_ITERATIONS epochs and\n",
    "testing the model after each epoch.\n",
    "\n",
    "Our test consists of\n",
    "- generating a character from the model given a random input, \n",
    "- then dropping the first character from the input \n",
    "- and appending the predicted character from our previous run, \n",
    "- and generating another character from the model.\n",
    "\n",
    "We continue this 100 times (NUM_PREDS_PER_EPOCH=100) and\n",
    "generate and print the resulting string. The string gives us an indication of the quality of the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We train the model in batches and test output generated at each step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Iteration #: 0\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 6s 38us/step - loss: 2.3333\n",
      "Generating from seed: no meaning\n",
      "no meaning the wathe said the wathe said the wathe said the wathe said the wathe said the wathe said the wathe==================================================\n",
      "Iteration #: 1\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 34us/step - loss: 2.0449\n",
      "Generating from seed:  felt quit\n",
      " felt quite so the groject gutenberg-tm the she said the groject gutenberg-tm the she said the groject gutenbe==================================================\n",
      "Iteration #: 2\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 34us/step - loss: 1.9429\n",
      "Generating from seed: an wrappin\n",
      "an wrapping to the said the done the said the done the said the done the said the done the said the done the s==================================================\n",
      "Iteration #: 3\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 35us/step - loss: 1.8639\n",
      "Generating from seed: after-time\n",
      "after-time of the say and the maste the said the matee the said the matee the said the matee the said the mate==================================================\n",
      "Iteration #: 4\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 6s 35us/step - loss: 1.7971\n",
      "Generating from seed: at alice q\n",
      "at alice queen the project gutenberg-tm the project gutenberg-tm the project gutenberg-tm the project gutenber==================================================\n",
      "Iteration #: 5\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 6s 40us/step - loss: 1.7417\n",
      "Generating from seed: e was goin\n",
      "e was going on the could the saided the project gutenberg-tm little she said the could the saided the project ==================================================\n",
      "Iteration #: 6\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 6s 35us/step - loss: 1.6960\n",
      "Generating from seed: tners-- --\n",
      "tners-- --with the caterpelled and the reat to the gropen the realing the caterpelled and the reat to the grop==================================================\n",
      "Iteration #: 7\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 6s 36us/step - loss: 1.6586\n",
      "Generating from seed: oes your w\n",
      "oes your was the rook of the rook of the rook of the rook of the rook of the rook of the rook of the rook of t==================================================\n",
      "Iteration #: 8\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 6s 35us/step - loss: 1.6281\n",
      "Generating from seed: o a farmer\n",
      "o a farmer peared the say of the say. i she said the mouse the say of the say. i she said the mouse the say of==================================================\n",
      "Iteration #: 9\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 6s 38us/step - loss: 1.6011\n",
      "Generating from seed: rning to t\n",
      "rning to the tor said the dormouse in the dormouse in the dormouse in the dormouse in the dormouse in the dorm==================================================\n",
      "Iteration #: 10\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 6s 38us/step - loss: 1.5769\n",
      "Generating from seed: ou, she sa\n",
      "ou, she said the march hare the mone here was a little said the march hare the mone here was a little said the==================================================\n",
      "Iteration #: 11\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 6s 37us/step - loss: 1.5554\n",
      "Generating from seed: rried on, \n",
      "rried on, the say of the mouse have the say at the rabbit down the rook and the stome the mouse have the say a==================================================\n",
      "Iteration #: 12\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 33us/step - loss: 1.5359\n",
      "Generating from seed: and then a\n",
      "and then a rears with a rigute berant then it said the mock turtle and there went on a little and there went o==================================================\n",
      "Iteration #: 13\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 33us/step - loss: 1.5176\n",
      "Generating from seed:  these wor\n",
      " these words ard would be and soup of the said the march hare the tarts come of the said the march hare the ta==================================================\n",
      "Iteration #: 14\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 33us/step - loss: 1.5029\n",
      "Generating from seed: ould be fr\n",
      "ould be frem the stall she said the king she said the king she said the king she said the king she said the ki==================================================\n",
      "Iteration #: 15\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 34us/step - loss: 1.4881\n",
      "Generating from seed: s hardly r\n",
      "s hardly repaster and the sermemser the caterpillar you could the mouse the serther alice was a little she wen==================================================\n",
      "Iteration #: 16\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 5s 35us/step - loss: 1.4749\n",
      "Generating from seed: t tree in \n",
      "t tree in the same alice was a little sher simple shem so much as she could the mouse the mouse the mouse the ==================================================\n",
      "Iteration #: 17\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 6s 40us/step - loss: 1.4631\n",
      "Generating from seed: retched he\n",
      "retched her flate project gutenberg-tm electronic works work the gryphon, and when she had not to her flate pr==================================================\n",
      "Iteration #: 18\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 43us/step - loss: 1.4529\n",
      "Generating from seed: ly up into\n",
      "ly up into a surtors, and she was the way the white rabbit and which was a little she was the way the white ra==================================================\n",
      "Iteration #: 19\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 42us/step - loss: 1.4429\n",
      "Generating from seed: l, certain\n",
      "l, certainly said the mock turtle said the mock turtle said the mock turtle said the mock turtle said the mock==================================================\n",
      "Iteration #: 20\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 42us/step - loss: 1.4330\n",
      "Generating from seed: oo slipper\n",
      "oo slipper to the project gutenberg-tm electronic works of the court in a to say what it was a little she was ==================================================\n",
      "Iteration #: 21\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 44us/step - loss: 1.4229\n",
      "Generating from seed: ng again. \n",
      "ng again. the dirent to the this more the thing as it was not at the this more the thing as it was not at the ==================================================\n",
      "Iteration #: 22\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 6s 41us/step - loss: 1.4164\n",
      "Generating from seed: ent! screa\n",
      "ent! scream the dormouse in the door as her hand the mock turtle in the door as her hand the mock turtle in th==================================================\n",
      "Iteration #: 23\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 42us/step - loss: 1.4088\n",
      "Generating from seed: he hatter.\n",
      "he hatter. alice thought alice was the mock turtle replied to the patter and she thought alice was the mock tu==================================================\n",
      "Iteration #: 24\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 43us/step - loss: 1.4015\n",
      "Generating from seed: r and went\n",
      "r and went to say would be nerely to the project gutenberg-tm electronic works in the door and more the dormou\n"
     ]
    }
   ],
   "source": [
    "# We train the model in batches and test output generated at each step\n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Iteration #: %d\" % (iteration))\n",
    "    model.fit(X, y, \n",
    "              batch_size=BATCH_SIZE, \n",
    "              epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "    \n",
    "    # testing model\n",
    "    # randomly choose a row from input_chars, then use it to \n",
    "    # generate text from model for next 100 chars\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "    print(\"Generating from seed: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "        Xtest = np.zeros((1, SEQLEN, nb_chars))                      # input is a tensor with size: SEQLEN x nb_chars\n",
    "        for i, ch in enumerate(test_chars):\n",
    "            Xtest[0, i, char2index[ch]] = 1\n",
    "        pred = model.predict(Xtest, verbose=0)[0]\n",
    "        ypred = index2char[np.argmax(pred)]\n",
    "        print(ypred, end=\"\")\n",
    "        # move forward with test_chars + ypred\n",
    "        test_chars = test_chars[1:] + ypred\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the next character or next word of text is not the only thing you can do with this sort of\n",
    "model. This kind of model has been successfully used to make\n",
    "- stock predictions (Financial Market Time Series Prediction with Recurrent Neural Networks, by A. Bernal, S. Fok, and R. Pidaparthi, 2012) \n",
    "- and generate classical music ( DeepBach: A Steerable Model for Bach Chorales Generation, by G. Hadjeres and F. Pachet, arXiv:1612.01010, 2016)\n",
    "- Andrej Karpathy covers a few other fun examples, such as generating fake Wikipedia pages, algebraic geometry proofs, and Linux source code in his blog post at: The Unreasonable Effectiveness of Recurrent Neural Networks at http://karpathy.github.io/2015/05/21/rnn-effectiveness/."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
