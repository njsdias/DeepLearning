{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with Keras: sentiment analysis\n",
    "\n",
    "Our network will take in a sentence (a sequence of words) and outputs a sentiment value (positive or\n",
    "negative). Our training set is a dataset of about 7,000 short sentences from UMICH SI650 sentiment\n",
    "classification competition on Kaggle (https://inclass.kaggle.com/c/si650winter11). Each sentence is labeled 1 or\n",
    "0 for positive or negative sentiment respectively, which our network will learn to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "#For TensorBoard\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from time import gmtime, strftime\n",
    "import datetime, os\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create folder for TensorBoard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"lstmumich{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Data and Generate vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"data/umich-sentiment-train.txt\"\n",
    "ftrain = codecs.open(INPUT_FILE, \"r\", encoding='ascii', errors='ignore')\n",
    "\n",
    "# Read training data and generate vocabulary\n",
    "maxlen = 0\n",
    "word_freqs = collections.Counter()\n",
    "num_recs = 0\n",
    "\n",
    "for line in ftrain:\n",
    "    labels, sentence = line.strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    if len(words) > maxlen:\n",
    "        maxlen = len(words)\n",
    "    for word in words:\n",
    "        word_freqs[word] += 1\n",
    "    num_recs += 1\n",
    "ftrain.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7086"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimates for our Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "2311\n"
     ]
    }
   ],
   "source": [
    "## Get some information about our corpus\n",
    "print(maxlen)            # 42\n",
    "print(len(word_freqs))   # 2311"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the number of unique words _len(word_freqs)_, we set our vocabulary size to a fixed number and\n",
    "treat all the other words as **out of vocabulary (OOV) words** and replace them with the pseudo-word\n",
    "UNK (for unknown). At prediction time, this will allow us to handle previously unseen words as\n",
    "OOV words as well.\n",
    "\n",
    "The number of words in the sentence (maxlen) allows us to set a fixed sequence length and zero pad\n",
    "shorter sentences and truncate longer sentences to that length as appropriate. Even though RNNs\n",
    "handle variable sequence length, this is usually achieved either by padding and truncating as above,\n",
    "or by grouping the inputs in different batches by sequence length. We will use the former approach\n",
    "here. For the latter approach, Keras recommends using batches of size one (for more information\n",
    "refer to: https://github.com/fchollet/keras/issues/40).\n",
    "\n",
    "**Based on the preceding estimates,** we set our _VOCABULARY_SIZE_ to 2002. This is 2000 words from our\n",
    "vocabulary plus the UNK pseudo-word and the PAD pseudo word (used for padding sentences to a\n",
    "fixed number of words), in our case 40 given by _MAX_SENTENCE_LENGTH_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 2000\n",
    "MAX_SENTENCE_LENGTH = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need a pair of lookup tables. Each row of input to the RNN is a sequence of word indices,\n",
    "where the indices are ordered by most frequent to least frequent word in the training set. The two\n",
    "lookup tables allow us to lookup an index given the word and the word given the index. This includes\n",
    "the PAD and UNK pseudo-words as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = min(MAX_FEATURES, len(word_freqs)) + 2\n",
    "word2index = {x[0]: i+2 for i, x in\n",
    "              enumerate(word_freqs.most_common(MAX_FEATURES))}\n",
    "word2index[\"PAD\"] = 0\n",
    "word2index[\"UNK\"] = 1\n",
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert our input sentences to word index sequences, pad them to the MAX_SENTENCE_LENGTH\n",
    "words. Since our output label in this case is binary (positive or negative sentiment), we don't need to\n",
    "process the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sentences to sequences\n",
    "X = np.empty((num_recs, ), dtype=list)\n",
    "y = np.zeros((num_recs, ))\n",
    "i = 0\n",
    "ftrain = codecs.open(INPUT_FILE, \"r\", encoding='ascii', errors='ignore')\n",
    "for line in ftrain:\n",
    "    label, sentence = line.strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    seqs = []\n",
    "    for word in words:\n",
    "        #if word2index.has_key(word):\n",
    "        if word in word2index:\n",
    "            seqs.append(word2index[word])\n",
    "        else:\n",
    "            seqs.append(word2index[\"UNK\"])\n",
    "    X[i] = seqs\n",
    "    y[i] = int(label)\n",
    "    i += 1\n",
    "ftrain.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pad the sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences (left padded with zeros)\n",
    "X = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split input into training and test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5668, 40) (1418, 40) (5668,) (1418,)\n"
     ]
    }
   ],
   "source": [
    "# Split input into training and test\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, \n",
    "                                                random_state=42)\n",
    "\n",
    "print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram shows the structure of our RNN:\n",
    "<img src=\"lstm_struct_model.JPG\">\n",
    "\n",
    "The **input for each row** is a sequence of word indices. The sequence length is given by\n",
    "MAX_SENTENCE_LENGTH. The **first dimension of the tensor** is set to _None_ to indicate that the batch size (the\n",
    "number of records fed to the network each time) is currently unknown at definition time; it is\n",
    "specified during run time using the batch_size parameter. So assuming an as - yet undetermined batch\n",
    "size, the shape of **the input tensor** is (None, MAX_SENTENCE_LENGTH, 1). These tensors are fed into an\n",
    "**embedding layer** of size EMBEDDING_SIZE whose **weights are initialized** with small random values and\n",
    "learned during training. This layer (embedding layer) will transform the tensor to a shape (None,MAX_SENTENCE_LENGTH,\n",
    "EMBEDDING_SIZE). The **output of the embedding layer** is fed into an LSTM with sequence length\n",
    "MAX_SENTENCE_LENGTH and output layer size HIDDEN_LAYER_SIZE, so **the output of the LSTM** is a tensor of shape\n",
    "(None, HIDDEN_LAYER_SIZE, MAX_SENTENCE_LENGTH). By default, the LSTM will output a single tensor of shape\n",
    "(None, HIDDEN_LAYER_SIZE) at its last sequence (return_sequences=False). This is fed **to a dense layer with\n",
    "output size** of 1 with a sigmoid activation function, so it will output either 0 (negative review) or 1\n",
    "(positive review).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constants of the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_LAYER_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**\n",
    "\n",
    "We compile the model using the binary cross-entropy loss function since it predicts a binary value,\n",
    "and the Adam optimizer, a good general purpose optimizer. Note that the hyperparameters\n",
    "EMBEDDING_SIZE, HIDDEN_LAYER_SIZE, BATCH_SIZE and NUM_EPOCHS were tuned\n",
    "experimentally over several runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ctw00071\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ctw00071\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, \n",
    "                    EMBEDDING_SIZE,\n",
    "                    input_length=MAX_SENTENCE_LENGTH))\n",
    "\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "model.add(LSTM(HIDDEN_LAYER_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the model**\n",
    "\n",
    "We then train the network for 10 epochs (NUM_EPOCHS) and batch size of 32 (BATCH_SIZE). At each epoch we\n",
    "validate the model using the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5668 samples, validate on 1418 samples\n",
      "Epoch 1/10\n",
      "5668/5668 [==============================] - 8s 1ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0621 - val_acc: 0.9908\n",
      "Epoch 2/10\n",
      "5668/5668 [==============================] - 7s 1ms/step - loss: 9.8968e-04 - acc: 0.9998 - val_loss: 0.0759 - val_acc: 0.9887\n",
      "Epoch 3/10\n",
      "5668/5668 [==============================] - 6s 1ms/step - loss: 7.9657e-04 - acc: 0.9996 - val_loss: 0.0720 - val_acc: 0.9887\n",
      "Epoch 4/10\n",
      "5668/5668 [==============================] - 6s 1ms/step - loss: 2.5949e-04 - acc: 0.9998 - val_loss: 0.0781 - val_acc: 0.9887\n",
      "Epoch 5/10\n",
      "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0687 - val_acc: 0.9894\n",
      "Epoch 6/10\n",
      "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0011 - acc: 0.9996 - val_loss: 0.0698 - val_acc: 0.9880\n",
      "Epoch 7/10\n",
      "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0014 - acc: 0.9993 - val_loss: 0.0728 - val_acc: 0.9873\n",
      "Epoch 8/10\n",
      "5668/5668 [==============================] - 7s 1ms/step - loss: 8.2486e-04 - acc: 0.9996 - val_loss: 0.0650 - val_acc: 0.9880\n",
      "Epoch 9/10\n",
      "5668/5668 [==============================] - 7s 1ms/step - loss: 4.7989e-04 - acc: 0.9996 - val_loss: 0.0653 - val_acc: 0.9894\n",
      "Epoch 10/10\n",
      "5668/5668 [==============================] - 8s 1ms/step - loss: 2.9657e-04 - acc: 0.9996 - val_loss: 0.0663 - val_acc: 0.9908\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(Xtrain, ytrain, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    callbacks=[tensorboard],                #for plot in TensorBoard\n",
    "                    validation_data=(Xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation plots for Loss and Accuracy** \n",
    "<img src=\"lstm_umich.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure of the Neural Network Model**\n",
    "<img src=\"network_lstm.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the results, we get back close to 99% accuracy. The predictions the model makes\n",
    "for this particular set match exactly with the labels, although this is not the case for all predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1418/1418 [==============================] - 0s 258us/step\n",
      "Test score: 0.066, accuracy: 0.991\n",
      "1\t1\tda vinci code was an awesome movie ...\n",
      "0\t0\tda vinci code = up , up , down , down , left , right , left , right , b , a , suck !\n",
      "1\t1\tvery da vinci code slash amazing race .\n",
      "1\t1\tharry potter is awesome i do n't care if anyone says differently ! ..\n",
      "1\t1\ti love harry potter..\n",
      "1\t1\ti either love brokeback mountain or think it 's great that homosexuality is becoming more acceptable ! :\n",
      "0\t0\tthis quiz sucks and harry potter sucks ok bye..\n",
      "0\t0\tthen we drove to bayers lake for the da vinci code , which as expected , tom hanks sucks ass in that movie , but the dramatic last 2 minutes were good .\n",
      "1\t1\ti love harry potter .\n",
      "1\t1\ti want to be here because i love harry potter , and i really want a place where people take it serious , but it is still so much fun .\n",
      "1\t1\ti love harry potter .\n",
      "1\t1\tso as felicia 's mom is cleaning the table , felicia grabs my keys and we dash out like freakin mission impossible .\n",
      "1\t1\tand i like brokeback mountain .\n",
      "1\t1\tthe da vinci code is awesome ! !\n",
      "0\t0\tby the way , the da vinci code sucked , just letting you know ...\n",
      "1\t1\tda vinci code is awesome ! !\n",
      "1\t1\tman i loved brokeback mountain !\n",
      "1\t1\tharry potter is awesome i do n't care if anyone says differently ! ..\n",
      "0\t0\t`` thinking back to my comment to anax , i decided to write a really awful harry potter fanfic , rife with faked mary sue-ism ...\n",
      "1\t1\twe 're gon na like watch mission impossible or hoot . (\n",
      "1\t1\ti love brokeback mountain .\n",
      "0\t0\tbrokeback mountain is fucking horrible..\n",
      "0\t0\tlol nd i heard da vinci code sucks..\n",
      "0\t0\tda vinci code sucks be ...\n",
      "1\t1\tso as felicia 's mom is cleaning the table , felicia grabs my keys and we dash out like freakin mission impossible .\n",
      "0\t0\ti heard da vinci code sucked soo much only 2.5 stars :\n",
      "1\t1\tthe people who are worth it know how much i love the da vinci code .\n",
      "1\t1\tbrokeback mountain was beautiful ...\n",
      "1\t1\tbrokeback mountain is the most amazing / beautiful / romantic / heartbraking movie i have ever or will ever see in my life ... ...\n",
      "0\t0\tso brokeback mountain was really depressing .\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "score, acc = model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)\n",
    "print(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))\n",
    "\n",
    "for i in range(30):\n",
    "    idx = np.random.randint(len(Xtest))\n",
    "    xtest = Xtest[idx].reshape(1,MAX_SENTENCE_LENGTH)\n",
    "    ylabel = ytest[idx]\n",
    "    ypred = model.predict(xtest)[0][0]\n",
    "    sent = \" \".join([index2word[x] for x in xtest[0].tolist() if x != 0])\n",
    "    print(\"%.0f\\t%d\\t%s\" % (ypred, ylabel, sent))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
