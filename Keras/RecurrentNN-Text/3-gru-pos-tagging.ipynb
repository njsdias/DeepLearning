{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Gated Recurrent Unit (GRU) with Keras: POS(Part-of-Speech) tagging \n",
    "Keras provides a GRU implementation, that we will use here to build a network that does POS\n",
    "tagging. A **POS** is a grammatical category of words that are used in the same way across multiple\n",
    "sentences. **Examples of POS** are nouns, verbs, adjectives, and so on. For example, **nouns** are typically\n",
    "used to identify things, **verbs** are typically used to identify what they do, and **adjectives** to describe\n",
    "some attribute of these things. POS tagging used to be done manually, but nowadays this is done\n",
    "**automatically using statistical models**. In recent years, deep learning has been applied to this problem\n",
    "as well (Natural Language Processing from Scratch, by R. Collobert, Journal of Machine Learning Research, Pp. 2493-2537, 2011)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset: The Penn Treebank**\n",
    "\n",
    "For our training data, we will need sentences tagged with part of speech tags. The Penn Treebank (http\n",
    "s://catalog.ldc.upenn.edu/ldc99t42) is one such dataset, it is a human annotated corpus of about 4.5 million\n",
    "words of American English. However, it is a non-free resource. A 10% sample of the Penn Treebank\n",
    "is freely available as part of the NLTK (http://www.nltk.org/), which we will use to train our network.\n",
    "Our model will take in a sequence of words in a sentence and output the corresponding POS tags for\n",
    "each word. Thus for an input sequence consisting of the words [The, cat, sat, on, the, mat, .], the\n",
    "output sequence emitted would be the POS symbols [DT, NN, VB, IN, DT, NN]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42) # setting seed before importing from keras\n",
    "from keras.layers.core import Activation, Dense, Dropout, RepeatVector, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding, Bidirectional\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import nltk\n",
    "\n",
    "import os\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"treebank_sents.txt\"), \"w\") as fedata, \\\n",
    "        open(os.path.join(DATA_DIR, \"treebank_poss.txt\"), \"w\") as ffdata:\n",
    "    sents = nltk.corpus.treebank.tagged_sents()\n",
    "    for sent in sents:\n",
    "        words, poss = [], []\n",
    "        for word, pos in sent:\n",
    "            if pos == \"-NONE-\":\n",
    "                continue\n",
    "            words.append(word)\n",
    "            poss.append(pos)\n",
    "        fedata.write(\"{:s}\\n\".format(\" \".join(words)))\n",
    "        ffdata.write(\"{:s}\\n\".format(\" \".join(poss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore the data to find out what vocabulary size to set**\n",
    "\n",
    "We need to find: \n",
    "- the number of unique words in each vocabulary (in file of words and in file of tags);\n",
    "- the maximum number of words in a sentence in our training corpus; \n",
    "- the number of records. \n",
    "\n",
    "Because of the one-to-one nature of POS tagging, the last two values are identical for both vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentences(filename):\n",
    "    word_freqs = collections.Counter()\n",
    "    num_recs, maxlen = 0, 0\n",
    "    with open(filename, \"r\") as fin:\n",
    "        for line in fin:\n",
    "            words = line.strip().lower().split()\n",
    "            for word in words:\n",
    "                word_freqs[word] += 1\n",
    "            maxlen = max(maxlen, len(words))\n",
    "            num_recs += 1\n",
    "    return word_freqs, maxlen, num_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# records: 3914\n",
      "# unique words: 10947\n",
      "# unique POS tags: 45\n",
      "# words/sentence: max: 249\n"
     ]
    }
   ],
   "source": [
    "s_wordfreqs, s_maxlen, s_numrecs = \\\n",
    "    parse_sentences(os.path.join(DATA_DIR, \"treebank_sents.txt\"))\n",
    "t_wordfreqs, t_maxlen, t_numrecs = \\\n",
    "    parse_sentences(os.path.join(DATA_DIR, \"treebank_poss.txt\"))\n",
    "print(\"# records: {:d}\".format(s_numrecs))\n",
    "print(\"# unique words: {:d}\".format(len(s_wordfreqs)))\n",
    "print(\"# unique POS tags: {:d}\".format(len(t_wordfreqs)))\n",
    "print(\"# words/sentence: max: {:d}\".format(s_maxlen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that:\n",
    "- 10947 unique words; \n",
    "- 45 unique POS tags;\n",
    "- The maximum sentence size is 249; \n",
    "- The number of sentences is 249. \n",
    "\n",
    "Using this information, we decide to consider only the top 5000 words for our source vocabulary. Our target vocabulary has 45 unique POS tags, we want to be able to predict all of them, so we will consider all of them in our\n",
    "vocabulary. Finally, we set 250 to be our maximum sequence length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQLEN = 250\n",
    "S_MAX_FEATURES = 5000\n",
    "T_MAX_FEATURES = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building Lookup Tables**\n",
    "\n",
    "Just like our sentiment analysis example:\n",
    "- each row of the input will be represented as a sequence of word indices.\n",
    "- the corresponding output will be a sequence of POS tag indices. \n",
    "\n",
    "So we need to build lookup tables to translate between the words/POS tags and their corresponding indices. \n",
    "On the source side, we build a vocabulary index with two extra slots to hold the PAD\n",
    "and UNK pseudo-words. On the target side, we don't drop any words so there is no need for the UNK\n",
    "pseudo-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_vocabsize = min(len(s_wordfreqs), S_MAX_FEATURES) + 2\n",
    "s_word2index = {x[0]: i+2 for i, x in\n",
    "                enumerate(s_wordfreqs.most_common(S_MAX_FEATURES))}\n",
    "s_word2index[\"PAD\"] = 0\n",
    "s_word2index[\"UNK\"] = 1\n",
    "s_index2word = {v: k for k, v in s_word2index.items()}\n",
    "\n",
    "t_vocabsize = len(t_wordfreqs) + 1\n",
    "t_word2index = {x[0]: i for i, x in\n",
    "                enumerate(t_wordfreqs.most_common(T_MAX_FEATURES))}\n",
    "t_word2index[\"PAD\"] = 0\n",
    "t_index2word = {v: k for k, v in t_word2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building Dataset to our Network**\n",
    "\n",
    "We will use these lookup tables to convert our input sentences into a word ID sequence of length MAX_SEQLEN (250). \n",
    "\n",
    "The labels need to be structured as a sequence of one-hot vectors of size T_MAX_FEATURES + 1 (151), also of length MAX_SEQLEN (250).\n",
    "\n",
    "The build_tensor function reads the data from the two files and converts them to the input and output\n",
    "tensors. Additional default parameters are passed in to build the output tensor. This triggers the call to\n",
    "np_utils.to_categorical() to convert the output sequence of POS tag IDs to one-hot vector representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tensor(filename, numrecs, word2index, maxlen):\n",
    "    data = np.empty((numrecs, ), dtype=list)\n",
    "    with open(filename, \"r\") as fin:\n",
    "        for i, line in enumerate(fin):\n",
    "            wids = []\n",
    "            for word in line.strip().lower().split():\n",
    "                if word in word2index:\n",
    "                    wids.append(word2index[word])\n",
    "                else:\n",
    "                    wids.append(word2index['UNK'])\n",
    "            data[i] = wids\n",
    "    pdata = sequence.pad_sequences(data, maxlen=maxlen)\n",
    "    return pdata\n",
    "\n",
    "X = build_tensor(os.path.join(DATA_DIR, \"treebank_sents.txt\"),s_numrecs, s_word2index, MAX_SEQLEN)\n",
    "Y = build_tensor(os.path.join(DATA_DIR, \"treebank_poss.txt\"),t_numrecs, t_word2index, MAX_SEQLEN)\n",
    "Y = np.array([np_utils.to_categorical(d, t_vocabsize) for d in Y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spliting the data:**\n",
    "\n",
    "Training:80 / Test:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schematic of the Network** \n",
    "<img src=\"gru_pos.JPG\">\n",
    "\n",
    "As previously, assuming that \n",
    "- 1: the batch size is as yet undetermined\n",
    "- 2: the input to the network is a tensor of word IDs of shape (None, MAX_SEQLEN, 1). \n",
    "- 3: the input is sent through an embedding layer, which converts each word into a dense vector of shape (EMBED_SIZE)\n",
    "- 4: so, the output tensor from this layer has the shape(None, MAX_SEQLEN, EMBED_SIZE). \n",
    "- 5: the output tensor is fed to the encoder GRU with an output size of HIDDEN_SIZE. The GRU is set to return a single context vector (return_sequences=False) after seeing a sequence of size MAX_SEQLEN, so the output tensor from the GRU layer has shape (None, HIDDEN_SIZE).\n",
    "- 6: This context vector is then replicated using the RepeatVector layer into a tensor of shape (None, MAX_SEQLEN, HIDDEN_SIZE) and fed into the decoder GRU layer. \n",
    "- 7: This is then fed into a dense layer which produces an output tensor of shape (None, MAX_SEQLEN, t_vocab_size). The activation function on the dense layer is a softmax. The argmax of each column of this tensor is the index of the predicted POS tag for the word at that position.\n",
    "\n",
    "The model definition is shown as follows: EMBED_SIZE, HIDDEN_SIZE, BATCH_SIZE, and NUM_EPOCHS are\n",
    "hyperparameters which have been assigned these values after experimenting with multiple different\n",
    "values.\n",
    "\n",
    "The model is compiled with the categorical_crossentropy loss function since we have multiple\n",
    "categories of labels, and the optimizer used is the popular adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ctw00071\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters of the model\n",
    "EMBED_SIZE = 128\n",
    "HIDDEN_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(s_vocabsize,\n",
    "                    EMBED_SIZE,\n",
    "                    input_length=MAX_SEQLEN))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(GRU(HIDDEN_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(RepeatVector(MAX_SEQLEN))\n",
    "\n",
    "model.add(GRU(HIDDEN_SIZE, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(t_vocabsize)))\n",
    "\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", \n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train this model for a single epoch. The model is very rich, with many parameters, and begins to\n",
    "overfit after the first epoch of training. When fed the same data multiple times in the next epochs, the\n",
    "model begins to overfit to the training data and does worse on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ctw00071\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\ctw00071\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/1\n",
      "3131/3131 [==============================] - 38s 12ms/step - loss: 1.1363 - acc: 0.8994 - val_loss: 0.5454 - val_acc: 0.9159\n",
      "783/783 [==============================] - 2s 2ms/step\n",
      "Test score: 0.545, accuracy: 0.916\n"
     ]
    }
   ],
   "source": [
    "model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS, validation_data=[Xtest, Ytest])\n",
    "score, acc = model.evaluate(Xtest, Ytest, batch_size=BATCH_SIZE)\n",
    "print(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using LSTM and campare the results with GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/1\n",
      "3131/3131 [==============================] - 41s 13ms/step - loss: 0.9809 - acc: 0.9077 - val_loss: 0.5483 - val_acc: 0.9159\n",
      "783/783 [==============================] - 2s 2ms/step\n",
      "Test score: 0.548, accuracy: 0.916\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(s_vocabsize, EMBED_SIZE, input_length=MAX_SEQLEN))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(HIDDEN_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(RepeatVector(MAX_SEQLEN))\n",
    "\n",
    "model.add(LSTM(HIDDEN_SIZE, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(t_vocabsize)))\n",
    "\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS, validation_data=[Xtest, Ytest])\n",
    "\n",
    "score, acc = model.evaluate(Xtest, Ytest, batch_size=BATCH_SIZE)\n",
    "print(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the output, the results of the GRU-based network are quite comparable to our\n",
    "previous LSTM-based network.\n",
    "\n",
    "**Sequence-to-sequence models**\n",
    "- most used in **machine translation**\n",
    "- **entity recognition**: J. Hammerton, 2003 _Named Entity Recognition with Long Short Term Memory_, Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL, Association for Computational Linguistics\n",
    "- **sentence parsing**: O. Vinyals, 2015, Grammar as a Foreign Language, Advances in Neural Information Processing Systems.\n",
    "- **image captioning**: A. Karpathy, and F. Li, 2015, _Deep Visual-Semantic Alignments for Generating Image Descriptions_, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNNs\n",
    "\n",
    "At a given time step t, the output of the RNN is dependent on the outputs at all previous time steps.\n",
    "However, it is entirely possible that the output is also dependent on the future outputs as well. This is\n",
    "especially true for applications such as NLP, where the attributes **of the word or phrase we are trying\n",
    "to predict may be dependent on the context given by the entire enclosing sentence, not just the words\n",
    "that came before it.** Bidirectional RNNs also help a network architecture **place equal emphasis on the\n",
    "beginning and end of the sequence, and increase the data available for training.**\n",
    "\n",
    "Bidirectional RNNs **are two RNNs stacked on top of each other, reading the input in opposite\n",
    "directions.** So in our example, one RNN will read the words left to right and the other RNN will read\n",
    "the words right to left. The output at each time step will be based on the hidden state of both RNNs.\n",
    "\n",
    "Keras provides support for bidirectional RNNs through a bidirectional wrapper layer. For example,\n",
    "for our POS tagging example, we could **make our LSTMs bidirectional simply by wrapping them with\n",
    "this Bidirectional wrapper**, as shown in the model definition code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/1\n",
      "3131/3131 [==============================] - 64s 20ms/step - loss: 0.8009 - acc: 0.9085 - val_loss: 0.4426 - val_acc: 0.9159\n",
      "783/783 [==============================] - 3s 4ms/step\n",
      "Test score: 0.443, accuracy: 0.916\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(s_vocabsize, EMBED_SIZE, input_length=MAX_SEQLEN))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Bidirectional(LSTM(HIDDEN_SIZE, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(RepeatVector(MAX_SEQLEN))\n",
    "\n",
    "model.add(Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(t_vocabsize)))\n",
    "\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS, validation_data=[Xtest, Ytest])\n",
    "\n",
    "score, acc = model.evaluate(Xtest, Ytest, batch_size=BATCH_SIZE)\n",
    "print(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
