{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is based in the book: \n",
    "# Deep Learning with Keras\n",
    "## Implement neural networks with Keras on Theano and TensorFlow\n",
    "\n",
    "### Authors:  Antonio Gulli, Sujit Pal\n",
    "\n",
    "- Code: https://github.com/PacktPublishing/Deep-Learning-with-Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first example of Keras code\n",
    "The initial building block of Keras is a model, and the simplest model is called sequential. A sequential Keras model is a linear pipeline (a stack) of neural networks layers. This code fragment defines a single layer with **12 artificial neurons**, and it expects **8 input variables (also known as features)**.In this example **random_uniform** is used to initialize the weiths of the layers with uniformly random small values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "model = Sequential()\n",
    "#random_uniform Weights are initialized to uniformly random small values in (-0.05, 0.05).\n",
    "#There are more ways to initialize weights: https://keras.io/initializers/\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='random_uniform')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The _net is dense_, meaning that each neuron in a layer is connected to all neurons\n",
    "located in the previous layer and to all the neurons in the following layer.\n",
    "\n",
    "**Sigmoid Function:** $\\sigma(x)= \\frac{1}{1+e^{-x}}$ -> used by neuron for computing the nonlinear function $\\sigma(z=wx+b)\n",
    "$. A neuron with sigmoid activation has a behavior similar to the perceptron, but the changes are gradual.\n",
    "\n",
    "**Sigmoid** and **ReLU** are generally called **activation functions** in neural network jargon. They are the basic building blocks to developing a learning algorithm which adapts little by little, by progressively reducing the mistakes made by our nets.\n",
    "\n",
    "Full list of activation function is available at https://keras.io/activations/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot enconding- OHE**\n",
    "\n",
    "In many applications, it is convenient to transform categorical (non-numerical) features into numerical\n",
    "variables. For instance, the categorical feature digit with the value d in [0-9] can be encoded into a\n",
    "binary vector with 10 positions, which always has 0 value, except the d-th position where a 1 is\n",
    "present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A real example — recognizing handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will build a network that can recognize handwritten numbers. For achieving this\n",
    "goal, we use MNIST (for more information, refer to http://yann.lecun.com/exdb/mnist/), a database of\n",
    "handwritten digits made up of a training set of 60,000 examples and a test set of 10,000 examples.\n",
    "The training examples are annotated by humans with the correct answer. For instance, if the\n",
    "handwritten digit is the number three, then three is simply the label associated with that example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist                       #import dataset\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1671)                                   # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network and training\n",
    "NB_EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10                        # number of outputs = number of digits\n",
    "OPTIMIZER = SGD()                      # SGD optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2                   # how much TRAIN is reserved for VALIDATION: \n",
    "                                       # 60000*0.8 ->Training 60000*0.2->Validation\n",
    "                                       # Testing after Training: 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need terminologies like **epochs, batch size, iterations** only when the data is too big which happens all the time in machine learning and we can’t pass all the data to the computer at once. So, to overcome this problem we need to divide the data into smaller sizes and give it to our computer one by one and update the weights of the neural networks at the end of every step to fit it to the data given.\n",
    "\n",
    "**Epochs**: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE.\n",
    "\n",
    "As the number of **epochs increases**, more number of times the weight are changed in the neural network and the curve goes from underfitting to optimal to overfitting curve.\n",
    "\n",
    "**Batch Size**: Total number of training examples present in a single batch. As I said, you can’t pass the entire dataset into the neural net at once. So, you divide dataset into Number of Batches or sets or parts. Just like you divide a big article into multiple sets/batches/parts like Introduction, Gradient descent, Epoch, Batch size and Iterations which makes it easy to read the entire article for the reader and understand it.\n",
    "\n",
    "**Iterations**: It is the number of batches needed to complete one epoch. Let’s say we have 2000 training examples that we are going to use. We can divide the dataset of 2000 examples into batches of 500 then it will take 4 iterations to complete 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 11s 1us/step\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# data: shuffled and split between train and test sets\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **input layer has a neuron associated with each pixel in the image for a total of 28 x 28 = 784\n",
    "neurons**, one for each pixel in the MNIST images.\n",
    "Typically, the values associated with each pixel are normalized in the range [0, 1] (which means that\n",
    "the intensity of each pixel is divided by 255, the maximum intensity value). The output is 10 classes,\n",
    "one for each digit.\n",
    "The **final layer** is a single neuron with **activation function softmax**, which is a generalization of the\n",
    "sigmoid function. **Softmax squashes** a k-dimensional vector of arbitrary real values into a k-dimensional\n",
    "vector of real values in the range (0, 1). In our case, it aggregates 10 answers provided by the previous layer with **10 neurons:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(NB_CLASSES, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compilation**\n",
    "\n",
    "Once we define the model, we have to compile it so that it can be executed by the Keras backend\n",
    "(either Theano or TensorFlow). There are a few choices to be made during compilation:\n",
    "- We need to **select the optimizer** that is the specific algorithm used **to update weights** while we train our model\n",
    "- We need to **select the objective function** that is used by the optimizer to navigate the space of weights (frequently, objective functions are called **loss function**, and the process of optimization is defined as a **process of loss minimization**) -> https://keras.io/losses/ -> MSE, Binary Cross-entropy, Categorical Cross-entropy, Accuracy, Precision, Recall)\n",
    "- We need to evaluate the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=OPTIMIZER, \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics are similar to objective functions, with the only difference that they are not used for training a\n",
    "model but only for evaluating a model.\n",
    "\n",
    "Once the model is compiled, it can be then **trained** with the **fit() function**, which specifies a few\n",
    "parameters:\n",
    "- **epochs:** This is the number of times the model is exposed to the training set. At each iteration, the optimizer tries to adjust the weights so that the objective function is minimized.\n",
    "- **batch_size:** This is the number of training instances observed before the optimizer performs a weight update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 1.3633 - acc: 0.6796 - val_loss: 0.8904 - val_acc: 0.8246\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.7913 - acc: 0.8272 - val_loss: 0.6572 - val_acc: 0.8546\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.6436 - acc: 0.8497 - val_loss: 0.5625 - val_acc: 0.8681\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.5717 - acc: 0.8602 - val_loss: 0.5098 - val_acc: 0.8765\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.5276 - acc: 0.8678 - val_loss: 0.4758 - val_acc: 0.8826\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.4973 - acc: 0.8726 - val_loss: 0.4515 - val_acc: 0.8866\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.4748 - acc: 0.8775 - val_loss: 0.4333 - val_acc: 0.8882\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.4574 - acc: 0.8803 - val_loss: 0.4189 - val_acc: 0.8920\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.4433 - acc: 0.8834 - val_loss: 0.4075 - val_acc: 0.8939\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.4317 - acc: 0.8850 - val_loss: 0.3977 - val_acc: 0.8966\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.4218 - acc: 0.8873 - val_loss: 0.3896 - val_acc: 0.8984\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.4134 - acc: 0.8888 - val_loss: 0.3827 - val_acc: 0.8995\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.4060 - acc: 0.8902 - val_loss: 0.3766 - val_acc: 0.9003\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3995 - acc: 0.8918 - val_loss: 0.3712 - val_acc: 0.9013\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3936 - acc: 0.8928 - val_loss: 0.3664 - val_acc: 0.9016\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3884 - acc: 0.8945 - val_loss: 0.3621 - val_acc: 0.9031\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3837 - acc: 0.8950 - val_loss: 0.3582 - val_acc: 0.9033\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3794 - acc: 0.8962 - val_loss: 0.3546 - val_acc: 0.9039\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3755 - acc: 0.8970 - val_loss: 0.3514 - val_acc: 0.9048\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3718 - acc: 0.8979 - val_loss: 0.3485 - val_acc: 0.9053\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3685 - acc: 0.8985 - val_loss: 0.3457 - val_acc: 0.9058\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3653 - acc: 0.8995 - val_loss: 0.3431 - val_acc: 0.9058\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3625 - acc: 0.8999 - val_loss: 0.3407 - val_acc: 0.9063\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3598 - acc: 0.9008 - val_loss: 0.3385 - val_acc: 0.9070\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3572 - acc: 0.9012 - val_loss: 0.3364 - val_acc: 0.9074\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3548 - acc: 0.9019 - val_loss: 0.3345 - val_acc: 0.9084\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3525 - acc: 0.9022 - val_loss: 0.3326 - val_acc: 0.9082\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3504 - acc: 0.9032 - val_loss: 0.3311 - val_acc: 0.9090\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3484 - acc: 0.9031 - val_loss: 0.3293 - val_acc: 0.9094\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3465 - acc: 0.9041 - val_loss: 0.3277 - val_acc: 0.9097\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3447 - acc: 0.9044 - val_loss: 0.3264 - val_acc: 0.9097\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3430 - acc: 0.9047 - val_loss: 0.3249 - val_acc: 0.9097\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.3413 - acc: 0.9051 - val_loss: 0.3235 - val_acc: 0.9103\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3397 - acc: 0.9056 - val_loss: 0.3222 - val_acc: 0.9104\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3382 - acc: 0.9058 - val_loss: 0.3211 - val_acc: 0.9110\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3368 - acc: 0.9062 - val_loss: 0.3198 - val_acc: 0.9110\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3353 - acc: 0.9069 - val_loss: 0.3187 - val_acc: 0.9117\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3340 - acc: 0.9075 - val_loss: 0.3177 - val_acc: 0.9120\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3327 - acc: 0.9075 - val_loss: 0.3166 - val_acc: 0.9122\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3314 - acc: 0.9078 - val_loss: 0.3159 - val_acc: 0.9118\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3303 - acc: 0.9080 - val_loss: 0.3147 - val_acc: 0.9127\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3291 - acc: 0.9084 - val_loss: 0.3138 - val_acc: 0.9132\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3280 - acc: 0.9089 - val_loss: 0.3130 - val_acc: 0.9132\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3270 - acc: 0.9091 - val_loss: 0.3121 - val_acc: 0.9132\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3259 - acc: 0.9093 - val_loss: 0.3113 - val_acc: 0.9135\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3249 - acc: 0.9095 - val_loss: 0.3105 - val_acc: 0.9137\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3239 - acc: 0.9105 - val_loss: 0.3098 - val_acc: 0.9141\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3230 - acc: 0.9105 - val_loss: 0.3090 - val_acc: 0.9146\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3221 - acc: 0.9102 - val_loss: 0.3083 - val_acc: 0.9151\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3212 - acc: 0.9109 - val_loss: 0.3075 - val_acc: 0.9150\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3204 - acc: 0.9109 - val_loss: 0.3070 - val_acc: 0.9150\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3195 - acc: 0.9112 - val_loss: 0.3063 - val_acc: 0.9148\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3187 - acc: 0.9114 - val_loss: 0.3057 - val_acc: 0.9153\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3180 - acc: 0.9117 - val_loss: 0.3050 - val_acc: 0.9148\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3171 - acc: 0.9121 - val_loss: 0.3044 - val_acc: 0.9149\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3164 - acc: 0.9121 - val_loss: 0.3037 - val_acc: 0.9156\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3157 - acc: 0.9128 - val_loss: 0.3034 - val_acc: 0.9152\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3149 - acc: 0.9121 - val_loss: 0.3029 - val_acc: 0.9148\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3143 - acc: 0.9128 - val_loss: 0.3022 - val_acc: 0.9151\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3136 - acc: 0.9129 - val_loss: 0.3016 - val_acc: 0.9161\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3130 - acc: 0.9133 - val_loss: 0.3011 - val_acc: 0.9158\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3123 - acc: 0.9131 - val_loss: 0.3007 - val_acc: 0.9151\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3117 - acc: 0.9136 - val_loss: 0.3003 - val_acc: 0.9156\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3110 - acc: 0.9137 - val_loss: 0.2997 - val_acc: 0.9158\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3105 - acc: 0.9137 - val_loss: 0.2992 - val_acc: 0.9159\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3098 - acc: 0.9138 - val_loss: 0.2988 - val_acc: 0.9161\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3093 - acc: 0.9141 - val_loss: 0.2983 - val_acc: 0.9165\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3087 - acc: 0.9139 - val_loss: 0.2979 - val_acc: 0.9166\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3082 - acc: 0.9144 - val_loss: 0.2976 - val_acc: 0.9164\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3077 - acc: 0.9145 - val_loss: 0.2971 - val_acc: 0.9166\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3071 - acc: 0.9146 - val_loss: 0.2967 - val_acc: 0.9172\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3066 - acc: 0.9147 - val_loss: 0.2964 - val_acc: 0.9167\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3061 - acc: 0.9151 - val_loss: 0.2960 - val_acc: 0.9169\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3056 - acc: 0.9150 - val_loss: 0.2956 - val_acc: 0.9173\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3051 - acc: 0.9151 - val_loss: 0.2952 - val_acc: 0.9177\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3046 - acc: 0.9152 - val_loss: 0.2950 - val_acc: 0.9173\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3042 - acc: 0.9154 - val_loss: 0.2945 - val_acc: 0.9172\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3037 - acc: 0.9154 - val_loss: 0.2942 - val_acc: 0.9176\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3032 - acc: 0.9157 - val_loss: 0.2939 - val_acc: 0.9179\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3028 - acc: 0.9156 - val_loss: 0.2936 - val_acc: 0.9177\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.3024 - acc: 0.9157 - val_loss: 0.2933 - val_acc: 0.9179\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3019 - acc: 0.9157 - val_loss: 0.2930 - val_acc: 0.9178\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3015 - acc: 0.9160 - val_loss: 0.2926 - val_acc: 0.9182\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.3011 - acc: 0.9161 - val_loss: 0.2924 - val_acc: 0.9179\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3007 - acc: 0.9165 - val_loss: 0.2920 - val_acc: 0.9184\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.3003 - acc: 0.9164 - val_loss: 0.2918 - val_acc: 0.9185\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2999 - acc: 0.9165 - val_loss: 0.2914 - val_acc: 0.9185\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2995 - acc: 0.9166 - val_loss: 0.2911 - val_acc: 0.9188\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2991 - acc: 0.9167 - val_loss: 0.2909 - val_acc: 0.9191\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2988 - acc: 0.9169 - val_loss: 0.2906 - val_acc: 0.9191\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2984 - acc: 0.9168 - val_loss: 0.2903 - val_acc: 0.9192\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 1s 10us/step - loss: 0.2981 - acc: 0.9170 - val_loss: 0.2901 - val_acc: 0.9196\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2977 - acc: 0.9171 - val_loss: 0.2898 - val_acc: 0.9195\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2973 - acc: 0.9172 - val_loss: 0.2895 - val_acc: 0.9196\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2970 - acc: 0.9174 - val_loss: 0.2894 - val_acc: 0.9196\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2967 - acc: 0.9174 - val_loss: 0.2891 - val_acc: 0.9198\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2963 - acc: 0.9176 - val_loss: 0.2889 - val_acc: 0.9197\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2960 - acc: 0.9174 - val_loss: 0.2886 - val_acc: 0.9202\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2957 - acc: 0.9176 - val_loss: 0.2884 - val_acc: 0.9202\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2953 - acc: 0.9178 - val_loss: 0.2882 - val_acc: 0.9200\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2950 - acc: 0.9179 - val_loss: 0.2879 - val_acc: 0.9201\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2947 - acc: 0.9180 - val_loss: 0.2877 - val_acc: 0.9204\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2944 - acc: 0.9180 - val_loss: 0.2875 - val_acc: 0.9202\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2941 - acc: 0.9184 - val_loss: 0.2873 - val_acc: 0.9202\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2938 - acc: 0.9183 - val_loss: 0.2871 - val_acc: 0.9206\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2935 - acc: 0.9183 - val_loss: 0.2868 - val_acc: 0.9202\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2932 - acc: 0.9186 - val_loss: 0.2867 - val_acc: 0.9206\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2929 - acc: 0.9185 - val_loss: 0.2864 - val_acc: 0.9208\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2927 - acc: 0.9185 - val_loss: 0.2863 - val_acc: 0.9206\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2923 - acc: 0.9187 - val_loss: 0.2860 - val_acc: 0.9204\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2921 - acc: 0.9184 - val_loss: 0.2858 - val_acc: 0.9210\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2918 - acc: 0.9187 - val_loss: 0.2857 - val_acc: 0.9207\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2915 - acc: 0.9189 - val_loss: 0.2854 - val_acc: 0.9210\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2913 - acc: 0.9188 - val_loss: 0.2853 - val_acc: 0.9211\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2910 - acc: 0.9189 - val_loss: 0.2852 - val_acc: 0.9205\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2908 - acc: 0.9189 - val_loss: 0.2849 - val_acc: 0.9213\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2905 - acc: 0.9193 - val_loss: 0.2847 - val_acc: 0.9213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2902 - acc: 0.9192 - val_loss: 0.2846 - val_acc: 0.9212\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2900 - acc: 0.9191 - val_loss: 0.2844 - val_acc: 0.9212\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2898 - acc: 0.9192 - val_loss: 0.2842 - val_acc: 0.9212\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2895 - acc: 0.9191 - val_loss: 0.2841 - val_acc: 0.9212\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2892 - acc: 0.9192 - val_loss: 0.2840 - val_acc: 0.9212\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2890 - acc: 0.9194 - val_loss: 0.2838 - val_acc: 0.9211\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2888 - acc: 0.9197 - val_loss: 0.2837 - val_acc: 0.9210\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2885 - acc: 0.9193 - val_loss: 0.2835 - val_acc: 0.9207\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2883 - acc: 0.9197 - val_loss: 0.2834 - val_acc: 0.9217\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2881 - acc: 0.9194 - val_loss: 0.2832 - val_acc: 0.9212\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2879 - acc: 0.9194 - val_loss: 0.2830 - val_acc: 0.9210\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2876 - acc: 0.9196 - val_loss: 0.2828 - val_acc: 0.9217\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2874 - acc: 0.9197 - val_loss: 0.2826 - val_acc: 0.9216\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2871 - acc: 0.9200 - val_loss: 0.2827 - val_acc: 0.9211\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2870 - acc: 0.9197 - val_loss: 0.2824 - val_acc: 0.9213\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2868 - acc: 0.9198 - val_loss: 0.2823 - val_acc: 0.9216\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2866 - acc: 0.9199 - val_loss: 0.2822 - val_acc: 0.9214\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2863 - acc: 0.9203 - val_loss: 0.2820 - val_acc: 0.9213\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2861 - acc: 0.9196 - val_loss: 0.2818 - val_acc: 0.9215\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2859 - acc: 0.9198 - val_loss: 0.2818 - val_acc: 0.9217\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2857 - acc: 0.9203 - val_loss: 0.2815 - val_acc: 0.9218\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2855 - acc: 0.9203 - val_loss: 0.2814 - val_acc: 0.9215\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2853 - acc: 0.9201 - val_loss: 0.2812 - val_acc: 0.9216\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2852 - acc: 0.9204 - val_loss: 0.2811 - val_acc: 0.9217\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2849 - acc: 0.9201 - val_loss: 0.2810 - val_acc: 0.9217\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2848 - acc: 0.9205 - val_loss: 0.2809 - val_acc: 0.9219\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2846 - acc: 0.9208 - val_loss: 0.2808 - val_acc: 0.9217\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 1s 10us/step - loss: 0.2844 - acc: 0.9207 - val_loss: 0.2806 - val_acc: 0.9221\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2841 - acc: 0.9206 - val_loss: 0.2806 - val_acc: 0.9220\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2840 - acc: 0.9207 - val_loss: 0.2804 - val_acc: 0.9217\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2838 - acc: 0.9209 - val_loss: 0.2803 - val_acc: 0.9218\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2836 - acc: 0.9208 - val_loss: 0.2802 - val_acc: 0.9216\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2835 - acc: 0.9210 - val_loss: 0.2800 - val_acc: 0.9225\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2833 - acc: 0.9210 - val_loss: 0.2799 - val_acc: 0.9226\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2831 - acc: 0.9211 - val_loss: 0.2798 - val_acc: 0.9222\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2829 - acc: 0.9207 - val_loss: 0.2797 - val_acc: 0.9224\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2827 - acc: 0.9209 - val_loss: 0.2796 - val_acc: 0.9222\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2826 - acc: 0.9208 - val_loss: 0.2795 - val_acc: 0.9225\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2824 - acc: 0.9210 - val_loss: 0.2794 - val_acc: 0.9224\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2822 - acc: 0.9210 - val_loss: 0.2793 - val_acc: 0.9224\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2821 - acc: 0.9214 - val_loss: 0.2792 - val_acc: 0.9226\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2819 - acc: 0.9214 - val_loss: 0.2791 - val_acc: 0.9226\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2817 - acc: 0.9213 - val_loss: 0.2790 - val_acc: 0.9225\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2816 - acc: 0.9214 - val_loss: 0.2789 - val_acc: 0.9222\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2814 - acc: 0.9215 - val_loss: 0.2788 - val_acc: 0.9227\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2812 - acc: 0.9213 - val_loss: 0.2787 - val_acc: 0.9225\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2811 - acc: 0.9216 - val_loss: 0.2786 - val_acc: 0.9225\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2809 - acc: 0.9215 - val_loss: 0.2785 - val_acc: 0.9227\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2807 - acc: 0.9216 - val_loss: 0.2784 - val_acc: 0.9225\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2806 - acc: 0.9217 - val_loss: 0.2784 - val_acc: 0.9227\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2804 - acc: 0.9219 - val_loss: 0.2782 - val_acc: 0.9228\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2803 - acc: 0.9216 - val_loss: 0.2782 - val_acc: 0.9227\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2801 - acc: 0.9216 - val_loss: 0.2781 - val_acc: 0.9227\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2800 - acc: 0.9220 - val_loss: 0.2780 - val_acc: 0.9226\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2798 - acc: 0.9218 - val_loss: 0.2778 - val_acc: 0.9231\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2797 - acc: 0.9217 - val_loss: 0.2778 - val_acc: 0.9229\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2796 - acc: 0.9217 - val_loss: 0.2777 - val_acc: 0.9227\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2794 - acc: 0.9218 - val_loss: 0.2776 - val_acc: 0.9232\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2793 - acc: 0.9220 - val_loss: 0.2775 - val_acc: 0.9232\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2791 - acc: 0.9219 - val_loss: 0.2774 - val_acc: 0.9234\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2790 - acc: 0.9221 - val_loss: 0.2774 - val_acc: 0.9228\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2788 - acc: 0.9221 - val_loss: 0.2773 - val_acc: 0.9232\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2787 - acc: 0.9221 - val_loss: 0.2771 - val_acc: 0.9235\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2785 - acc: 0.9223 - val_loss: 0.2770 - val_acc: 0.9232\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2784 - acc: 0.9220 - val_loss: 0.2769 - val_acc: 0.9231\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2783 - acc: 0.9223 - val_loss: 0.2769 - val_acc: 0.9231\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2781 - acc: 0.9223 - val_loss: 0.2768 - val_acc: 0.9230\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2780 - acc: 0.9224 - val_loss: 0.2767 - val_acc: 0.9233\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2779 - acc: 0.9223 - val_loss: 0.2766 - val_acc: 0.9236\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2777 - acc: 0.9224 - val_loss: 0.2766 - val_acc: 0.9233\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2776 - acc: 0.9226 - val_loss: 0.2765 - val_acc: 0.9236\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2775 - acc: 0.9225 - val_loss: 0.2764 - val_acc: 0.9235\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2773 - acc: 0.9225 - val_loss: 0.2764 - val_acc: 0.9235\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 0s 10us/step - loss: 0.2772 - acc: 0.9225 - val_loss: 0.2763 - val_acc: 0.9237\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2770 - acc: 0.9226 - val_loss: 0.2762 - val_acc: 0.9238\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2770 - acc: 0.9226 - val_loss: 0.2761 - val_acc: 0.9237\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2768 - acc: 0.9226 - val_loss: 0.2761 - val_acc: 0.9236\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2767 - acc: 0.9231 - val_loss: 0.2760 - val_acc: 0.9239\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2766 - acc: 0.9226 - val_loss: 0.2758 - val_acc: 0.9241\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 1s 11us/step - loss: 0.2765 - acc: 0.9229 - val_loss: 0.2758 - val_acc: 0.9242\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2763 - acc: 0.9231 - val_loss: 0.2758 - val_acc: 0.9236\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2762 - acc: 0.9229 - val_loss: 0.2757 - val_acc: 0.9241\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2761 - acc: 0.9230 - val_loss: 0.2756 - val_acc: 0.9241\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** At each iteration the accuracy improves. \n",
    "                At the end of Training we have: \n",
    "    - Traning: Accuracy = 92.30%\n",
    "    - Validation: Accuracy = 92.41%\n",
    "\n",
    "**Note:** We reserved part of the training set for validation. The key idea is that we reserve a\n",
    "part of the training data for measuring the performance on the validation while\n",
    "training. This is a **good practice** to follow for any machine learning task, which we\n",
    "will adopt in all our examples.\n",
    "\n",
    "Once the model is trained, we can evaluate it on the test set that contains new unseen examples. In this\n",
    "way, we can get the minimal value reached by the objective function and best value reached by the\n",
    "evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 15us/step\n",
      "Test score: 0.2773858496874571\n",
      "Test accuracy: 0.9227\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** The end results are: \n",
    "    - Traning: Accuracy = 92.30%\n",
    "    - Validation: Accuracy = 92.41%\n",
    "    - Test: Accuracy = 92.27%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='brown'> Improving the simple net in Keras with hidden layers </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the last model we can **add additional layers to our network**. So, after the input layer, we have a\n",
    "first dense layer with the N_HIDDEN neurons and an *activation function ReLu*. This additional layer is\n",
    "considered hidden because it is not directly connected to either the input or the output. After the first\n",
    "hidden layer, we have a second hidden layer, again with the N_HIDDEN neurons, followed by an output\n",
    "layer with 10 neurons, each of which will fire when the relative digit is recognized. The following\n",
    "code defines this new network. \n",
    "\n",
    "So, we have the follow: Input + 2 Hidden Layers + Output (10 neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 1.4829 - acc: 0.6231 - val_loss: 0.7584 - val_acc: 0.8286\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.6049 - acc: 0.8464 - val_loss: 0.4550 - val_acc: 0.8852\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.4398 - acc: 0.8801 - val_loss: 0.3710 - val_acc: 0.9019\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.3767 - acc: 0.8952 - val_loss: 0.3322 - val_acc: 0.9082\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.3415 - acc: 0.9025 - val_loss: 0.3055 - val_acc: 0.9147\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.3175 - acc: 0.9085 - val_loss: 0.2880 - val_acc: 0.9182\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.2989 - acc: 0.9137 - val_loss: 0.2727 - val_acc: 0.9224\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2839 - acc: 0.9180 - val_loss: 0.2607 - val_acc: 0.9265\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2714 - acc: 0.9218 - val_loss: 0.2504 - val_acc: 0.9300\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.2602 - acc: 0.9252 - val_loss: 0.2430 - val_acc: 0.9308\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2501 - acc: 0.9286 - val_loss: 0.2341 - val_acc: 0.9334\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2409 - acc: 0.9301 - val_loss: 0.2271 - val_acc: 0.9351\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2325 - acc: 0.9334 - val_loss: 0.2227 - val_acc: 0.9367\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.2253 - acc: 0.9353 - val_loss: 0.2147 - val_acc: 0.9397\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2181 - acc: 0.9375 - val_loss: 0.2082 - val_acc: 0.9410\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2116 - acc: 0.9393 - val_loss: 0.2030 - val_acc: 0.9430\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2055 - acc: 0.9414 - val_loss: 0.1981 - val_acc: 0.9444\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1996 - acc: 0.9430 - val_loss: 0.1932 - val_acc: 0.9458\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.1941 - acc: 0.9432 - val_loss: 0.1894 - val_acc: 0.9468\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.1890 - acc: 0.9456 - val_loss: 0.1849 - val_acc: 0.9497\n",
      "10000/10000 [==============================] - 0s 24us/step\n",
      "\n",
      "Test score: 0.1859930982068181\n",
      "Test accuracy: 0.9462\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(1671)  # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 20                          #Reducing the epoch from 200 to 20 (10x less)     ******\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10                       # number of outputs = number of digits\n",
    "OPTIMIZER = SGD()                     # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128                        # Nodes to Hidden Layer                            ******\n",
    "VALIDATION_SPLIT=0.2                  # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize \n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "# ONE-HOT ENCONDING\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "\n",
    "model = Sequential()\n",
    "# First Hidden Layer\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "# Second Hidden Layer\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "# Output Layer\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#Compile Model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "# Training Model\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "# Test Model\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "#Print results\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this two hidden layers we had the next result:\n",
    "    - Training: 94.56%\n",
    "    - Validation: 94.97%\n",
    "    - Test: 94.62%\n",
    "With this newtwork we have an improvement of 2.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='brown'> Further improving the simple net in Keras with DROPOUT </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularization:** randomly drop with the dropout probability some\n",
    "of the values propagated inside our internal dense network of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/250\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 1.7404 - acc: 0.4539 - val_loss: 0.9293 - val_acc: 0.8124\n",
      "Epoch 2/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.9232 - acc: 0.7229 - val_loss: 0.5400 - val_acc: 0.8652\n",
      "Epoch 3/250\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.6935 - acc: 0.7881 - val_loss: 0.4298 - val_acc: 0.8883\n",
      "Epoch 4/250\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.5947 - acc: 0.8209 - val_loss: 0.3790 - val_acc: 0.8977\n",
      "Epoch 5/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.5347 - acc: 0.8393 - val_loss: 0.3456 - val_acc: 0.9039\n",
      "Epoch 6/250\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.4976 - acc: 0.8524 - val_loss: 0.3232 - val_acc: 0.9107\n",
      "Epoch 7/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.4616 - acc: 0.8628 - val_loss: 0.3048 - val_acc: 0.9129\n",
      "Epoch 8/250\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 0.4386 - acc: 0.8688 - val_loss: 0.2896 - val_acc: 0.9172\n",
      "Epoch 9/250\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 0.4181 - acc: 0.8762 - val_loss: 0.2776 - val_acc: 0.9198\n",
      "Epoch 10/250\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.3990 - acc: 0.8839 - val_loss: 0.2657 - val_acc: 0.9234\n",
      "Epoch 11/250\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.3819 - acc: 0.8876 - val_loss: 0.2551 - val_acc: 0.9256\n",
      "Epoch 12/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.3688 - acc: 0.8920 - val_loss: 0.2465 - val_acc: 0.9283\n",
      "Epoch 13/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.3571 - acc: 0.8944 - val_loss: 0.2388 - val_acc: 0.9299\n",
      "Epoch 14/250\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.3466 - acc: 0.8992 - val_loss: 0.2320 - val_acc: 0.9322\n",
      "Epoch 15/250\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 0.3358 - acc: 0.9014 - val_loss: 0.2261 - val_acc: 0.9339\n",
      "Epoch 16/250\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.3244 - acc: 0.9055 - val_loss: 0.2180 - val_acc: 0.9353\n",
      "Epoch 17/250\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.3142 - acc: 0.9084 - val_loss: 0.2121 - val_acc: 0.9377\n",
      "Epoch 18/250\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.3103 - acc: 0.9095 - val_loss: 0.2076 - val_acc: 0.9388\n",
      "Epoch 19/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.3019 - acc: 0.9118 - val_loss: 0.2018 - val_acc: 0.9408\n",
      "Epoch 20/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.2931 - acc: 0.9132 - val_loss: 0.1974 - val_acc: 0.9420\n",
      "Epoch 21/250\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.2866 - acc: 0.9172 - val_loss: 0.1920 - val_acc: 0.9437\n",
      "Epoch 22/250\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.2789 - acc: 0.9172 - val_loss: 0.1878 - val_acc: 0.9446\n",
      "Epoch 23/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.2730 - acc: 0.9199 - val_loss: 0.1841 - val_acc: 0.9463\n",
      "Epoch 24/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.2686 - acc: 0.9211 - val_loss: 0.1811 - val_acc: 0.9466\n",
      "Epoch 25/250\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.2618 - acc: 0.9233 - val_loss: 0.1770 - val_acc: 0.9478\n",
      "Epoch 26/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.2584 - acc: 0.9250 - val_loss: 0.1736 - val_acc: 0.9487\n",
      "Epoch 27/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.2539 - acc: 0.9254 - val_loss: 0.1706 - val_acc: 0.9497\n",
      "Epoch 28/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.2453 - acc: 0.9276 - val_loss: 0.1676 - val_acc: 0.9501\n",
      "Epoch 29/250\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.2427 - acc: 0.9274 - val_loss: 0.1641 - val_acc: 0.9517\n",
      "Epoch 30/250\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.2397 - acc: 0.9296 - val_loss: 0.1615 - val_acc: 0.9521\n",
      "Epoch 31/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.2360 - acc: 0.9304 - val_loss: 0.1590 - val_acc: 0.9533\n",
      "Epoch 32/250\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.2320 - acc: 0.9305 - val_loss: 0.1567 - val_acc: 0.9544\n",
      "Epoch 33/250\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.2284 - acc: 0.9327 - val_loss: 0.1534 - val_acc: 0.9552\n",
      "Epoch 34/250\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.2257 - acc: 0.9326 - val_loss: 0.1519 - val_acc: 0.9551\n",
      "Epoch 35/250\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 0.2214 - acc: 0.9354 - val_loss: 0.1501 - val_acc: 0.9556\n",
      "Epoch 36/250\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.2169 - acc: 0.9354 - val_loss: 0.1484 - val_acc: 0.9563\n",
      "Epoch 37/250\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 0.2124 - acc: 0.9375 - val_loss: 0.1459 - val_acc: 0.9571\n",
      "Epoch 38/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.2122 - acc: 0.9373 - val_loss: 0.1432 - val_acc: 0.9578\n",
      "Epoch 39/250\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.2091 - acc: 0.9387 - val_loss: 0.1422 - val_acc: 0.9576\n",
      "Epoch 40/250\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.2042 - acc: 0.9392 - val_loss: 0.1410 - val_acc: 0.9581\n",
      "Epoch 41/250\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.2027 - acc: 0.9397 - val_loss: 0.1396 - val_acc: 0.9583\n",
      "Epoch 42/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1984 - acc: 0.9415 - val_loss: 0.1366 - val_acc: 0.9595\n",
      "Epoch 43/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.2003 - acc: 0.9409 - val_loss: 0.1349 - val_acc: 0.9608\n",
      "Epoch 44/250\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.1953 - acc: 0.9421 - val_loss: 0.1337 - val_acc: 0.9607\n",
      "Epoch 45/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1920 - acc: 0.9431 - val_loss: 0.1331 - val_acc: 0.9602\n",
      "Epoch 46/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1901 - acc: 0.9445 - val_loss: 0.1316 - val_acc: 0.9615\n",
      "Epoch 47/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1876 - acc: 0.9449 - val_loss: 0.1299 - val_acc: 0.9612\n",
      "Epoch 48/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1867 - acc: 0.9443 - val_loss: 0.1300 - val_acc: 0.9617\n",
      "Epoch 49/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1865 - acc: 0.9453 - val_loss: 0.1282 - val_acc: 0.9615\n",
      "Epoch 50/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1803 - acc: 0.9461 - val_loss: 0.1267 - val_acc: 0.9622\n",
      "Epoch 51/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1822 - acc: 0.9466 - val_loss: 0.1254 - val_acc: 0.9636\n",
      "Epoch 52/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1794 - acc: 0.9459 - val_loss: 0.1244 - val_acc: 0.9632\n",
      "Epoch 53/250\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.1752 - acc: 0.9481 - val_loss: 0.1233 - val_acc: 0.9633\n",
      "Epoch 54/250\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.1738 - acc: 0.9477 - val_loss: 0.1220 - val_acc: 0.9637\n",
      "Epoch 55/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1735 - acc: 0.9491 - val_loss: 0.1208 - val_acc: 0.9647\n",
      "Epoch 56/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1719 - acc: 0.9487 - val_loss: 0.1208 - val_acc: 0.9637\n",
      "Epoch 57/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1692 - acc: 0.9502 - val_loss: 0.1188 - val_acc: 0.9651\n",
      "Epoch 58/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1663 - acc: 0.9507 - val_loss: 0.1187 - val_acc: 0.9649\n",
      "Epoch 59/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1682 - acc: 0.9501 - val_loss: 0.1172 - val_acc: 0.9654\n",
      "Epoch 60/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1647 - acc: 0.9514 - val_loss: 0.1165 - val_acc: 0.9652\n",
      "Epoch 61/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1614 - acc: 0.9522 - val_loss: 0.1156 - val_acc: 0.9658\n",
      "Epoch 62/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1592 - acc: 0.9526 - val_loss: 0.1149 - val_acc: 0.9657\n",
      "Epoch 63/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1587 - acc: 0.9533 - val_loss: 0.1142 - val_acc: 0.9657\n",
      "Epoch 64/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1564 - acc: 0.9530 - val_loss: 0.1126 - val_acc: 0.9667\n",
      "Epoch 65/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1560 - acc: 0.9537 - val_loss: 0.1128 - val_acc: 0.9667\n",
      "Epoch 66/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1572 - acc: 0.9535 - val_loss: 0.1120 - val_acc: 0.9662\n",
      "Epoch 67/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1553 - acc: 0.9548 - val_loss: 0.1105 - val_acc: 0.9668\n",
      "Epoch 68/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1525 - acc: 0.9545 - val_loss: 0.1102 - val_acc: 0.9672\n",
      "Epoch 69/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1523 - acc: 0.9552 - val_loss: 0.1088 - val_acc: 0.9676\n",
      "Epoch 70/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1502 - acc: 0.9551 - val_loss: 0.1085 - val_acc: 0.9678\n",
      "Epoch 71/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1478 - acc: 0.9567 - val_loss: 0.1081 - val_acc: 0.9679\n",
      "Epoch 72/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1450 - acc: 0.9568 - val_loss: 0.1072 - val_acc: 0.9685\n",
      "Epoch 73/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1462 - acc: 0.9568 - val_loss: 0.1068 - val_acc: 0.9680\n",
      "Epoch 74/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1439 - acc: 0.9582 - val_loss: 0.1067 - val_acc: 0.9680\n",
      "Epoch 75/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1447 - acc: 0.9565 - val_loss: 0.1059 - val_acc: 0.9682\n",
      "Epoch 76/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1414 - acc: 0.9579 - val_loss: 0.1059 - val_acc: 0.9685\n",
      "Epoch 77/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1421 - acc: 0.9580 - val_loss: 0.1055 - val_acc: 0.9680\n",
      "Epoch 78/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1398 - acc: 0.9587 - val_loss: 0.1044 - val_acc: 0.9691\n",
      "Epoch 79/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1415 - acc: 0.9572 - val_loss: 0.1041 - val_acc: 0.9686\n",
      "Epoch 80/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1392 - acc: 0.9595 - val_loss: 0.1033 - val_acc: 0.9689\n",
      "Epoch 81/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1370 - acc: 0.9594 - val_loss: 0.1035 - val_acc: 0.9688\n",
      "Epoch 82/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1365 - acc: 0.9580 - val_loss: 0.1031 - val_acc: 0.9687\n",
      "Epoch 83/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1344 - acc: 0.9596 - val_loss: 0.1019 - val_acc: 0.9691\n",
      "Epoch 84/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1338 - acc: 0.9600 - val_loss: 0.1013 - val_acc: 0.9692\n",
      "Epoch 85/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1337 - acc: 0.9604 - val_loss: 0.1014 - val_acc: 0.9696\n",
      "Epoch 86/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1346 - acc: 0.9601 - val_loss: 0.1005 - val_acc: 0.9697\n",
      "Epoch 87/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1304 - acc: 0.9606 - val_loss: 0.1003 - val_acc: 0.9705\n",
      "Epoch 88/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1321 - acc: 0.9596 - val_loss: 0.0999 - val_acc: 0.9697\n",
      "Epoch 89/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1304 - acc: 0.9609 - val_loss: 0.0990 - val_acc: 0.9702\n",
      "Epoch 90/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1321 - acc: 0.9603 - val_loss: 0.0987 - val_acc: 0.9703\n",
      "Epoch 91/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1286 - acc: 0.9622 - val_loss: 0.0982 - val_acc: 0.9710\n",
      "Epoch 92/250\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 0.1318 - acc: 0.9600 - val_loss: 0.0985 - val_acc: 0.9713\n",
      "Epoch 93/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1285 - acc: 0.9614 - val_loss: 0.0976 - val_acc: 0.9711\n",
      "Epoch 94/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1249 - acc: 0.9621 - val_loss: 0.0974 - val_acc: 0.9710\n",
      "Epoch 95/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1265 - acc: 0.9627 - val_loss: 0.0974 - val_acc: 0.9712\n",
      "Epoch 96/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1239 - acc: 0.9624 - val_loss: 0.0969 - val_acc: 0.9715\n",
      "Epoch 97/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1241 - acc: 0.9622 - val_loss: 0.0960 - val_acc: 0.9712\n",
      "Epoch 98/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1235 - acc: 0.9631 - val_loss: 0.0965 - val_acc: 0.9718\n",
      "Epoch 99/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1217 - acc: 0.9643 - val_loss: 0.0957 - val_acc: 0.9717\n",
      "Epoch 100/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1211 - acc: 0.9636 - val_loss: 0.0956 - val_acc: 0.9717\n",
      "Epoch 101/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1227 - acc: 0.9631 - val_loss: 0.0961 - val_acc: 0.9723\n",
      "Epoch 102/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1215 - acc: 0.9640 - val_loss: 0.0947 - val_acc: 0.9722\n",
      "Epoch 103/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1192 - acc: 0.9648 - val_loss: 0.0949 - val_acc: 0.9719\n",
      "Epoch 104/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1177 - acc: 0.9646 - val_loss: 0.0941 - val_acc: 0.9722\n",
      "Epoch 105/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1164 - acc: 0.9654 - val_loss: 0.0942 - val_acc: 0.9727\n",
      "Epoch 106/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1169 - acc: 0.9650 - val_loss: 0.0939 - val_acc: 0.9727\n",
      "Epoch 107/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1169 - acc: 0.9647 - val_loss: 0.0940 - val_acc: 0.9731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1139 - acc: 0.9666 - val_loss: 0.0933 - val_acc: 0.9724\n",
      "Epoch 109/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1146 - acc: 0.9658 - val_loss: 0.0933 - val_acc: 0.9734\n",
      "Epoch 110/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1140 - acc: 0.9659 - val_loss: 0.0927 - val_acc: 0.9727\n",
      "Epoch 111/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1146 - acc: 0.9658 - val_loss: 0.0927 - val_acc: 0.9722\n",
      "Epoch 112/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1117 - acc: 0.9661 - val_loss: 0.0917 - val_acc: 0.9737\n",
      "Epoch 113/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1127 - acc: 0.9658 - val_loss: 0.0921 - val_acc: 0.9733\n",
      "Epoch 114/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1144 - acc: 0.9654 - val_loss: 0.0914 - val_acc: 0.9737\n",
      "Epoch 115/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1113 - acc: 0.9663 - val_loss: 0.0913 - val_acc: 0.9742\n",
      "Epoch 116/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1087 - acc: 0.9673 - val_loss: 0.0911 - val_acc: 0.9738\n",
      "Epoch 117/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1115 - acc: 0.9664 - val_loss: 0.0912 - val_acc: 0.9736\n",
      "Epoch 118/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1086 - acc: 0.9671 - val_loss: 0.0907 - val_acc: 0.9739\n",
      "Epoch 119/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1117 - acc: 0.9660 - val_loss: 0.0910 - val_acc: 0.9739\n",
      "Epoch 120/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1070 - acc: 0.9676 - val_loss: 0.0901 - val_acc: 0.9743\n",
      "Epoch 121/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1083 - acc: 0.9669 - val_loss: 0.0904 - val_acc: 0.9744\n",
      "Epoch 122/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1074 - acc: 0.9674 - val_loss: 0.0895 - val_acc: 0.9744\n",
      "Epoch 123/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1042 - acc: 0.9679 - val_loss: 0.0891 - val_acc: 0.9745\n",
      "Epoch 124/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1046 - acc: 0.9683 - val_loss: 0.0894 - val_acc: 0.9744\n",
      "Epoch 125/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1043 - acc: 0.9691 - val_loss: 0.0892 - val_acc: 0.9743\n",
      "Epoch 126/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1035 - acc: 0.9685 - val_loss: 0.0888 - val_acc: 0.9745\n",
      "Epoch 127/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1033 - acc: 0.9685 - val_loss: 0.0889 - val_acc: 0.9747\n",
      "Epoch 128/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1042 - acc: 0.9686 - val_loss: 0.0883 - val_acc: 0.9752\n",
      "Epoch 129/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1050 - acc: 0.9676 - val_loss: 0.0882 - val_acc: 0.9752\n",
      "Epoch 130/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1039 - acc: 0.9690 - val_loss: 0.0883 - val_acc: 0.9752\n",
      "Epoch 131/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1025 - acc: 0.9689 - val_loss: 0.0875 - val_acc: 0.9751\n",
      "Epoch 132/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0999 - acc: 0.9704 - val_loss: 0.0879 - val_acc: 0.9752\n",
      "Epoch 133/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1009 - acc: 0.9687 - val_loss: 0.0877 - val_acc: 0.9750\n",
      "Epoch 134/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0989 - acc: 0.9688 - val_loss: 0.0877 - val_acc: 0.9748\n",
      "Epoch 135/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1007 - acc: 0.9692 - val_loss: 0.0880 - val_acc: 0.9747\n",
      "Epoch 136/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1000 - acc: 0.9705 - val_loss: 0.0876 - val_acc: 0.9751\n",
      "Epoch 137/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0996 - acc: 0.9694 - val_loss: 0.0877 - val_acc: 0.9755\n",
      "Epoch 138/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.1003 - acc: 0.9691 - val_loss: 0.0874 - val_acc: 0.9756\n",
      "Epoch 139/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0975 - acc: 0.9705 - val_loss: 0.0872 - val_acc: 0.9753\n",
      "Epoch 140/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0964 - acc: 0.9708 - val_loss: 0.0868 - val_acc: 0.9757\n",
      "Epoch 141/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0971 - acc: 0.9699 - val_loss: 0.0867 - val_acc: 0.9761\n",
      "Epoch 142/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0952 - acc: 0.9708 - val_loss: 0.0864 - val_acc: 0.9761\n",
      "Epoch 143/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0969 - acc: 0.9703 - val_loss: 0.0867 - val_acc: 0.9760\n",
      "Epoch 144/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0946 - acc: 0.9711 - val_loss: 0.0864 - val_acc: 0.9759\n",
      "Epoch 145/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0961 - acc: 0.9709 - val_loss: 0.0858 - val_acc: 0.9760\n",
      "Epoch 146/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0939 - acc: 0.9720 - val_loss: 0.0863 - val_acc: 0.9754\n",
      "Epoch 147/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0936 - acc: 0.9716 - val_loss: 0.0865 - val_acc: 0.9761\n",
      "Epoch 148/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0948 - acc: 0.9708 - val_loss: 0.0861 - val_acc: 0.9759\n",
      "Epoch 149/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0925 - acc: 0.9719 - val_loss: 0.0856 - val_acc: 0.9757\n",
      "Epoch 150/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0917 - acc: 0.9722 - val_loss: 0.0862 - val_acc: 0.9760\n",
      "Epoch 151/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0942 - acc: 0.9720 - val_loss: 0.0856 - val_acc: 0.9760\n",
      "Epoch 152/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0923 - acc: 0.9723 - val_loss: 0.0852 - val_acc: 0.9762\n",
      "Epoch 153/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0892 - acc: 0.9728 - val_loss: 0.0852 - val_acc: 0.9758\n",
      "Epoch 154/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0916 - acc: 0.9722 - val_loss: 0.0854 - val_acc: 0.9759\n",
      "Epoch 155/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0909 - acc: 0.9725 - val_loss: 0.0849 - val_acc: 0.9763\n",
      "Epoch 156/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0911 - acc: 0.9726 - val_loss: 0.0848 - val_acc: 0.9757\n",
      "Epoch 157/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0900 - acc: 0.9732 - val_loss: 0.0849 - val_acc: 0.9761\n",
      "Epoch 158/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0885 - acc: 0.9731 - val_loss: 0.0853 - val_acc: 0.9757\n",
      "Epoch 159/250\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.0877 - acc: 0.9726 - val_loss: 0.0845 - val_acc: 0.9764\n",
      "Epoch 160/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0893 - acc: 0.9730 - val_loss: 0.0848 - val_acc: 0.9762\n",
      "Epoch 161/250\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0887 - acc: 0.9728 - val_loss: 0.0841 - val_acc: 0.9765\n",
      "Epoch 162/250\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.0884 - acc: 0.9736 - val_loss: 0.0842 - val_acc: 0.9766\n",
      "Epoch 163/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0869 - acc: 0.9733 - val_loss: 0.0845 - val_acc: 0.9762\n",
      "Epoch 164/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0878 - acc: 0.9732 - val_loss: 0.0840 - val_acc: 0.9767\n",
      "Epoch 165/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0860 - acc: 0.9734 - val_loss: 0.0839 - val_acc: 0.9764\n",
      "Epoch 166/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0862 - acc: 0.9730 - val_loss: 0.0847 - val_acc: 0.9762\n",
      "Epoch 167/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0857 - acc: 0.9738 - val_loss: 0.0846 - val_acc: 0.9759\n",
      "Epoch 168/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0837 - acc: 0.9747 - val_loss: 0.0842 - val_acc: 0.9759\n",
      "Epoch 169/250\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.0853 - acc: 0.9740 - val_loss: 0.0838 - val_acc: 0.9758\n",
      "Epoch 170/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0871 - acc: 0.9734 - val_loss: 0.0833 - val_acc: 0.9762\n",
      "Epoch 171/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0854 - acc: 0.9738 - val_loss: 0.0831 - val_acc: 0.9766\n",
      "Epoch 172/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0845 - acc: 0.9738 - val_loss: 0.0832 - val_acc: 0.9762\n",
      "Epoch 173/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0858 - acc: 0.9739 - val_loss: 0.0839 - val_acc: 0.9762\n",
      "Epoch 174/250\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0826 - acc: 0.9749 - val_loss: 0.0832 - val_acc: 0.9763\n",
      "Epoch 175/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0857 - acc: 0.9732 - val_loss: 0.0836 - val_acc: 0.9767\n",
      "Epoch 176/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0807 - acc: 0.9753 - val_loss: 0.0837 - val_acc: 0.9767\n",
      "Epoch 177/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0823 - acc: 0.9751 - val_loss: 0.0828 - val_acc: 0.9767\n",
      "Epoch 178/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0825 - acc: 0.9746 - val_loss: 0.0827 - val_acc: 0.9768\n",
      "Epoch 179/250\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.0811 - acc: 0.9750 - val_loss: 0.0822 - val_acc: 0.9763\n",
      "Epoch 180/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0835 - acc: 0.9742 - val_loss: 0.0830 - val_acc: 0.9768\n",
      "Epoch 181/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0811 - acc: 0.9748 - val_loss: 0.0818 - val_acc: 0.9769\n",
      "Epoch 182/250\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.0784 - acc: 0.9758 - val_loss: 0.0826 - val_acc: 0.9769\n",
      "Epoch 183/250\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.0803 - acc: 0.9758 - val_loss: 0.0823 - val_acc: 0.9769\n",
      "Epoch 184/250\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 0.0807 - acc: 0.9751 - val_loss: 0.0817 - val_acc: 0.9767\n",
      "Epoch 185/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0791 - acc: 0.9756 - val_loss: 0.0822 - val_acc: 0.9767\n",
      "Epoch 186/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0806 - acc: 0.9755 - val_loss: 0.0824 - val_acc: 0.9768\n",
      "Epoch 187/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0784 - acc: 0.9755 - val_loss: 0.0820 - val_acc: 0.9765\n",
      "Epoch 188/250\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.0795 - acc: 0.9753 - val_loss: 0.0812 - val_acc: 0.9769\n",
      "Epoch 189/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0775 - acc: 0.9759 - val_loss: 0.0821 - val_acc: 0.9767\n",
      "Epoch 190/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0775 - acc: 0.9759 - val_loss: 0.0821 - val_acc: 0.9767\n",
      "Epoch 191/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0780 - acc: 0.9768 - val_loss: 0.0822 - val_acc: 0.9763\n",
      "Epoch 192/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0797 - acc: 0.9759 - val_loss: 0.0820 - val_acc: 0.9772\n",
      "Epoch 193/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0775 - acc: 0.9764 - val_loss: 0.0818 - val_acc: 0.9771\n",
      "Epoch 194/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0779 - acc: 0.9762 - val_loss: 0.0818 - val_acc: 0.9768\n",
      "Epoch 195/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0768 - acc: 0.9763 - val_loss: 0.0820 - val_acc: 0.9764\n",
      "Epoch 196/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0742 - acc: 0.9770 - val_loss: 0.0817 - val_acc: 0.9763\n",
      "Epoch 197/250\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.0769 - acc: 0.9758 - val_loss: 0.0816 - val_acc: 0.9768\n",
      "Epoch 198/250\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.0749 - acc: 0.9770 - val_loss: 0.0814 - val_acc: 0.9769\n",
      "Epoch 199/250\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.0750 - acc: 0.9771 - val_loss: 0.0811 - val_acc: 0.9770\n",
      "Epoch 200/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0742 - acc: 0.9767 - val_loss: 0.0809 - val_acc: 0.9766\n",
      "Epoch 201/250\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0746 - acc: 0.9763 - val_loss: 0.0814 - val_acc: 0.9767\n",
      "Epoch 202/250\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.0781 - acc: 0.9763 - val_loss: 0.0810 - val_acc: 0.9772\n",
      "Epoch 203/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0748 - acc: 0.9772 - val_loss: 0.0809 - val_acc: 0.9769\n",
      "Epoch 204/250\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.0719 - acc: 0.9776 - val_loss: 0.0810 - val_acc: 0.9770\n",
      "Epoch 205/250\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.0752 - acc: 0.9766 - val_loss: 0.0813 - val_acc: 0.9772\n",
      "Epoch 206/250\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.0731 - acc: 0.9772 - val_loss: 0.0813 - val_acc: 0.9772\n",
      "Epoch 207/250\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.0724 - acc: 0.9774 - val_loss: 0.0811 - val_acc: 0.9762\n",
      "Epoch 208/250\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.0725 - acc: 0.9771 - val_loss: 0.0814 - val_acc: 0.9769\n",
      "Epoch 209/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0731 - acc: 0.9775 - val_loss: 0.0813 - val_acc: 0.9767\n",
      "Epoch 210/250\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.0710 - acc: 0.9779 - val_loss: 0.0815 - val_acc: 0.9766\n",
      "Epoch 211/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0733 - acc: 0.9767 - val_loss: 0.0819 - val_acc: 0.9770\n",
      "Epoch 212/250\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.0739 - acc: 0.9770 - val_loss: 0.0814 - val_acc: 0.9768\n",
      "Epoch 213/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0724 - acc: 0.9779 - val_loss: 0.0808 - val_acc: 0.9776\n",
      "Epoch 214/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0721 - acc: 0.9779 - val_loss: 0.0810 - val_acc: 0.9776\n",
      "Epoch 215/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0705 - acc: 0.9784 - val_loss: 0.0808 - val_acc: 0.9772\n",
      "Epoch 216/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0707 - acc: 0.9776 - val_loss: 0.0808 - val_acc: 0.9772\n",
      "Epoch 217/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0706 - acc: 0.9783 - val_loss: 0.0807 - val_acc: 0.9776\n",
      "Epoch 218/250\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0687 - acc: 0.9787 - val_loss: 0.0810 - val_acc: 0.9772\n",
      "Epoch 219/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0706 - acc: 0.9780 - val_loss: 0.0802 - val_acc: 0.9772\n",
      "Epoch 220/250\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.0689 - acc: 0.9789 - val_loss: 0.0803 - val_acc: 0.9770\n",
      "Epoch 221/250\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 0.0705 - acc: 0.9779 - val_loss: 0.0811 - val_acc: 0.9767\n",
      "Epoch 222/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0687 - acc: 0.9786 - val_loss: 0.0808 - val_acc: 0.9769\n",
      "Epoch 223/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0707 - acc: 0.9773 - val_loss: 0.0807 - val_acc: 0.9771\n",
      "Epoch 224/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0699 - acc: 0.9791 - val_loss: 0.0804 - val_acc: 0.9772\n",
      "Epoch 225/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0673 - acc: 0.9788 - val_loss: 0.0806 - val_acc: 0.9780\n",
      "Epoch 226/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0683 - acc: 0.9788 - val_loss: 0.0805 - val_acc: 0.9770\n",
      "Epoch 227/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0667 - acc: 0.9792 - val_loss: 0.0807 - val_acc: 0.9772\n",
      "Epoch 228/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0693 - acc: 0.9788 - val_loss: 0.0806 - val_acc: 0.9769\n",
      "Epoch 229/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0687 - acc: 0.9786 - val_loss: 0.0806 - val_acc: 0.9769\n",
      "Epoch 230/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0684 - acc: 0.9787 - val_loss: 0.0801 - val_acc: 0.9768\n",
      "Epoch 231/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0671 - acc: 0.9793 - val_loss: 0.0799 - val_acc: 0.9774\n",
      "Epoch 232/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0681 - acc: 0.9786 - val_loss: 0.0801 - val_acc: 0.9772\n",
      "Epoch 233/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0647 - acc: 0.9795 - val_loss: 0.0805 - val_acc: 0.9775\n",
      "Epoch 234/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0678 - acc: 0.9784 - val_loss: 0.0802 - val_acc: 0.9777\n",
      "Epoch 235/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0663 - acc: 0.9793 - val_loss: 0.0794 - val_acc: 0.9774\n",
      "Epoch 236/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0686 - acc: 0.9787 - val_loss: 0.0794 - val_acc: 0.9772\n",
      "Epoch 237/250\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.0650 - acc: 0.9796 - val_loss: 0.0800 - val_acc: 0.9774\n",
      "Epoch 238/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0664 - acc: 0.9793 - val_loss: 0.0804 - val_acc: 0.9774\n",
      "Epoch 239/250\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0669 - acc: 0.9794 - val_loss: 0.0807 - val_acc: 0.9778\n",
      "Epoch 240/250\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 0.0687 - acc: 0.9787 - val_loss: 0.0803 - val_acc: 0.9775\n",
      "Epoch 241/250\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.0648 - acc: 0.9798 - val_loss: 0.0805 - val_acc: 0.9776\n",
      "Epoch 242/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0656 - acc: 0.9789 - val_loss: 0.0798 - val_acc: 0.9778\n",
      "Epoch 243/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0658 - acc: 0.9793 - val_loss: 0.0795 - val_acc: 0.9780\n",
      "Epoch 244/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0655 - acc: 0.9796 - val_loss: 0.0798 - val_acc: 0.9777\n",
      "Epoch 245/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0634 - acc: 0.9806 - val_loss: 0.0798 - val_acc: 0.9773\n",
      "Epoch 246/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0628 - acc: 0.9803 - val_loss: 0.0810 - val_acc: 0.9772\n",
      "Epoch 247/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0621 - acc: 0.9803 - val_loss: 0.0801 - val_acc: 0.9777\n",
      "Epoch 248/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0632 - acc: 0.9804 - val_loss: 0.0802 - val_acc: 0.9772\n",
      "Epoch 249/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0635 - acc: 0.9799 - val_loss: 0.0804 - val_acc: 0.9777\n",
      "Epoch 250/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0616 - acc: 0.9808 - val_loss: 0.0801 - val_acc: 0.9775\n",
      "10000/10000 [==============================] - 0s 21us/step\n",
      "Test score: 0.07742765812133438\n",
      "Test accuracy: 0.9779\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Dropout\n",
    "\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 250\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10                           # number of outputs = number of digits\n",
    "OPTIMIZER = SGD()                         # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2                      # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3                             # Dropout                                         *********\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# M_HIDDEN hidden layers 10 outputs\n",
    "model = Sequential()\n",
    "\n",
    "#First Layer with dropout\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "#Second Layer with dropout\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "#Output Layer\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE, \n",
    "                    validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this two hidden layers and dropout we had the next result:\n",
    "    - Training: 98.08%\n",
    "    - Validation: 97.75%\n",
    "    - Test: 97.79%\n",
    "\n",
    "If we run the same network but only with 20 epoch we will have:\n",
    "    - Training: 91.54%\n",
    "    - Validation: 94.48%\n",
    "    - Test: 94.25%\n",
    "\n",
    "**Important:** It tell us the 20 epochs is not appropriate to training the model because the Training Accuracy needs to be ABOVE of Test accuracy. Due to that, it was necessary to increase the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to observe how **accuracy increases** on training and test sets **when the number of epochs\n",
    "increases**. As you can see in the following graph, these two curves touch at about 250 epochs, and\n",
    "therefore, there is no need to train further after that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it has been frequently observed that networks with random dropout in internal hidden layers\n",
    "can generalize better on unseen examples contained in test sets. Intuitively, one can think of this as\n",
    "each neuron becoming more capable because it knows it cannot depend on its neighbors. During\n",
    "testing, there is no dropout, so we are now using all our highly tuned neurons. In short, it is generally\n",
    "a good approach to test how a net performs when some dropout function is adopted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
