{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "The TensorFlow library has made our lives easier by introducing multiple\n",
    "predefined functions to be used in the implementation of word2vec\n",
    "algorithms.\n",
    "\n",
    "\n",
    "This notebook includes the implementation for both the\n",
    "word2vec algos, skip-gram.\n",
    "\n",
    "https://www.tensorflow.org/tutorials/word2vec\n",
    "\n",
    "**Note:** The data used for our exercise is a compressed format of the\n",
    "English Wikipedia dump made on March 3, 2006. It is available from the\n",
    "following link: http://mattmahoney.net/dc/textdata.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Importing the required packages\"\"\"\n",
    "import random\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "import re \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pylab\n",
    "%matplotlib inline\n",
    "\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the required packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Make sure the dataset link is copied correctly\"\"\"\n",
    "\n",
    "dataset_link = 'http://mattmahoney.net/dc/'\n",
    "zip_file = 'text8.zip'\n",
    "\n",
    "def data_download(zip_file):\n",
    "    \"\"\"Downloading the required file\"\"\"\n",
    "    if not os.path.exists(zip_file):\n",
    "        zip_file, _ = urlretrieve(dataset_link + zip_file, zip_file)\n",
    "        print('File downloaded successfully!')\n",
    "    return None\n",
    "\n",
    "data_download(zip_file)\n",
    "\n",
    "\"\"\"Extracting the dataset in separate folder\"\"\"\n",
    "extracted_folder = 'dataset'\n",
    "\n",
    "if not os.path.isdir(extracted_folder):\n",
    "    with zipfile.ZipFile(zip_file) as zf:\n",
    "        zf.extractall(extracted_folder)\n",
    "        \n",
    "with open('dataset/text8') as ft_ :\n",
    "    full_text = ft_.read() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save data into a variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/text8') as ft_ :\n",
    "    full_text = ft_.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to do the treatment of text punctuation**\n",
    "\n",
    "As the input data has multiple punctuation and other symbols\n",
    "across the text, the same are replaced with their respective tokens, with\n",
    "the type of punctuation and symbol name in the token. This helps the\n",
    "model to identify each of the punctuation and other symbols individually\n",
    "and produce a vector. The function text_processing() performs this\n",
    "operation. It takes the Wikipedia text data as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(ft8_text):\n",
    "    \"\"\"Replacing punctuation marks with tokens\"\"\"\n",
    "    ft8_text = ft8_text.lower()\n",
    "    ft8_text = ft8_text.replace('.', ' <period> ')\n",
    "    ft8_text = ft8_text.replace(',', ' <comma> ')\n",
    "    ft8_text = ft8_text.replace('\"', ' <quotation> ')\n",
    "    ft8_text = ft8_text.replace(';', ' <semicolon> ')\n",
    "    ft8_text = ft8_text.replace('!', ' <exclamation> ')\n",
    "    ft8_text = ft8_text.replace('?', ' <question> ')\n",
    "    ft8_text = ft8_text.replace('(', ' <paren_l> ')\n",
    "    ft8_text = ft8_text.replace(')', ' <paren_r> ')\n",
    "    ft8_text = ft8_text.replace('--', ' <hyphen> ')\n",
    "    ft8_text = ft8_text.replace(':', ' <colon> ')\n",
    "    ft8_text_tokens = ft8_text.split()\n",
    "    \n",
    "    return ft8_text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_tokens = text_processing(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecting the words with frequency higher than a threshold**\n",
    "\n",
    "To improve the quality of the vector representations produced, it is\n",
    "recommended to remove the noise related to the words, i.e., words with a\n",
    "frequency of less than 7 in the input dataset, as these words will not have\n",
    "enough information to provide the context they are present in.\n",
    "One can change this threshold by checking the distribution of the word\n",
    "count and in the dataset. For convenience, we have taken it as 7 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_tokens = text_processing(full_text)\n",
    "\"\"\"Shortlisting words with frequency more than 7\"\"\"\n",
    "word_cnt = collections.Counter(ft_tokens)\n",
    "shortlisted_words = [w for w in ft_tokens if word_cnt[w] > 7 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the top words present in the dataset on the basis of their\n",
    "frequency, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including']\n"
     ]
    }
   ],
   "source": [
    "print(shortlisted_words[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the stats of the total words present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of shortlisted words :  16616688\n",
      "Unique number of shortlisted words :  53721\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of shortlisted words : \",len(shortlisted_words))\n",
    "print(\"Unique number of shortlisted words : \",len(set(shortlisted_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oder words by the frequency**\n",
    "\n",
    "To process the unique words present in the corpus, we have made a\n",
    "set of the words, followed by their frequency in the training dataset. The\n",
    "following function creates a dictionary and converts words to integers\n",
    "and, conversely, integers to words. The most frequent word is assigned the\n",
    "least value, 0, and in similar fashion, numbers are assigned to other words.\n",
    "Conversion of words to integers has been stored in a separate list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The function creates a dictionary of the words present in dataset along with their frequency order\"\"\"\n",
    "def dict_creation(shortlisted_words):\n",
    "    counts = collections.Counter(shortlisted_words)\n",
    "    vocabulary = sorted(counts, key=counts.get, reverse=True)\n",
    "    rev_dictionary_ = {ii: word for ii, word in enumerate(vocabulary)}\n",
    "    dictionary_ = {word: ii for ii, word in rev_dictionary_.items()}\n",
    "    return dictionary_, rev_dictionary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_, rev_dictionary_ = dict_creation(shortlisted_words)\n",
    "words_cnt = [dictionary_[word] for word in shortlisted_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram\n",
    "\n",
    "All the words with higher frequency and\n",
    "without any significant context around the center words are removed by\n",
    "putting a threshold on their frequency. This results in faster training and\n",
    "better word vector representations.\n",
    "\n",
    "We have made use of the probability score function given in\n",
    "the paper on skip-gram for the implementation here. For each word,\n",
    "$w_i$, in the training set, weâ€™ll discard it with the probability given by: \n",
    "\n",
    "\n",
    "$P(w_i)= 1- \\left( \\sqrt \\frac{t}{f(w_i)} \\right) $\n",
    "\n",
    "where $t$ is a threshold parameter and $f(w_i)$ is the frequency of word $w_i$ in the total dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating the threshold and performing the subsampling\"\"\"\n",
    "thresh = 0.00005\n",
    "word_counts = collections.Counter(words_cnt)\n",
    "total_count = len(words_cnt)\n",
    "freqs = {word: count / total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1 - np.sqrt(thresh/freqs[word]) for word in word_counts}\n",
    "train_words = [word for word in words_cnt if p_drop[word] < random.random()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the skip-gram model takes the center word and predicts words\n",
    "surrounding it, the \n",
    "\n",
    "    skipG_target_set_generation() \n",
    "\n",
    "function creates the\n",
    "input for the skip-gram model in the desired format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipG_target_set_generation(batch_, batch_index, word_window): \n",
    "    \"\"\"The function combines the words of given word_window size next to the index, for the SkipGram model\"\"\"\n",
    "    random_num = np.random.randint(1, word_window+1)\n",
    "    words_start = batch_index - random_num if (batch_index - random_num) > 0 else 0\n",
    "    words_stop = batch_index + random_num\n",
    "    window_target = set(batch_[words_start:batch_index] + batch_[batch_index+1:words_stop+1])\n",
    "    return list(window_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \n",
    "\n",
    "    skipG_batch_creation()\n",
    "\n",
    "function makes use of the \n",
    "\n",
    "    skipG_target_set_generation()\n",
    "\n",
    "function and creates a combined format of the\n",
    "center word and the words surrounding it on either side as target text and\n",
    "returns the batch output, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipG_batch_creation(short_words, batch_length, word_window):\n",
    "    \"\"\"The function internally makes use of the skipG_target_set_generation() function and combines each of the label \n",
    "    words in the shortlisted_words with the words of word_window size around\"\"\"\n",
    "    batch_cnt = len(short_words)//batch_length\n",
    "    short_words = short_words[:batch_cnt*batch_length]  \n",
    "    \n",
    "    for word_index in range(0, len(short_words), batch_length):\n",
    "        input_words, label_words = [], []\n",
    "        word_batch = short_words[word_index:word_index+batch_length]\n",
    "        for index_ in range(len(word_batch)):\n",
    "            batch_input = word_batch[index_]\n",
    "            batch_label = skipG_target_set_generation(word_batch, index_, word_window)\n",
    "            # Appending the label and inputs to the initial list. Replicating input to the size of labels in the window \n",
    "            label_words.extend(batch_label)\n",
    "            input_words.extend([batch_input]*len(batch_label))\n",
    "        yield input_words, label_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code registers a TensorFlow graph for use of the\n",
    "skip-gram implementation, declaring the variableâ€™s inputs and labels\n",
    "placeholders, which will be used to assign one-hot-encoded vectors for\n",
    "input words and batches of varying size, as per the combination of the\n",
    "center and surrounding words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ctw00071\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "tf_graph = tf.Graph()\n",
    "with tf_graph.as_default():\n",
    "    input_ = tf.placeholder(tf.int32, [None], name='input_')\n",
    "    label_ = tf.placeholder(tf.int32, [None, None], name='label_')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code following declares variables for the embedding matrix, which\n",
    "has a dimension equal to the size of the vocabulary and the dimension of\n",
    "the word embedding vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf_graph.as_default():\n",
    "    word_embed = tf.Variable(tf.random_uniform((len(rev_dictionary_), 300), -1, 1))\n",
    "    embedding = tf.nn.embedding_lookup(word_embed, input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \n",
    "\n",
    "    tf.train.AdamOptimizer\n",
    "\n",
    "uses Kingma and Ba's Adam algorithm (http://arxiv.org/pdf/1412.6980v8.pdf) to control the learning rate. For further reference, one can refer to the following paper as well by Bengio, http://arxiv.org/pdf/1206.5533.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ctw00071\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:1444: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\ctw00071\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The code includes the following  :\n",
    " # Initializing weights and bias to be used in the softmax layer\n",
    " # Loss function calculation using the Negative Sampling\n",
    " # Usage of Adam Optimizer\n",
    " # Negative sampling on 100 words, to be included in the loss function\n",
    " # 300 is the word embedding vector size\n",
    "\"\"\"\n",
    "vocabulary_size = len(rev_dictionary_)\n",
    "\n",
    "with tf_graph.as_default():\n",
    "    sf_weights = tf.Variable(tf.truncated_normal((vocabulary_size, 300), stddev=0.1) )\n",
    "    sf_bias = tf.Variable(tf.zeros(vocabulary_size) )\n",
    "\n",
    "    loss_fn = tf.nn.sampled_softmax_loss(weights=sf_weights, biases=sf_bias, \n",
    "                                         labels=label_, inputs=embedding, \n",
    "                                         num_sampled=100, num_classes=vocabulary_size)\n",
    "    cost_fn = tf.reduce_mean(loss_fn)\n",
    "    optim = tf.train.AdamOptimizer().minimize(cost_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the word vector representation is holding the semantic\n",
    "similarity among words, a validation set is generated in the following\n",
    "section of code. This will select a combination of common and uncommon\n",
    "words across the corpus and return the words closest to them on the basis\n",
    "of the cosine similarity between the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-26-e08a1bea8a5b>:14: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The below code performs the following operations :\n",
    " # Performing validation here by making use of a random selection of 16 words from the dictionary of desired size\n",
    " # Selecting 8 words randomly from range of 1000    \n",
    " # Using the cosine distance to calculate the similarity between the words \n",
    "\"\"\"\n",
    "with tf_graph.as_default():\n",
    "    validation_cnt = 16\n",
    "    validation_dict = 100\n",
    "    \n",
    "    validation_words = np.array(random.sample(range(validation_dict), validation_cnt//2))\n",
    "    validation_words = np.append(validation_words, random.sample(range(1000,1000+validation_dict), validation_cnt//2))\n",
    "    validation_data = tf.constant(validation_words, dtype=tf.int32)\n",
    "\n",
    "    normalization_embed = word_embed / (tf.sqrt(tf.reduce_sum(tf.square(word_embed), 1, keep_dims=True)))\n",
    "    validation_embed = tf.nn.embedding_lookup(normalization_embed, validation_data)\n",
    "    word_similarity = tf.matmul(validation_embed, tf.transpose(normalization_embed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a folder model_checkpoint in the current working directory to\n",
    "store the model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file model_checkpoint already exists.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Creating the model checkpoint directory\"\"\"\n",
    "!mkdir model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 , Iteration: 100 , Avg. Training loss: 6.1708 , Processing : 0.3223 sec/batch\n",
      "Epoch 1/2 , Iteration: 200 , Avg. Training loss: 6.1667 , Processing : 0.2973 sec/batch\n",
      "Epoch 1/2 , Iteration: 300 , Avg. Training loss: 6.0667 , Processing : 0.3203 sec/batch\n",
      "Epoch 1/2 , Iteration: 400 , Avg. Training loss: 6.0098 , Processing : 0.3148 sec/batch\n",
      "Epoch 1/2 , Iteration: 500 , Avg. Training loss: 5.9529 , Processing : 0.2981 sec/batch\n",
      "Epoch 1/2 , Iteration: 600 , Avg. Training loss: 5.9786 , Processing : 0.2960 sec/batch\n",
      "Epoch 1/2 , Iteration: 700 , Avg. Training loss: 5.8557 , Processing : 0.2977 sec/batch\n",
      "Epoch 1/2 , Iteration: 800 , Avg. Training loss: 5.7385 , Processing : 0.3006 sec/batch\n",
      "Epoch 1/2 , Iteration: 900 , Avg. Training loss: 5.6670 , Processing : 0.2972 sec/batch\n",
      "Epoch 1/2 , Iteration: 1000 , Avg. Training loss: 5.5703 , Processing : 0.3254 sec/batch\n",
      "Epoch 1/2 , Iteration: 1100 , Avg. Training loss: 5.4528 , Processing : 0.3336 sec/batch\n",
      "Epoch 1/2 , Iteration: 1200 , Avg. Training loss: 5.3716 , Processing : 0.3334 sec/batch\n",
      "Epoch 1/2 , Iteration: 1300 , Avg. Training loss: 5.2304 , Processing : 0.3263 sec/batch\n",
      "Epoch 1/2 , Iteration: 1400 , Avg. Training loss: 5.1353 , Processing : 0.2987 sec/batch\n",
      "Epoch 1/2 , Iteration: 1500 , Avg. Training loss: 5.1338 , Processing : 0.2928 sec/batch\n",
      "Epoch 1/2 , Iteration: 1600 , Avg. Training loss: 5.0966 , Processing : 0.3015 sec/batch\n",
      "Epoch 1/2 , Iteration: 1700 , Avg. Training loss: 5.0524 , Processing : 0.2870 sec/batch\n",
      "Epoch 1/2 , Iteration: 1800 , Avg. Training loss: 4.9281 , Processing : 0.2919 sec/batch\n",
      "Epoch 1/2 , Iteration: 1900 , Avg. Training loss: 4.9689 , Processing : 0.3170 sec/batch\n",
      "Epoch 1/2 , Iteration: 2000 , Avg. Training loss: 4.9640 , Processing : 0.2978 sec/batch\n",
      "Nearest to is: determinism, chronometers, gly, engelbert, freak, hekate, shin, penal,\n",
      "Nearest to three: leicestershire, asserted, antagonism, semple, blackface, gollancz, teacher, resumes,\n",
      "Nearest to after: blacks, surprise, slot, sixth, xps, mechanic, fanfare, knightly,\n",
      "Nearest to zero: interwoven, churchyard, persecute, keyboards, isbn, gaborone, moron, reasoner,\n",
      "Nearest to so: oglethorpe, churches, minor, receptive, partnerships, quenched, symphony, smashing,\n",
      "Nearest to with: feudal, normalcy, csa, egoist, zhang, rodan, attention, corrode,\n",
      "Nearest to but: qasim, ancestry, epidemiological, seating, penchant, plantagenet, pople, binomial,\n",
      "Nearest to many: gchq, registered, eruptive, dresden, lehigh, negro, excise, harmonicas,\n",
      "Nearest to versions: ritual, mallory, indictable, olympics, liability, gogo, apathetic, dresser,\n",
      "Nearest to institute: bulges, prematurely, gaps, spirited, overseer, led, swapping, footlights,\n",
      "Nearest to quite: khad, asquith, letterbox, unpaired, colts, orville, mornings, conditioners,\n",
      "Nearest to woman: abercrombie, malmesbury, invest, undercut, knife, cardigan, multiverse, hurley,\n",
      "Nearest to shown: intents, fernandez, yosemite, supremes, taxi, eprom, dragging, brecht,\n",
      "Nearest to pressure: tranquility, canc, vega, pray, forefathers, himalayas, petomane, nls,\n",
      "Nearest to units: publicist, pleasures, determined, jones, beac, arezzo, ci, wally,\n",
      "Nearest to troops: cordilleras, granny, lodovico, annie, riverboat, inhalation, tegmark, boredom,\n",
      "Epoch 1/2 , Iteration: 2100 , Avg. Training loss: 4.9484 , Processing : 0.2899 sec/batch\n",
      "Epoch 1/2 , Iteration: 2200 , Avg. Training loss: 4.9086 , Processing : 0.2992 sec/batch\n",
      "Epoch 1/2 , Iteration: 2300 , Avg. Training loss: 4.8806 , Processing : 0.2986 sec/batch\n",
      "Epoch 1/2 , Iteration: 2400 , Avg. Training loss: 4.8690 , Processing : 0.3428 sec/batch\n",
      "Epoch 1/2 , Iteration: 2500 , Avg. Training loss: 4.8251 , Processing : 0.2971 sec/batch\n",
      "Epoch 1/2 , Iteration: 2600 , Avg. Training loss: 4.8615 , Processing : 0.3040 sec/batch\n",
      "Epoch 1/2 , Iteration: 2700 , Avg. Training loss: 4.8240 , Processing : 0.3167 sec/batch\n",
      "Epoch 1/2 , Iteration: 2800 , Avg. Training loss: 4.7886 , Processing : 0.2874 sec/batch\n",
      "Epoch 1/2 , Iteration: 2900 , Avg. Training loss: 4.7751 , Processing : 0.2895 sec/batch\n",
      "Epoch 1/2 , Iteration: 3000 , Avg. Training loss: 4.8082 , Processing : 0.3025 sec/batch\n",
      "Epoch 1/2 , Iteration: 3100 , Avg. Training loss: 4.7636 , Processing : 0.2991 sec/batch\n",
      "Epoch 1/2 , Iteration: 3200 , Avg. Training loss: 4.7310 , Processing : 0.2935 sec/batch\n",
      "Epoch 1/2 , Iteration: 3300 , Avg. Training loss: 4.6953 , Processing : 0.3032 sec/batch\n",
      "Epoch 1/2 , Iteration: 3400 , Avg. Training loss: 4.7409 , Processing : 0.3372 sec/batch\n",
      "Epoch 1/2 , Iteration: 3500 , Avg. Training loss: 4.7309 , Processing : 0.2897 sec/batch\n",
      "Epoch 1/2 , Iteration: 3600 , Avg. Training loss: 4.6719 , Processing : 0.3463 sec/batch\n",
      "Epoch 1/2 , Iteration: 3700 , Avg. Training loss: 4.7099 , Processing : 0.3160 sec/batch\n",
      "Epoch 1/2 , Iteration: 3800 , Avg. Training loss: 4.7131 , Processing : 0.3155 sec/batch\n",
      "Epoch 1/2 , Iteration: 3900 , Avg. Training loss: 4.6532 , Processing : 0.3619 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-917a88af920d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             train_loss, _ = sess.run([cost_fn, optim], \n\u001b[1;32m---> 18\u001b[1;33m                                      feed_dict={input_: x, label_: np.array(y)[:, None]})\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\test_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2            # Increase it as per computation resources. It has been kept low here for users to replicate the process, increase to 100 or more\n",
    "batch_length = 1000\n",
    "word_window = 10\n",
    "\n",
    "with tf_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=tf_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        batches = skipG_batch_creation(train_words, batch_length, word_window)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            train_loss, _ = sess.run([cost_fn, optim], \n",
    "                                     feed_dict={input_: x, label_: np.array(y)[:, None]})\n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100 == 0: \n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(e, epochs), \", Iteration: {}\".format(iteration),\n",
    "                      \", Avg. Training loss: {:.4f}\".format(loss/100),\", Processing : {:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            if iteration % 2000 == 0:\n",
    "                similarity_ = word_similarity.eval()\n",
    "                for i in range(validation_cnt):\n",
    "                    validated_words = rev_dictionary_[validation_words[i]]\n",
    "                    top_k = 8 # number of nearest neighbors\n",
    "                    nearest = (-similarity_[i, :]).argsort()[1:top_k+1]\n",
    "                    log = 'Nearest to %s:' % validated_words\n",
    "                    for k in range(top_k):\n",
    "                        close_word = rev_dictionary_[nearest[k]]\n",
    "                        log = '%s %s,' % (log, close_word)\n",
    "                    print(log)\n",
    "            \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, \"model_checkpoint/skipGram_text8.ckpt\")\n",
    "    embed_mat = sess.run(normalization_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar output will be printed for all other iterations, and the trained\n",
    "network will have been restored for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The Saver class adds ops to save and restore variables to and from checkpoints.\"\"\"\n",
    "with tf_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=tf_graph) as sess:\n",
    "    \"\"\"Restoring the trained network\"\"\"\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('model_checkpoint'))\n",
    "    embed_mat = sess.run(word_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the t-distributed stochastic neighbor embedding (t-SNE)\n",
    "for the purpose of visualization (https://lvdmaaten.github.io/tsne/).\n",
    "The high-dimensional, 300 vector representation of 250 random words has\n",
    "been used across a two-dimensional vector space. t-SNE ensures that the\n",
    "initial structure of the vector is reserved in the new dimension, even after\n",
    "conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_graph = 250\n",
    "tsne = TSNE()\n",
    "word_embedding_tsne = tsne.fit_transform(embed_mat[:word_graph, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe in Figure 2-13, words with semantic similarity\n",
    "have been placed closer to one another in their representation in the\n",
    "two-dimensional space, thereby retaining their similarity even after the\n",
    "dimensions have been further reduced.\n",
    "\n",
    "For example, words such as year,\n",
    "years, and age have been placed near one another and far from words such as international and religious.\n",
    "\n",
    "The model can be trained for a higher\n",
    "number of iterations, to achieve a better representation of the word\n",
    "embeddings, and further changes can be made in the threshold values, to\n",
    "fine-tune the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "for idx in range(word_graph):\n",
    "    plt.scatter(*word_embedding_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(rev_dictionary_[idx], (word_embedding_tsne[idx, 0], word_embedding_tsne[idx, 1]), alpha=0.6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
