{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence Use Case\n",
    "\n",
    "For the use case of seq2seq models, we have taken **textual content\n",
    "of annotated corpus** used in the research paper _“Development of a\n",
    "benchmark corpus to support the automatic extraction of drug-related\n",
    "adverse effects from medical case reports”_ (www.sciencedirect.com/\n",
    "science/article/pii/S1532046412000615), by H. Gurulingappa.\n",
    "\n",
    "The work presented  can support the development and validation\n",
    "of methods for the automatic extraction of drug-related\n",
    "adverse effects from medical case reports. \n",
    "\n",
    "The documents are systematically double annotated in various rounds to ensure\n",
    "consistent annotations. The annotated documents are finally\n",
    "harmonized to generate representative consensus annotations.\n",
    "\n",
    "The authors used an open source skip-gram model provided\n",
    "by NLPLab (http://evexdb.org/pmresources/vec-space-models/\n",
    "wikipedia-pubmed-and-PMC-w2v.bin), which was\n",
    "trained on all the PubMed abstracts and PMC full texts (4.08\n",
    "million distinct words). The output of skip-gram model is a set\n",
    "of word vectors of 200 dimensions.\n",
    "\n",
    "The ADE corpus used from the paper by Gurulingappa is distributed\n",
    "with three files: DRUG-AE.rel, DRUG-DOSE.rel, and ADE-NEG.txt. We are\n",
    "making use of the DRUG-AE.rel file, which provides relationships between\n",
    "drugs and adverse effects. \n",
    "\n",
    "The format of the DRUG-AE.rel file is as follows, fields are separated by\n",
    "pipe delimiters:\n",
    "\n",
    "- Column-1: PubMed-ID : 10030778\n",
    "- Column-2: Sentence : Intravenous azithromycin-induced ototoxicity.\n",
    "- Column-3: Adverse-Effect : ototoxicity\n",
    "- Column-4: Begin offset of Adverse-Effect at ‘document level’: 43\n",
    "- Column-5: End offset of Adverse-Effect at ‘document level’: 54\n",
    "- Column-6: Drug: azithromycin\n",
    "- Column-7: Begin offset of Drug at ‘document level’: 22\n",
    "- Column-8: End offset of Drug at ‘document level’: 34\n",
    "\n",
    "**Download Data (4.1 GB) from:** http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the required packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required packages\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the Keras and TensorFlow version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)            # Book -> 2.1.2\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)       # Book -> 1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'wikipedia-pubmed-and-PMC-w2v.bin'\n",
    "print('Indexing word vectors')\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE,binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE,binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the text file 'DRUG-AE.rel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the text file 'DRUG-AE.rel' which provides relations between drugs and adverse effects.\n",
    "TEXT_FILE = 'DRUG-AE.rel'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating input for the model**\n",
    "\n",
    "The input for our model is a sequence of characters that was pre-defined with a \n",
    "length of 200, i.e., we will have a dataset of size = “number of original\n",
    "characters-sequence length.”\n",
    "\n",
    "For each input data, i.e., 200-character sequence, next, one character\n",
    "will be our output in one-hot encoded format.\n",
    "We will append the input data fields, along with their corresponding labels, in the **input_data_ae** and **op_labels_ae tensors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Creating lists for the input fields and corresponding labels\n",
    "input_data_ae = []\n",
    "op_labels_ae = []\n",
    "\n",
    "sentences = []\n",
    "\n",
    "f = open(TEXT_FILE, 'r')\n",
    "\n",
    "for each_line in f.readlines():\n",
    "    sent_list = np.zeros([0,200])\n",
    "    labels = np.zeros([0,3])\n",
    "    tokens = each_line.split(\"|\")\n",
    "    sent = tokens[1]\n",
    "    if sent in sentences:\n",
    "        continue\n",
    "    sentences.append(sent)\n",
    "    begin_offset = int(tokens[3])\n",
    "    end_offset = int(tokens[4])\n",
    "    mid_offset = range(begin_offset+1, end_offset)\n",
    "    word_tokens = nltk.word_tokenize(sent)\n",
    "    offset = 0\n",
    "    for each_token in word_tokens:\n",
    "        offset = sent.find(each_token, offset)\n",
    "        offset1 = copy.deepcopy(offset)\n",
    "        offset += len(each_token)\n",
    "        if each_token in punctuation or re.search(r'\\d', each_token):\n",
    "            continue\n",
    "        each_token = each_token.lower()\n",
    "        each_token = re.sub(\"[^A-Za-z\\-]+\",\"\", each_token)\n",
    "        if each_token in word2vec.vocab:\n",
    "            new_word = word2vec.word_vec(each_token)\n",
    "        if offset1 == begin_offset:\n",
    "            sent_list = np.append(sent_list, np.array([new_word]), axis=0)\n",
    "            labels = np.append(labels, np.array([[0,0,1]]), axis=0)\n",
    "        elif offset == end_offset or offset in mid_offset:\n",
    "            sent_list = np.append(sent_list, np.array([new_word]), axis=0)\n",
    "            labels = np.append(labels, np.array([[0,1,0]]), axis=0)\n",
    "        else:\n",
    "            sent_list = np.append(sent_list, np.array([new_word]), axis=0)\n",
    "            labels = np.append(labels, np.array([[1,0,0]]), axis=0)\n",
    "\n",
    "    input_data_ae.append(sent_list)\n",
    "    op_labels_ae.append(labels)\n",
    "input_data_ae = np.array(input_data_ae)\n",
    "op_labels_ae  = np.array(op_labels_ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add padding to the input text**, with the maximum length of the input at\n",
    "any time step being 30 (a safe bet!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_ae = pad_sequences(input_data_ae, maxlen=30, dtype='float64', padding='post')\n",
    "op_labels_ae = pad_sequences(op_labels_ae, maxlen=30, dtype='float64', padding='post')\n",
    "print(len(input_data_ae))\n",
    "print(len(op_labels_ae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,Bidirectional, TimeDistributed\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Train and Validation datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Train and Validation datasets, for 4271 entries, 4000 in train dataset, and 271 in validation dataset\n",
    "x_train= input_data_ae[:4000]\n",
    "x_test = input_data_ae[4000:]\n",
    "y_train = op_labels_ae[:4000]\n",
    "y_test =op_labels_ae[4000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the model architecture**\n",
    "\n",
    "We are going to use one hidden layer of a bidirectional LSTM network, with 300 hidden\n",
    "units and a dropout probability of 0.2. In addition to this, we are making use\n",
    "of a TimeDistributedDense layer, with a dropout probability of 0.2.\n",
    "\n",
    "_Dropout_ is a regularization technique by which, while you’re updating\n",
    "layers of your neural net, you randomly don’t update, or dropout, some\n",
    "of the layer. That is, while updating your neural net layer, you update\n",
    "each node with a probability of 1-dropout, and leave it unchanged with a\n",
    "probability dropout.\n",
    "\n",
    "_Time distributed layers_ are used for RNN (and LSTMs) to maintain a\n",
    "one-to-one mapping between input and output. Assume we have 30 time\n",
    "steps with 200 samples of data, i.e., 30 × 200, and we want to use an RNN\n",
    "with an output of 3. If we don’t use a TimeDistributedDense layer, we will\n",
    "get a 200 × 30 × 3 tensor. So, we have the output flattened with each time\n",
    "step mixed. If we apply the _TimeDistributedDense_ layer, we are going to\n",
    "apply a fully connected dense layer on each of the time steps and get the\n",
    "output separately by time step.\n",
    "\n",
    "We are also using:\n",
    "- Loss Function: categorical_crossentropy \n",
    "- Optimizer: adam\n",
    "- Activation Function: softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 1      # Making the batch size as 1, as showing model each of the instances one-by-one\n",
    "\n",
    "# Adding Bidirectional LSTM with Dropout, and Time Distributed layer with Dropout\n",
    "# Finally using Adam optimizer for training purpose\n",
    "xin = Input(batch_shape=(batch,30,200), dtype='float')\n",
    "seq = Bidirectional(LSTM(300, return_sequences=True),merge_mode='concat')(xin)\n",
    "mlp1 = Dropout(0.2)(seq)\n",
    "mlp2 = TimeDistributed(Dense(60, activation='softmax'))(mlp1)\n",
    "mlp3 = Dropout(0.2)(mlp2)\n",
    "mlp4 = TimeDistributed(Dense(3, activation='softmax'))(mlp3)\n",
    "model = Model(inputs=xin, outputs=mlp4)\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the model**\n",
    "\n",
    "We are going train our model with 50 epochs and a batch size of 1.\n",
    "You can always increase the number of epochs, as long as the model keeps\n",
    "on improving. One can also create checkpoints, so that later, the model\n",
    "can be retrieved and used. The idea behind creating the checkpoint is to\n",
    "save the model weights while training, so that later, you do not have to go\n",
    "through the same process again. **This has been left as an exercise for the\n",
    "reader.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch,\n",
    "          epochs=50,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation of the Model**\n",
    "\n",
    "Validating the model results on the validation dataset with 271 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict(x_test,batch_size=batch)\n",
    "labels = []\n",
    "for i in range(len(val_pred)):\n",
    "    b = np.zeros_like(val_pred[i])\n",
    "    b[np.arange(len(val_pred[i])), val_pred[i].argmax(1)] = 1\n",
    "    labels.append(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the model performance using F1-score, along with precision and recall**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score =[]\n",
    "f1 = []\n",
    "precision =[]\n",
    "recall =[]\n",
    "point = []\n",
    "\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if(f1_score(labels[i],y_test[i],average='weighted')>.6):\n",
    "        point.append(i)\n",
    "    score.append(f1_score(labels[i],y_test[i],average='weighted'))\n",
    "    precision.append(precision_score(labels[i],y_test[i],average='weighted'))\n",
    "    recall.append(recall_score(labels[i],y_test[i],average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(score))\n",
    "print(np.mean(precision))\n",
    "print(np.mean(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)\n",
    "print(\"\\n------x------\\n\")\n",
    "print(precision)\n",
    "print(\"\\n------x------\\n\")\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** To get better results we can \n",
    "to build a denser network, increasing the number of epochs and the\n",
    "length of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
