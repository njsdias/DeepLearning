{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification\n",
    "\n",
    "We have made use of the Internet Movie Database, popularly known as\n",
    "IMDb (www.imdb.com), to select the dataset for the sentiment classification\n",
    "problem. It offers a great number of datasets, both image and text, which\n",
    "are useful for multiple research activities in deep learning and data\n",
    "analysis.\n",
    "\n",
    "For sentiment classification, we have made use of a set of 25,000\n",
    "movie reviews, which have their positive and negative label attached.\n",
    "The publicly available reviews have been already preprocessed and are\n",
    "encoded as a sequence of word indexes, i.e., integers. The words are ordered on the basis of their overall frequency in the dataset, i.e., the token\n",
    "or word with the second-highest frequency has been indexed as 2, and so\n",
    "on. Attaching such an index to the words will help in shortlisting the words\n",
    "on the basis of their frequency, such as to pick the top 2,000 most common\n",
    "words or remove the top-10 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf_nightly\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/c8/2c38f2e1d9a2a40d368e05f870e57c5e104d4a2a16ee45bd53ac56fc8e30/tf_nightly-1.14.1.dev20190515-cp36-cp36m-win_amd64.whl\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\ctw00071\\appdata\\local\\continuum\\miniconda3\\envs\\neuralnets\\lib\\site-packages (from tf_nightly) (3.7.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\ctw00071\\appdata\\local\\continuum\\miniconda3\\envs\\neuralnets\\lib\\site-packages (from tf_nightly) (0.7.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in c:\\users\\ctw00071\\appdata\\local\\continuum\\miniconda3\\envs\\neuralnets\\lib\\site-packages (from tf_nightly) (1.16.3)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\ctw00071\\appdata\\local\\continuum\\miniconda3\\envs\\neuralnets\\lib\\site-packages (from tf_nightly) (0.33.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ctw00071\\appdata\\local\\continuum\\miniconda3\\envs\\neuralnets\\lib\\site-packages (from tf_nightly) (1.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\ctw00071\\appdata\\local\\continuum\\miniconda3\\envs\\neuralnets\\lib\\site-packages (from tf_nightly) (0.7.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\ctw00071\\appdata\\local\\continuum\\miniconda3\\envs\\neuralnets\\lib\\site-packages (from tf_nightly) (1.0.7)\n",
      "Requirement already satisfied: gast>=0.2.0 in c:\\users\\ctw00071\\appdata\\local\\continuum\\miniconda3\\envs\\neuralnets\\lib\\site-packages (from tf_nightly) (0.2.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\ctw00071\\appdata\\local\\continuum\\miniconda3\\envs\\neuralnets\\lib\\site-packages (from tf_nightly) (1.20.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\ctw00071\\appdata\\local\\continuum\\miniconda3\\envs\\neuralnets\\lib\\site-packages (from tf_nightly) (1.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\ctw00071\\appdata\\local\\continuum\\miniconda3\\envs\\neuralnets\\lib\\site-packages (from tf_nightly) (1.0.9)\n",
      "Collecting wrapt>=1.11.1 (from tf_nightly)\n",
      "Collecting tb-nightly<1.15.0a0,>=1.14.0a0 (from tf_nightly)\n",
      "  Using cached https://files.pythonhosted.org/packages/ab/9f/ef1a3fea8d6426508ff4849d8f85ce4a86ebae5d1f03c65705e950cf9e0c/tb_nightly-1.14.0a20190515-py3-none-any.whl\n",
      "Collecting google-pasta>=0.1.6 (from tf_nightly)\n",
      "  Using cached https://files.pythonhosted.org/packages/f9/68/a14620bfb042691f532dcde8576ff82ee82e4c003cdc0a3dbee5f289cee6/google_pasta-0.1.6-py3-none-any.whl\n",
      "Collecting tf-estimator-nightly (from tf_nightly)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/3b/aadf6561d786f073509982cf842b14f806198918b610ca049dbd374cd4b9/tf_estimator_nightly-1.14.0.dev2019051501-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in c:\\users\\ctw00071\\appdata\\local\\continuum\\miniconda3\\envs\\neuralnets\\lib\\site-packages (from protobuf>=3.6.1->tf_nightly) (41.0.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\ctw00071\\appdata\\local\\continuum\\miniconda3\\envs\\neuralnets\\lib\\site-packages (from keras-applications>=1.0.6->tf_nightly) (2.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ctw00071\\appdata\\local\\continuum\\miniconda3\\envs\\neuralnets\\lib\\site-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf_nightly) (3.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\ctw00071\\appdata\\local\\continuum\\miniconda3\\envs\\neuralnets\\lib\\site-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf_nightly) (0.15.4)\n",
      "Installing collected packages: wrapt, tb-nightly, google-pasta, tf-estimator-nightly, tf-nightly\n",
      "Successfully installed google-pasta-0.1.6 tb-nightly-1.14.0a20190515 tf-estimator-nightly-1.14.0.dev2019051501 tf-nightly-1.14.1.dev20190515 wrapt-1.11.1\n"
     ]
    }
   ],
   "source": [
    "# keras.datasets.imdb is broken in 1.13 and 1.14, by np 1.16.3\n",
    "!pip install tf_nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.1-dev20190515\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fix the error: allow_pickle=True:**\n",
    "\n",
    "Downgraded numpy version from 1.16.3 to 1.16.1 which default value is still allow_pickle=True. \n",
    "Just remove current version then install the 1.16.1 version via:\n",
    "\n",
    "pip uninstall numpy\n",
    "pip install --upgrade numpy==1.16.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the IMDB dataset**\n",
    "The IMDB dataset comes packaged with TensorFlow. It has already been preprocessed such that the reviews (sequences of words) have been converted to sequences of integers, where each integer represents a specific word in a dictionary.\n",
    "\n",
    "The following code downloads the IMDB dataset to your machine (or uses a cached copy if you've already downloaded it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train), (X_test,y_test) = imdb.load_data(num_words=1000, index_from=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert the integers back to words**\n",
    "\n",
    "It may be useful to know how to convert integers back to text. \n",
    "Here, we'll create a helper function to query a dictionary object that contains the integer to string mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the word index used for encoding the sequences\n",
    "vocab_to_int = imdb.get_word_index()\n",
    "\n",
    "# The first indices are reserved\n",
    "vocab_to_int = {k:(v+3) for k,v in vocab_to_int.items()}  # Starting from word index offset onward\n",
    "# Creating indexes for the special characters : Padding, Start Token, Unknown words\n",
    "vocab_to_int[\"<PAD>\"] = 0\n",
    "vocab_to_int[\"<GO>\"] = 1\n",
    "vocab_to_int[\"<UNK>\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_vocab = {value:key for key,value in vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<GO> this film was just brilliant casting <UNK> <UNK> story direction <UNK> really <UNK> the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same <UNK> <UNK> as myself so i loved the fact there was a real <UNK> with this film the <UNK> <UNK> throughout the film were great it was just brilliant so much that i <UNK> the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the <UNK> <UNK> was amazing really <UNK> at the end it was so sad and you know what they say if you <UNK> at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of <UNK> and paul they were just brilliant children are often left out of the <UNK> <UNK> i think because the stars that play them all <UNK> up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so <UNK> because it was true and was <UNK> life after all that was <UNK> with us all\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(int_to_vocab[id] for id in X_train[0] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another way to do the same thing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in vocab_to_int.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<GO> this film was just brilliant casting <UNK> <UNK> story direction <UNK> really <UNK> the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same <UNK> <UNK> as myself so i loved the fact there was a real <UNK> with this film the <UNK> <UNK> throughout the film were great it was just brilliant so much that i <UNK> the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the <UNK> <UNK> was amazing really <UNK> at the end it was so sad and you know what they say if you <UNK> at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of <UNK> and paul they were just brilliant children are often left out of the <UNK> <UNK> i think because the stars that play them all <UNK> up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so <UNK> because it was true and was <UNK> life after all that was <UNK> with us all\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Train and Test datasets from labeled movie reviews \n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(path=\"imdb_full.pkl\",\n",
    "                                                      num_words=None,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=2,\n",
    "                                                      index_from=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [item for sublist in X_train for item in sublist]\n",
    "vocabulary = len(set(t))+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**shows the distribution of the word count in reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1aac48696d8>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = [len(x) for x in X_train]\n",
    "plt.plot(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify a maximum length for the selection of a sequence from the sentence**\n",
    "\n",
    "If the review length is lower than it, append the newly\n",
    "created sequence with padding, up to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200 # specifying the max length of the sequence in the sentence\n",
    "x_filter = []\n",
    "y_filter = []\n",
    "\n",
    "# if the selected length is lesser than the specified max_length, 200, then appending padding (0), else only selecting\n",
    "#       desired length only from sentence\n",
    "for i in range(len(X_train)):\n",
    "    if len(X_train[i])<max_length:\n",
    "        a = len(X_train[i])\n",
    "        X_train[i] = X_train[i] + [0] * (max_length - a)\n",
    "        x_filter.append(X_train[i])\n",
    "        y_filter.append(y_train[i])\n",
    "    elif len(X_train[i])>max_length:\n",
    "        X_train[i] = X_train[i][0:max_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Hyperparameters**\n",
    "\n",
    "Declare the model hyperparameters with word embedding size,\n",
    "number of hidden units, learning rate, batch size, and total number of\n",
    "training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaring the hyper params\n",
    "embedding_size = 100 # word vector size for initializing the word embeddings\n",
    "n_hidden = 200\n",
    "learning_rate = 0.06\n",
    "training_iters = 100000\n",
    "batch_size = 32\n",
    "beta =0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional Parameters**\n",
    "\n",
    "There are related to the current model architecture \n",
    "\n",
    "- dataset\n",
    "- max_length\n",
    "- number of classes to classify \n",
    "\n",
    "in, number of units in hidden layer of self-attention MLP, and number of rows in matrix embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = max_length  # timestepswords\n",
    "n_classes = 2         # 0/1 : binary classification for negative and positive reviews\n",
    "da = 350              # hyper-parameter : Self-attention MLP has hidden layer with da units\n",
    "r = 30                # count of different parts to be extracted from sentence (= # rows in matrix embedding)\n",
    "display_step =10 \n",
    "hidden_units = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the training dataset values and labels in the desired format\n",
    "of array post transformation and encoding, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(pd.get_dummies(y_filter))\n",
    "X_train = np.asarray([np.asarray(g) for g in x_filter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an internal folder to record logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = './recent_logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAME = \"lstmumich{}\".format(int(time.time()))\n",
    "#tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class, to yield random data in batches of given batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIterator:\n",
    "    def __init__(self, data1,data2, batch_size):\n",
    "        self.data1 = data1\n",
    "        self.data2 = data2\n",
    "        self.batch_size = batch_size\n",
    "        self.iter = self.make_random_iter()\n",
    "        \n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            idxs = next(self.iter)\n",
    "        except StopIteration:\n",
    "            self.iter = self.make_random_iter()\n",
    "            idxs = next(self.iter)\n",
    "        X =[self.data1[i] for i in idxs]\n",
    "        Y =[self.data2[i] for i in idxs]\n",
    "        \n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "    def make_random_iter(self):\n",
    "        splits = np.arange(self.batch_size, len(self.data1), self.batch_size)\n",
    "        it = np.split(np.random.permutation(range(len(self.data1))), splits)[:-1]\n",
    "        return iter(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize weights and biases and input**\n",
    "\n",
    "The general rule for setting the weights in a neural network is to be close\n",
    "to zero, without being too small. A good practice is to start your weights in\n",
    "the range of [−y, y], where: \n",
    "\n",
    "$ y = \\frac{1} {\\sqrt{n}} $\n",
    "\n",
    "(n is the number of inputs to a given\n",
    "neuron)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Graph Creation ################      \n",
    "\n",
    "# TF Graph Input\n",
    "with tf.name_scope(\"weights\"):\n",
    "    Win  = tf.Variable(tf.random_uniform([n_hidden*r, hidden_units],\n",
    "                                          -1/np.sqrt(n_hidden),\n",
    "                                          1/np.sqrt(n_hidden)), name='W-input')\n",
    "    Wout = tf.Variable(tf.random_uniform([hidden_units, n_classes],\n",
    "                                          -1/np.sqrt(hidden_units),\n",
    "                                          1/np.sqrt(hidden_units)), name='W-out')\n",
    "    Ws1  = tf.Variable(tf.random_uniform([da,n_hidden],\n",
    "                                          -1/np.sqrt(da),\n",
    "                                         1/np.sqrt(da)), name='Ws1')\n",
    "    \n",
    "    Ws2  = tf.Variable(tf.random_uniform([r,da],\n",
    "                                         -1/np.sqrt(r),\n",
    "                                         1/np.sqrt(r)), name='Ws2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"biases\"):            \n",
    "    biasesout = tf.Variable(tf.random_normal([n_classes]), name='biases-out')\n",
    "    biasesin  = tf.Variable(tf.random_normal([hidden_units]), name='biases-in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(\"int32\", [32,max_length], name='x-input')\n",
    "    y = tf.placeholder(\"int32\", [32, 2], name='y-input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors in the same default graph context with the embedded\n",
    "vectors. This takes the embedding matrix and an input tensor, such as the\n",
    "review vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('embedding'):\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary, embedding_size],-1, 1), name='embeddings')\n",
    "    embed = tf.nn.embedding_lookup(embeddings,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(sequence):\n",
    "    # Computing maximum of elements across dimensions of a tensor\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))   \n",
    "    \n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse the weights and biases using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0516 14:01:46.109194 47876 deprecation.py:323] From <ipython-input-49-4fa4132a33fc>:2: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('forward',reuse=True):\n",
    "        lstm_fw_cell = rnn_cell.BasicLSTMCell(n_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('model'):  \n",
    "    outputs, states = rnn.dynamic_rnn(lstm_fw_cell,\n",
    "                                      embed,sequence_length=length(embed),\n",
    "                                      dtype=tf.float32,\n",
    "                                      time_major=False)  \n",
    "\n",
    "    # in the next step we multiply the hidden-vec matrix with the Ws1 by reshaping \n",
    "    h = tf.nn.tanh(tf.transpose(tf.reshape(tf.matmul(Ws1,tf.reshape(outputs,\n",
    "                                                                    [n_hidden,batch_size*n_steps])), \n",
    "                                           [da,batch_size,n_steps]),[1,0,2]))\n",
    "    # in this step we multiply the generated matrix with Ws2\n",
    "    a = tf.reshape(tf.matmul(Ws2,tf.reshape(h,\n",
    "                                            [da,batch_size*n_steps])),\n",
    "                   [batch_size,r,n_steps])\n",
    "    \n",
    "    def fn3(a,x):\n",
    "            return tf.nn.softmax(x)\n",
    "    h3 = tf.scan(fn3,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('flattening'):\n",
    "    # here we again multiply(batch) of the generated batch with the same hidden matrix\n",
    "    h4 = tf.matmul(h3,outputs)\n",
    "    # flattening the output embedded matrix\n",
    "    last = tf.reshape(h4,[-1,r*n_hidden])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0516 14:15:09.616365 47876 deprecation.py:506] From <ipython-input-54-aa67141db579>:3: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.name_scope('MLP'):\n",
    "    tf.nn.dropout(last,.5, noise_shape=None, seed=None, name=None)\n",
    "    pred1 = tf.nn.sigmoid(tf.matmul(last,Win)+biasesin)\n",
    "    pred  = tf.matmul(pred1, Wout) + biasesout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0516 14:15:38.359052 47876 deprecation.py:323] From <ipython-input-55-daa06a6c8ffd>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define loss and optimizer\n",
    "with tf.name_scope('cross'):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits =pred, labels = y) + beta*tf.nn.l2_loss(Ws2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    gvs = optimizer.compute_gradients(cost)\n",
    "    capped_gvs = [(tf.clip_by_norm(grad,0.5), var) for grad, var in gvs]\n",
    "    optimizer.apply_gradients(capped_gvs)\n",
    "    optimized = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "with tf.name_scope('Accuracy'):\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy     = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.scalar(\"cost\", cost)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all summaries into a single \"summary operation\" which we can execute in a session \n",
    "summary_op =tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initializing the variables\n",
    "train_iter = DataIterator(X_train,y_train, batch_size)    \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# This could give warning if in case the required port is being used already\n",
    "# Running the command again or releasing the port before the subsequent run should solve the purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start to train the model.**\n",
    "\n",
    "Make sure the batch_size is sufficient enough to fit system requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 64, Minibatch Loss= 49.681767, Training Accuracy= 50.00%\n",
      "Iter 384, Minibatch Loss= 0.701942, Training Accuracy= 53.12%\n",
      "Iter 704, Minibatch Loss= 0.827410, Training Accuracy= 53.12%\n",
      "Iter 1024, Minibatch Loss= 0.807983, Training Accuracy= 40.62%\n",
      "Iter 1344, Minibatch Loss= 0.731755, Training Accuracy= 59.38%\n",
      "Iter 1664, Minibatch Loss= 0.782390, Training Accuracy= 43.75%\n",
      "Iter 1984, Minibatch Loss= 0.798124, Training Accuracy= 46.88%\n",
      "Iter 2304, Minibatch Loss= 0.719862, Training Accuracy= 46.88%\n",
      "Iter 2624, Minibatch Loss= 0.711035, Training Accuracy= 56.25%\n",
      "Iter 2944, Minibatch Loss= 0.713115, Training Accuracy= 56.25%\n",
      "Iter 3264, Minibatch Loss= 0.755966, Training Accuracy= 40.62%\n",
      "Iter 3584, Minibatch Loss= 0.695097, Training Accuracy= 59.38%\n",
      "Iter 3904, Minibatch Loss= 0.708559, Training Accuracy= 50.00%\n",
      "Iter 4224, Minibatch Loss= 0.742921, Training Accuracy= 46.88%\n",
      "Iter 4544, Minibatch Loss= 0.746765, Training Accuracy= 46.88%\n",
      "Iter 4864, Minibatch Loss= 0.684644, Training Accuracy= 56.25%\n",
      "Iter 5184, Minibatch Loss= 0.645956, Training Accuracy= 71.88%\n",
      "Iter 5504, Minibatch Loss= 0.758342, Training Accuracy= 43.75%\n",
      "Iter 5824, Minibatch Loss= 0.761538, Training Accuracy= 34.38%\n",
      "Iter 6144, Minibatch Loss= 0.714275, Training Accuracy= 56.25%\n",
      "Iter 6464, Minibatch Loss= 0.735854, Training Accuracy= 34.38%\n",
      "Iter 6784, Minibatch Loss= 0.660564, Training Accuracy= 65.62%\n",
      "Iter 7104, Minibatch Loss= 0.745878, Training Accuracy= 46.88%\n",
      "Iter 7424, Minibatch Loss= 0.747807, Training Accuracy= 43.75%\n",
      "Iter 7744, Minibatch Loss= 0.699763, Training Accuracy= 56.25%\n",
      "Iter 8064, Minibatch Loss= 0.689521, Training Accuracy= 62.50%\n",
      "Iter 8384, Minibatch Loss= 0.718817, Training Accuracy= 43.75%\n",
      "Iter 8704, Minibatch Loss= 0.698476, Training Accuracy= 59.38%\n",
      "Iter 9024, Minibatch Loss= 0.699105, Training Accuracy= 62.50%\n",
      "Iter 9344, Minibatch Loss= 0.758009, Training Accuracy= 40.62%\n",
      "Iter 9664, Minibatch Loss= 0.703371, Training Accuracy= 56.25%\n",
      "Iter 9984, Minibatch Loss= 0.708800, Training Accuracy= 53.12%\n",
      "Iter 10304, Minibatch Loss= 0.701073, Training Accuracy= 56.25%\n",
      "Iter 10624, Minibatch Loss= 0.836057, Training Accuracy= 53.12%\n",
      "Iter 10944, Minibatch Loss= 0.723400, Training Accuracy= 31.25%\n",
      "Iter 11264, Minibatch Loss= 0.821241, Training Accuracy= 43.75%\n",
      "Iter 11584, Minibatch Loss= 0.787448, Training Accuracy= 50.00%\n",
      "Iter 11904, Minibatch Loss= 0.786023, Training Accuracy= 59.38%\n",
      "Iter 12224, Minibatch Loss= 0.750730, Training Accuracy= 50.00%\n",
      "Iter 12544, Minibatch Loss= 0.851777, Training Accuracy= 46.88%\n",
      "Iter 12864, Minibatch Loss= 0.680801, Training Accuracy= 62.50%\n",
      "Iter 13184, Minibatch Loss= 0.787348, Training Accuracy= 43.75%\n",
      "Iter 13504, Minibatch Loss= 0.663573, Training Accuracy= 65.62%\n",
      "Iter 13824, Minibatch Loss= 1.017168, Training Accuracy= 53.12%\n",
      "Iter 14144, Minibatch Loss= 0.711307, Training Accuracy= 59.38%\n",
      "Iter 14464, Minibatch Loss= 0.788579, Training Accuracy= 53.12%\n",
      "Iter 14784, Minibatch Loss= 0.825668, Training Accuracy= 50.00%\n",
      "Iter 15104, Minibatch Loss= 0.707574, Training Accuracy= 53.12%\n",
      "Iter 15424, Minibatch Loss= 0.706989, Training Accuracy= 53.12%\n",
      "Iter 15744, Minibatch Loss= 0.690066, Training Accuracy= 62.50%\n",
      "Iter 16064, Minibatch Loss= 0.700892, Training Accuracy= 56.25%\n",
      "Iter 16384, Minibatch Loss= 0.753678, Training Accuracy= 40.62%\n",
      "Iter 16704, Minibatch Loss= 0.748071, Training Accuracy= 43.75%\n",
      "Iter 17024, Minibatch Loss= 0.718374, Training Accuracy= 50.00%\n",
      "Iter 17344, Minibatch Loss= 0.661165, Training Accuracy= 65.62%\n",
      "Iter 17664, Minibatch Loss= 0.721888, Training Accuracy= 50.00%\n",
      "Iter 17984, Minibatch Loss= 0.702014, Training Accuracy= 62.50%\n",
      "Iter 18304, Minibatch Loss= 0.836715, Training Accuracy= 46.88%\n",
      "Iter 18624, Minibatch Loss= 0.713184, Training Accuracy= 50.00%\n",
      "Iter 18944, Minibatch Loss= 0.737166, Training Accuracy= 46.88%\n",
      "Iter 19264, Minibatch Loss= 0.694766, Training Accuracy= 59.38%\n",
      "Iter 19584, Minibatch Loss= 0.772719, Training Accuracy= 50.00%\n",
      "Iter 19904, Minibatch Loss= 0.707069, Training Accuracy= 68.75%\n",
      "Iter 20224, Minibatch Loss= 0.723705, Training Accuracy= 43.75%\n",
      "Iter 20544, Minibatch Loss= 0.701246, Training Accuracy= 56.25%\n",
      "Iter 20864, Minibatch Loss= 0.685293, Training Accuracy= 62.50%\n",
      "Iter 21184, Minibatch Loss= 0.701171, Training Accuracy= 65.62%\n",
      "Iter 21504, Minibatch Loss= 0.706333, Training Accuracy= 53.12%\n",
      "Iter 21824, Minibatch Loss= 0.715513, Training Accuracy= 53.12%\n",
      "Iter 22144, Minibatch Loss= 0.699785, Training Accuracy= 56.25%\n",
      "Iter 22464, Minibatch Loss= 0.690210, Training Accuracy= 59.38%\n",
      "Iter 22784, Minibatch Loss= 0.699156, Training Accuracy= 68.75%\n",
      "Iter 23104, Minibatch Loss= 0.728522, Training Accuracy= 43.75%\n",
      "Iter 23424, Minibatch Loss= 0.902619, Training Accuracy= 56.25%\n",
      "Iter 23744, Minibatch Loss= 0.943045, Training Accuracy= 46.88%\n",
      "Iter 24064, Minibatch Loss= 0.728050, Training Accuracy= 46.88%\n",
      "Iter 24384, Minibatch Loss= 0.732392, Training Accuracy= 56.25%\n",
      "Iter 24704, Minibatch Loss= 0.732235, Training Accuracy= 53.12%\n",
      "Iter 25024, Minibatch Loss= 0.721416, Training Accuracy= 43.75%\n",
      "Iter 25344, Minibatch Loss= 0.755041, Training Accuracy= 46.88%\n",
      "Iter 25664, Minibatch Loss= 0.718379, Training Accuracy= 50.00%\n",
      "Iter 25984, Minibatch Loss= 0.744585, Training Accuracy= 53.12%\n",
      "Iter 26304, Minibatch Loss= 0.728716, Training Accuracy= 34.38%\n",
      "Iter 26624, Minibatch Loss= 0.701915, Training Accuracy= 59.38%\n",
      "Iter 26944, Minibatch Loss= 0.706684, Training Accuracy= 53.12%\n",
      "Iter 27264, Minibatch Loss= 0.706522, Training Accuracy= 56.25%\n",
      "Iter 27584, Minibatch Loss= 0.713825, Training Accuracy= 50.00%\n",
      "Iter 27904, Minibatch Loss= 0.745341, Training Accuracy= 50.00%\n",
      "Iter 28224, Minibatch Loss= 0.741502, Training Accuracy= 46.88%\n",
      "Iter 28544, Minibatch Loss= 0.688067, Training Accuracy= 62.50%\n",
      "Iter 28864, Minibatch Loss= 0.699745, Training Accuracy= 56.25%\n",
      "Iter 29184, Minibatch Loss= 0.725807, Training Accuracy= 50.00%\n",
      "Iter 29504, Minibatch Loss= 0.707444, Training Accuracy= 56.25%\n",
      "Iter 29824, Minibatch Loss= 0.719731, Training Accuracy= 59.38%\n",
      "Iter 30144, Minibatch Loss= 0.735816, Training Accuracy= 53.12%\n",
      "Iter 30464, Minibatch Loss= 0.696208, Training Accuracy= 59.38%\n",
      "Iter 30784, Minibatch Loss= 0.849228, Training Accuracy= 56.25%\n",
      "Iter 31104, Minibatch Loss= 0.707963, Training Accuracy= 53.12%\n",
      "Iter 31424, Minibatch Loss= 0.803942, Training Accuracy= 56.25%\n",
      "Iter 31744, Minibatch Loss= 0.705405, Training Accuracy= 56.25%\n",
      "Iter 32064, Minibatch Loss= 0.769522, Training Accuracy= 53.12%\n",
      "Iter 32384, Minibatch Loss= 0.705884, Training Accuracy= 53.12%\n",
      "Iter 32704, Minibatch Loss= 0.713016, Training Accuracy= 46.88%\n",
      "Iter 33024, Minibatch Loss= 0.719367, Training Accuracy= 50.00%\n",
      "Iter 33344, Minibatch Loss= 0.689580, Training Accuracy= 59.38%\n",
      "Iter 33664, Minibatch Loss= 0.755362, Training Accuracy= 34.38%\n",
      "Iter 33984, Minibatch Loss= 0.833865, Training Accuracy= 46.88%\n",
      "Iter 34304, Minibatch Loss= 0.705450, Training Accuracy= 53.12%\n",
      "Iter 34624, Minibatch Loss= 0.721727, Training Accuracy= 46.88%\n",
      "Iter 34944, Minibatch Loss= 0.727368, Training Accuracy= 53.12%\n",
      "Iter 35264, Minibatch Loss= 0.730225, Training Accuracy= 62.50%\n",
      "Iter 35584, Minibatch Loss= 0.753668, Training Accuracy= 43.75%\n",
      "Iter 35904, Minibatch Loss= 0.730058, Training Accuracy= 40.62%\n",
      "Iter 36224, Minibatch Loss= 0.705079, Training Accuracy= 59.38%\n",
      "Iter 36544, Minibatch Loss= 0.659181, Training Accuracy= 65.62%\n",
      "Iter 36864, Minibatch Loss= 0.901017, Training Accuracy= 34.38%\n",
      "Iter 37184, Minibatch Loss= 0.720536, Training Accuracy= 43.75%\n",
      "Iter 37504, Minibatch Loss= 0.691208, Training Accuracy= 59.38%\n",
      "Iter 37824, Minibatch Loss= 0.716298, Training Accuracy= 43.75%\n",
      "Iter 38144, Minibatch Loss= 0.698969, Training Accuracy= 56.25%\n",
      "Iter 38464, Minibatch Loss= 0.723142, Training Accuracy= 40.62%\n",
      "Iter 38784, Minibatch Loss= 0.695709, Training Accuracy= 59.38%\n",
      "Iter 39104, Minibatch Loss= 0.722255, Training Accuracy= 59.38%\n",
      "Iter 39424, Minibatch Loss= 0.692454, Training Accuracy= 59.38%\n",
      "Iter 39744, Minibatch Loss= 0.749366, Training Accuracy= 53.12%\n",
      "Iter 40064, Minibatch Loss= 0.876552, Training Accuracy= 53.12%\n",
      "Iter 40384, Minibatch Loss= 0.738306, Training Accuracy= 43.75%\n",
      "Iter 40704, Minibatch Loss= 0.707634, Training Accuracy= 50.00%\n",
      "Iter 41024, Minibatch Loss= 0.725743, Training Accuracy= 43.75%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 41344, Minibatch Loss= 0.764301, Training Accuracy= 46.88%\n",
      "Iter 41664, Minibatch Loss= 0.688696, Training Accuracy= 59.38%\n",
      "Iter 41984, Minibatch Loss= 0.731804, Training Accuracy= 43.75%\n",
      "Iter 42304, Minibatch Loss= 0.710098, Training Accuracy= 46.88%\n",
      "Iter 42624, Minibatch Loss= 0.688416, Training Accuracy= 59.38%\n",
      "Iter 42944, Minibatch Loss= 0.704524, Training Accuracy= 53.12%\n",
      "Iter 43264, Minibatch Loss= 0.688413, Training Accuracy= 59.38%\n",
      "Iter 43584, Minibatch Loss= 0.922255, Training Accuracy= 46.88%\n",
      "Iter 43904, Minibatch Loss= 0.698152, Training Accuracy= 59.38%\n",
      "Iter 44224, Minibatch Loss= 0.922108, Training Accuracy= 40.62%\n",
      "Iter 44544, Minibatch Loss= 0.696977, Training Accuracy= 59.38%\n",
      "Iter 44864, Minibatch Loss= 0.708457, Training Accuracy= 46.88%\n",
      "Iter 45184, Minibatch Loss= 0.712999, Training Accuracy= 50.00%\n",
      "Iter 45504, Minibatch Loss= 0.705058, Training Accuracy= 56.25%\n",
      "Iter 45824, Minibatch Loss= 0.716852, Training Accuracy= 56.25%\n",
      "Iter 46144, Minibatch Loss= 0.738991, Training Accuracy= 40.62%\n",
      "Iter 46464, Minibatch Loss= 0.740571, Training Accuracy= 62.50%\n",
      "Iter 46784, Minibatch Loss= 0.674421, Training Accuracy= 62.50%\n",
      "Iter 47104, Minibatch Loss= 0.706070, Training Accuracy= 50.00%\n",
      "Iter 47424, Minibatch Loss= 0.718861, Training Accuracy= 59.38%\n",
      "Iter 47744, Minibatch Loss= 0.709752, Training Accuracy= 50.00%\n",
      "Iter 48064, Minibatch Loss= 0.694074, Training Accuracy= 65.62%\n",
      "Iter 48384, Minibatch Loss= 0.747288, Training Accuracy= 50.00%\n",
      "Iter 48704, Minibatch Loss= 0.742632, Training Accuracy= 56.25%\n",
      "Iter 49024, Minibatch Loss= 0.711953, Training Accuracy= 53.12%\n",
      "Iter 49344, Minibatch Loss= 0.743932, Training Accuracy= 53.12%\n",
      "Iter 49664, Minibatch Loss= 0.755491, Training Accuracy= 46.88%\n",
      "Iter 49984, Minibatch Loss= 0.713845, Training Accuracy= 46.88%\n",
      "Iter 50304, Minibatch Loss= 0.762254, Training Accuracy= 53.12%\n",
      "Iter 50624, Minibatch Loss= 0.753291, Training Accuracy= 53.12%\n",
      "Iter 50944, Minibatch Loss= 0.703839, Training Accuracy= 56.25%\n",
      "Iter 51264, Minibatch Loss= 0.697620, Training Accuracy= 56.25%\n",
      "Iter 51584, Minibatch Loss= 0.738148, Training Accuracy= 40.62%\n",
      "Iter 51904, Minibatch Loss= 0.718256, Training Accuracy= 56.25%\n",
      "Iter 52224, Minibatch Loss= 0.697889, Training Accuracy= 59.38%\n",
      "Iter 52544, Minibatch Loss= 0.716829, Training Accuracy= 62.50%\n",
      "Iter 52864, Minibatch Loss= 0.697536, Training Accuracy= 59.38%\n",
      "Iter 53184, Minibatch Loss= 0.853215, Training Accuracy= 46.88%\n",
      "Iter 53504, Minibatch Loss= 0.726467, Training Accuracy= 50.00%\n",
      "Iter 53824, Minibatch Loss= 0.826343, Training Accuracy= 43.75%\n",
      "Iter 54144, Minibatch Loss= 0.687729, Training Accuracy= 59.38%\n",
      "Iter 54464, Minibatch Loss= 0.851609, Training Accuracy= 53.12%\n",
      "Iter 54784, Minibatch Loss= 0.938537, Training Accuracy= 43.75%\n",
      "Iter 55104, Minibatch Loss= 0.722964, Training Accuracy= 53.12%\n",
      "Iter 55424, Minibatch Loss= 0.740072, Training Accuracy= 50.00%\n",
      "Iter 55744, Minibatch Loss= 0.899574, Training Accuracy= 40.62%\n",
      "Iter 56064, Minibatch Loss= 0.678533, Training Accuracy= 62.50%\n",
      "Iter 56384, Minibatch Loss= 0.690873, Training Accuracy= 59.38%\n",
      "Iter 56704, Minibatch Loss= 0.690450, Training Accuracy= 59.38%\n",
      "Iter 57024, Minibatch Loss= 0.633022, Training Accuracy= 68.75%\n",
      "Iter 57344, Minibatch Loss= 0.898777, Training Accuracy= 56.25%\n",
      "Iter 57664, Minibatch Loss= 0.703163, Training Accuracy= 53.12%\n",
      "Iter 57984, Minibatch Loss= 0.675202, Training Accuracy= 62.50%\n",
      "Iter 58304, Minibatch Loss= 0.943451, Training Accuracy= 53.12%\n",
      "Iter 58624, Minibatch Loss= 0.928639, Training Accuracy= 62.50%\n",
      "Iter 58944, Minibatch Loss= 0.655370, Training Accuracy= 65.62%\n",
      "Iter 59264, Minibatch Loss= 0.819444, Training Accuracy= 46.88%\n",
      "Iter 59584, Minibatch Loss= 0.678855, Training Accuracy= 65.62%\n",
      "Iter 59904, Minibatch Loss= 0.821019, Training Accuracy= 53.12%\n",
      "Iter 60224, Minibatch Loss= 0.986783, Training Accuracy= 56.25%\n",
      "Iter 60544, Minibatch Loss= 0.779496, Training Accuracy= 46.88%\n",
      "Iter 60864, Minibatch Loss= 0.754407, Training Accuracy= 50.00%\n",
      "Iter 61184, Minibatch Loss= 0.706201, Training Accuracy= 50.00%\n",
      "Iter 61504, Minibatch Loss= 0.725691, Training Accuracy= 34.38%\n",
      "Iter 61824, Minibatch Loss= 0.693137, Training Accuracy= 59.38%\n",
      "Iter 62144, Minibatch Loss= 0.723184, Training Accuracy= 53.12%\n",
      "Iter 62464, Minibatch Loss= 0.696365, Training Accuracy= 59.38%\n",
      "Iter 62784, Minibatch Loss= 0.742703, Training Accuracy= 40.62%\n",
      "Iter 63104, Minibatch Loss= 0.706709, Training Accuracy= 43.75%\n",
      "Iter 63424, Minibatch Loss= 0.704542, Training Accuracy= 50.00%\n",
      "Iter 63744, Minibatch Loss= 0.705254, Training Accuracy= 50.00%\n",
      "Iter 64064, Minibatch Loss= 0.744418, Training Accuracy= 46.88%\n",
      "Iter 64384, Minibatch Loss= 0.745434, Training Accuracy= 50.00%\n",
      "Iter 64704, Minibatch Loss= 0.745867, Training Accuracy= 43.75%\n",
      "Iter 65024, Minibatch Loss= 0.802434, Training Accuracy= 34.38%\n",
      "Iter 65344, Minibatch Loss= 0.670738, Training Accuracy= 71.88%\n",
      "Iter 65664, Minibatch Loss= 0.716902, Training Accuracy= 37.50%\n",
      "Iter 65984, Minibatch Loss= 0.707037, Training Accuracy= 43.75%\n",
      "Iter 66304, Minibatch Loss= 0.763223, Training Accuracy= 43.75%\n",
      "Iter 66624, Minibatch Loss= 0.738742, Training Accuracy= 62.50%\n",
      "Iter 66944, Minibatch Loss= 0.826630, Training Accuracy= 46.88%\n",
      "Iter 67264, Minibatch Loss= 0.704833, Training Accuracy= 53.12%\n",
      "Iter 67584, Minibatch Loss= 0.804046, Training Accuracy= 53.12%\n",
      "Iter 67904, Minibatch Loss= 0.699430, Training Accuracy= 56.25%\n",
      "Iter 68224, Minibatch Loss= 0.788316, Training Accuracy= 46.88%\n",
      "Iter 68544, Minibatch Loss= 0.704264, Training Accuracy= 50.00%\n",
      "Iter 68864, Minibatch Loss= 0.801020, Training Accuracy= 37.50%\n",
      "Iter 69184, Minibatch Loss= 0.706772, Training Accuracy= 50.00%\n",
      "Iter 69504, Minibatch Loss= 0.704345, Training Accuracy= 53.12%\n",
      "Iter 69824, Minibatch Loss= 0.760422, Training Accuracy= 56.25%\n",
      "Iter 70144, Minibatch Loss= 0.880416, Training Accuracy= 40.62%\n",
      "Iter 70464, Minibatch Loss= 0.699821, Training Accuracy= 56.25%\n",
      "Iter 70784, Minibatch Loss= 0.726497, Training Accuracy= 34.38%\n",
      "Iter 71104, Minibatch Loss= 0.795550, Training Accuracy= 40.62%\n",
      "Iter 71424, Minibatch Loss= 0.738045, Training Accuracy= 50.00%\n",
      "Iter 71744, Minibatch Loss= 0.753927, Training Accuracy= 43.75%\n",
      "Iter 72064, Minibatch Loss= 0.772237, Training Accuracy= 46.88%\n",
      "Iter 72384, Minibatch Loss= 0.714051, Training Accuracy= 56.25%\n",
      "Iter 72704, Minibatch Loss= 0.701492, Training Accuracy= 56.25%\n",
      "Iter 73024, Minibatch Loss= 0.743280, Training Accuracy= 40.62%\n",
      "Iter 73344, Minibatch Loss= 0.724674, Training Accuracy= 59.38%\n",
      "Iter 73664, Minibatch Loss= 0.728874, Training Accuracy= 40.62%\n",
      "Iter 73984, Minibatch Loss= 0.704175, Training Accuracy= 59.38%\n",
      "Iter 74304, Minibatch Loss= 0.796547, Training Accuracy= 50.00%\n",
      "Iter 74624, Minibatch Loss= 0.697439, Training Accuracy= 56.25%\n",
      "Iter 74944, Minibatch Loss= 0.705905, Training Accuracy= 56.25%\n",
      "Iter 75264, Minibatch Loss= 0.818118, Training Accuracy= 56.25%\n",
      "Iter 75584, Minibatch Loss= 0.703763, Training Accuracy= 46.88%\n",
      "Iter 75904, Minibatch Loss= 0.771255, Training Accuracy= 46.88%\n",
      "Iter 76224, Minibatch Loss= 0.701057, Training Accuracy= 56.25%\n",
      "Iter 76544, Minibatch Loss= 0.685772, Training Accuracy= 59.38%\n",
      "Iter 76864, Minibatch Loss= 0.702945, Training Accuracy= 53.12%\n",
      "Iter 77184, Minibatch Loss= 0.693820, Training Accuracy= 59.38%\n",
      "Iter 77504, Minibatch Loss= 0.720303, Training Accuracy= 56.25%\n",
      "Iter 77824, Minibatch Loss= 0.688007, Training Accuracy= 62.50%\n",
      "Iter 78144, Minibatch Loss= 0.726596, Training Accuracy= 53.12%\n",
      "Iter 78464, Minibatch Loss= 0.674980, Training Accuracy= 65.62%\n",
      "Iter 78784, Minibatch Loss= 0.687422, Training Accuracy= 59.38%\n",
      "Iter 79104, Minibatch Loss= 0.713757, Training Accuracy= 46.88%\n",
      "Iter 79424, Minibatch Loss= 0.707437, Training Accuracy= 46.88%\n",
      "Iter 79744, Minibatch Loss= 0.731098, Training Accuracy= 53.12%\n",
      "Iter 80064, Minibatch Loss= 0.711454, Training Accuracy= 53.12%\n",
      "Iter 80384, Minibatch Loss= 0.698496, Training Accuracy= 59.38%\n",
      "Iter 80704, Minibatch Loss= 0.795704, Training Accuracy= 40.62%\n",
      "Iter 81024, Minibatch Loss= 0.709752, Training Accuracy= 53.12%\n",
      "Iter 81344, Minibatch Loss= 0.704951, Training Accuracy= 56.25%\n",
      "Iter 81664, Minibatch Loss= 0.702406, Training Accuracy= 59.38%\n",
      "Iter 81984, Minibatch Loss= 0.708223, Training Accuracy= 50.00%\n",
      "Iter 82304, Minibatch Loss= 0.740937, Training Accuracy= 50.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 82624, Minibatch Loss= 0.708601, Training Accuracy= 50.00%\n",
      "Iter 82944, Minibatch Loss= 0.817616, Training Accuracy= 46.88%\n",
      "Iter 83264, Minibatch Loss= 0.716550, Training Accuracy= 34.38%\n",
      "Iter 83584, Minibatch Loss= 0.654846, Training Accuracy= 65.62%\n",
      "Iter 83904, Minibatch Loss= 0.685152, Training Accuracy= 59.38%\n",
      "Iter 84224, Minibatch Loss= 0.703475, Training Accuracy= 50.00%\n",
      "Iter 84544, Minibatch Loss= 0.707668, Training Accuracy= 50.00%\n",
      "Iter 84864, Minibatch Loss= 0.750228, Training Accuracy= 53.12%\n",
      "Iter 85184, Minibatch Loss= 0.718305, Training Accuracy= 53.12%\n",
      "Iter 85504, Minibatch Loss= 0.697662, Training Accuracy= 62.50%\n",
      "Iter 85824, Minibatch Loss= 0.753453, Training Accuracy= 50.00%\n",
      "Iter 86144, Minibatch Loss= 0.741124, Training Accuracy= 46.88%\n",
      "Iter 86464, Minibatch Loss= 0.725558, Training Accuracy= 50.00%\n",
      "Iter 86784, Minibatch Loss= 0.799057, Training Accuracy= 43.75%\n",
      "Iter 87104, Minibatch Loss= 0.711870, Training Accuracy= 46.88%\n",
      "Iter 87424, Minibatch Loss= 0.760621, Training Accuracy= 40.62%\n",
      "Iter 87744, Minibatch Loss= 0.697533, Training Accuracy= 62.50%\n",
      "Iter 88064, Minibatch Loss= 0.675053, Training Accuracy= 62.50%\n",
      "Iter 88384, Minibatch Loss= 0.696829, Training Accuracy= 59.38%\n",
      "Iter 88704, Minibatch Loss= 0.733567, Training Accuracy= 46.88%\n",
      "Iter 89024, Minibatch Loss= 0.664974, Training Accuracy= 68.75%\n",
      "Iter 89344, Minibatch Loss= 0.699281, Training Accuracy= 59.38%\n",
      "Iter 89664, Minibatch Loss= 0.795135, Training Accuracy= 40.62%\n",
      "Iter 89984, Minibatch Loss= 0.692276, Training Accuracy= 62.50%\n",
      "Iter 90304, Minibatch Loss= 0.717003, Training Accuracy= 40.62%\n",
      "Iter 90624, Minibatch Loss= 0.695754, Training Accuracy= 56.25%\n",
      "Iter 90944, Minibatch Loss= 0.776177, Training Accuracy= 46.88%\n",
      "Iter 91264, Minibatch Loss= 0.692099, Training Accuracy= 62.50%\n",
      "Iter 91584, Minibatch Loss= 0.790185, Training Accuracy= 43.75%\n",
      "Iter 91904, Minibatch Loss= 0.965628, Training Accuracy= 43.75%\n",
      "Iter 92224, Minibatch Loss= 0.707802, Training Accuracy= 50.00%\n",
      "Iter 92544, Minibatch Loss= 0.594891, Training Accuracy= 75.00%\n",
      "Iter 92864, Minibatch Loss= 0.715817, Training Accuracy= 50.00%\n",
      "Iter 93184, Minibatch Loss= 0.694265, Training Accuracy= 56.25%\n",
      "Iter 93504, Minibatch Loss= 0.708649, Training Accuracy= 56.25%\n",
      "Iter 93824, Minibatch Loss= 0.787586, Training Accuracy= 56.25%\n",
      "Iter 94144, Minibatch Loss= 0.701019, Training Accuracy= 53.12%\n",
      "Iter 94464, Minibatch Loss= 0.826924, Training Accuracy= 50.00%\n",
      "Iter 94784, Minibatch Loss= 0.837523, Training Accuracy= 40.62%\n",
      "Iter 95104, Minibatch Loss= 0.753272, Training Accuracy= 59.38%\n",
      "Iter 95424, Minibatch Loss= 0.697183, Training Accuracy= 56.25%\n",
      "Iter 95744, Minibatch Loss= 0.845076, Training Accuracy= 59.38%\n",
      "Iter 96064, Minibatch Loss= 0.760136, Training Accuracy= 50.00%\n",
      "Iter 96384, Minibatch Loss= 0.811644, Training Accuracy= 50.00%\n",
      "Iter 96704, Minibatch Loss= 0.703894, Training Accuracy= 46.88%\n",
      "Iter 97024, Minibatch Loss= 0.745020, Training Accuracy= 46.88%\n",
      "Iter 97344, Minibatch Loss= 0.694608, Training Accuracy= 56.25%\n",
      "Iter 97664, Minibatch Loss= 0.686304, Training Accuracy= 59.38%\n",
      "Iter 97984, Minibatch Loss= 0.814550, Training Accuracy= 43.75%\n",
      "Iter 98304, Minibatch Loss= 0.846873, Training Accuracy= 31.25%\n",
      "Iter 98624, Minibatch Loss= 0.677712, Training Accuracy= 62.50%\n",
      "Iter 98944, Minibatch Loss= 0.759914, Training Accuracy= 43.75%\n",
      "Iter 99264, Minibatch Loss= 0.694157, Training Accuracy= 56.25%\n",
      "Iter 99584, Minibatch Loss= 0.703086, Training Accuracy= 40.62%\n",
      "Iter 99904, Minibatch Loss= 0.724936, Training Accuracy= 46.88%\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Creating log file writer object\n",
    "    writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    step = 1\n",
    "    \n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = train_iter.next_batch()\n",
    "        sess.run(optimized, feed_dict={x: batch_x, y: batch_y})\n",
    "        # Executing the summary operation in the session\n",
    "        summary = sess.run(summary_op, feed_dict={x: batch_x, y: batch_y})\n",
    "        # Writing the values in log file using the FileWriter object created above\n",
    "        writer.add_summary(summary,  step*batch_size)\n",
    "        if step % display_step == 2:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print (\"Iter \" + str(step*batch_size) + \\\n",
    "                   \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \\\n",
    "                   \", Training Accuracy= \" + \"{:.2f}\".format(acc*100) + \"%\")\n",
    "        step += 1\n",
    "    print (\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the graph visualization type the next line in your terminal inside of the project folder:\n",
    "    \n",
    "    tensorboard --logdir=./ --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows the variables that have been scoped\n",
    "throughout the code, which helps in understanding the flow of the data\n",
    "and connections across the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"network_structure.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Figure shows the MLP component of the graph, which is used to\n",
    "add the addition of dropout to the last layer, and the sigmoid function to\n",
    "predict the final sentiment classification results. The final predictions are\n",
    "further used to gather the model’s accuracy and cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"MLP.JPG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
