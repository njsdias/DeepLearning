{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is based in the book: \n",
    "# Deep Learning with Keras\n",
    "## Implement neural netwroks with Keras on Thano and TensorFlow\n",
    "\n",
    "### Authors:  Antonio Gulli, Sujit Pal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first example of Keras code\n",
    "The initial building block of Keras is a model, and the simplest model is called sequential. A sequential Keras model is a linear pipeline (a stack) of neural networks layers. This code fragment defines a single layer with **12 artificial neurons**, and it expects **8 input variables (also known as features)**.In this example **random_uniform** is used to initialize the weiths of the layers with uniformly random small values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "model = Sequential()\n",
    "#random_uniform Weights are initialized to uniformly random small values in (-0.05, 0.05).\n",
    "#There are more ways to initialize weights: https://keras.io/initializers/\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='random_uniform')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The _net is dense_, meaning that each neuron in a layer is connected to all neurons\n",
    "located in the previous layer and to all the neurons in the following layer.\n",
    "\n",
    "**Sigmoid Function:** $\\sigma(x)= \\frac{1}{1+e^{-x}}$ -> used by neuron for computing the nonlinear function $\\sigma(z=wx+b)\n",
    "$. A neuron with sigmoid activation has a behavior similar to the perceptron, but the changes are gradual.\n",
    "\n",
    "**Sigmoid** and **ReLU** are generally called **activation functions** in neural network jargon. They are the basic building blocks to developing a learning algorithm which adapts little by little, by progressively reducing the mistakes made by our nets.\n",
    "\n",
    "Full list of activation function is available at https://keras.io/activations/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot enconding- OHE**\n",
    "\n",
    "In many applications, it is convenient to transform categorical (non-numerical) features into numerical\n",
    "variables. For instance, the categorical feature digit with the value d in [0-9] can be encoded into a\n",
    "binary vector with 10 positions, which always has 0 value, except the d-th position where a 1 is\n",
    "present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A real example — recognizing handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will build a network that can recognize handwritten numbers. For achieving this\n",
    "goal, we use MNIST (for more information, refer to http://yann.lecun.com/exdb/mnist/), a database of\n",
    "handwritten digits made up of a training set of 60,000 examples and a test set of 10,000 examples.\n",
    "The training examples are annotated by humans with the correct answer. For instance, if the\n",
    "handwritten digit is the number three, then three is simply the label associated with that example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist                       #import dataset\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671)                                   # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network and training\n",
    "NB_EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10                        # number of outputs = number of digits\n",
    "OPTIMIZER = SGD()                      # SGD optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2                   # how much TRAIN is reserved for VALIDATION: 0.8 is for training and 0.2 is for test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need terminologies like **epochs, batch size, iterations** only when the data is too big which happens all the time in machine learning and we can’t pass all the data to the computer at once. So, to overcome this problem we need to divide the data into smaller sizes and give it to our computer one by one and update the weights of the neural networks at the end of every step to fit it to the data given.\n",
    "\n",
    "**Epochs**: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE.\n",
    "\n",
    "As the number of **epochs increases**, more number of times the weight are changed in the neural network and the curve goes from underfitting to optimal to overfitting curve.\n",
    "\n",
    "**Batch Size**: Total number of training examples present in a single batch. As I said, you can’t pass the entire dataset into the neural net at once. So, you divide dataset into Number of Batches or sets or parts. Just like you divide a big article into multiple sets/batches/parts like Introduction, Gradient descent, Epoch, Batch size and Iterations which makes it easy to read the entire article for the reader and understand it.\n",
    "\n",
    "**Iterations**: It is the number of batches needed to complete one epoch. Let’s say we have 2000 training examples that we are going to use. We can divide the dataset of 2000 examples into batches of 500 then it will take 4 iterations to complete 1 epoch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
